---
title: "Introduction to Turing.jl"
description: "A quick introduction to probabilistic programming language Turing.jl"
format: 
  html: 
    toc: true
    number-sections: true
jupyter: julia-1.10
draft: false
---

Turing.jl is a probabilistic proramming language that let's us define a generative model and does inference.

## Coin example

Let's flip a biased coin a hundred times.

```{julia}
using Turing
using DataFrames
coin_flips = rand(Bernoulli(0.7), 100);
```

In order to do inference, we need to specify a model.

 $$ 
 \begin{align*}
 p & \sim Beta(1,1) \\
 coinflip & \sim Bernoulli(p) 
 \end{align*} 
 $$


```{julia}
@model function coin(data)
  p ~ Beta(1,1)
  for i in eachindex(data) 
    data[i] ~ Bernoulli(p)
  end
end
```


### Sample from prior

```{julia}
prior = sample(coin(coin_flips), Prior(), 1000)
```

An overview of the chain can be obtained using `summarystats` or `quantile`.
Or `describe`, which returns both.

```{julia}
summarystats(prior)
```

```{julia}
DataFrame(prior)[1:10, :]
```


### Sample from posterior

```{julia}
posterior = sample(coin(coin_flips), NUTS(), 1000)
```


```{julia}
summarystats(posterior)
```

```{julia}
DataFrame(posterior)
```


### Prior predictive check

Here we pass in a vector containing `missing`. Sampling then generates observations.

```{julia}
observations = Vector{Missing}(missing, length(coin_flips))
prior_check = predict(coin(observations), prior)
```

`prior_check` is a `Chains` instance. To convert to an `Array`, use 

```{julia}
Array(prior_check, [:parameters])
```

### Posterior predictive check

```{julia}
observations = Vector{Missing}(missing, length(coin_flips))
posterior_check = predict(coin(observations), posterior)
```

### Chains

The results of sampling runs are `MCMCChains.Chains` instances.

```{julia}
value = rand(500, 2, 3)
chn = Chains(value, [:a, :b])
chn2 = Chains(value, ["A[1]", "A[2]"])
```

Names of variables stored in a chain can be retrieved using

```{julia}
names(prior)
```

Often, names are organized in to groups, i.e. `data[1]`, `data[2]`, ...
To retrieve the names within a group one can use `namesingroup(chn, :data)`.
`group(chn, :data)`

Names can be organized into sections. Turings `sample` returns a `Chains` instance with sections `:parameters` and `:internals`.

```{julia}
sections(prior)
```

The mapping of names into sections is stored in `prior.name_map` as a `NamedTuple`.

To retrieve the names for a specific section only, use e.g.

```{julia}
names(prior, :parameters)
```

Many functions can be restricted to specific sections. E.g. 
`summarystats(prior, :parameters)`, `Array(prior, :parameters)`.

A `Chains` instance has the following fields:
- value: `AxisArray` object of size `:iter` x `:var` x `:chain`
- logevidence
- name_map (to define sections, NamedTuple section -> names, default section is `:parameters`)
- info

Chains can be indexed, 
- `prior[:p]` returns an `AxisArray` for `:p`
- `prior[[:p, :lp]]` returns a `Chain` restricted to `[:p, :lp]`
- `prior[1:10, [:p], 1]` restricts chain to iterations `1:10`, `:p`, and the first chain 


An `AxisArray` has fields `:axes` and `:data`. It is like an `Array` but
is aware of dimension names.

For example, our `prior_check` from above has axes `[:iter, :var, :chain]`.

Can use 
`prior_check.value[var]`

I.e. to get only a slice of a dimension `:dim`, one can use `A[dim = idx]`,
so in the case of a MCMC sample, we might use `prior.value[var = :p]`, or `prior.value[chain = 1]`

To convert a chain to a DataFrame containing the `:parameters` section only, one can do

```{julia}
DataFrame(posterior[names(posterior, :parameters)]);
```

To extract the parameters as an `Array`one can use

```{julia}
Array(prior_check, :parameters);
```


## Linear regression

Linear regression models a continuous (dependent) variable as a linear combination of independent predictors.

$$
y = X \beta + \alpha + \epsilon
$$


The corresponding Bayesian formulation can be written as

$$
\begin{align*}
  \mathbf{y} & \sim N(\alpha + \mathbf{X} \mathbf{\beta}, \sigma) \\
  \alpha & \sim N(\mu_{\alpha}, \sigma_{\alpha}) \\
  \beta & \sim N(\mu_{\beta}, \sigma_{\beta}) \\
  \sigma & \sim Exp(\lambda_{\sigma})
\end{align*}
$$

The purpose of inference is to obtain $P(\alpha, \beta, \sigma | \mathbf{y}, \mathbf{X})$.

In `Turing.jl`, we first need to specify the generative model

```{julia}
using LinearAlgebra: I
using StatsBase
using Turing

@model function linreg(X, y; n = size(X, 2))

  α ~ Normal(mean(y), 2.5 * std(y))
  β ~ filldist(Normal(0, 2), n)
  σ ~ Exponential(1)

  return y ~ MvNormal(X*β .+ α, I * σ^2)
end
```

```{julia}
using CSV
using DataFrames

kidiq = CSV.read("kidiq.csv", DataFrame)
describe(kidiq)
```

```{julia}
X = Matrix(select(kidiq, Not(:kid_score)))
y = kidiq[:, :kid_score]
model = linreg(X, y)

names(select(kidiq, Not(:kid_score)))
```

```{julia}
chain = sample(model, NUTS(), 1000)
summarystats(chain)
```



## Logistic regression

```{julia}
using CSV
using DataFrames

wells = CSV.read("wells.csv", DataFrame)
describe(wells)
```


```{julia}
using Turing

@model function logreg(X, y; predictors=size(X, 2))
    #priors
    α ~ Normal(0, 2.5)
    β ~ filldist(TDist(3), predictors)

    #likelihood
    return y .~  BernoulliLogit.(α .+ X * β)
end;

function logistic(x)
    return 1 / (1 + exp(-x))
end

@model function logreg2(X, y; predictors=size(X, 2))
    #priors
    α ~ Normal(0, 2.5)
    β ~ filldist(TDist(3), predictors)

    #likelihood
    p = logistic.(α .+ X * β)
    return y .~  Bernoulli.(p)
end;



```

```{julia}
X = Matrix(select(wells, Not(:switch)))
y = wells[:, :switch]
model = logreg2(X, y);
```

```{julia}
chain = sample(model, NUTS(), 1000)
summarystats(chain)
```


## References

- Bayesian Statistics using Julia and Turing, https://storopoli.io/Bayesian-Julia/
- Turing.jl tutorials, https://turing.ml/v0.22/tutorials/ 
