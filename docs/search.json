[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Notes",
    "section": "",
    "text": "Double pendulum\n\n\n\njulia\n\n\nphysics\n\n\n\nSimulating a double pendulum in Julia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Turing.jl\n\n\nA quick introduction to probabilistic programming language Turing.jl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime-dependent Schrödinger equation\n\n\n\njulia\n\n\nphysics\n\n\n\nSolving the time-dependent Schrödinger equation in Julia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensor notation\n\n\n\nphysics\n\n\n\nIntroduction to tensor, index, or Einstein notation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPendulum\n\n\n\njulia\n\n\nphysics\n\n\n\nSimulating a pendulum in Julia, analytically, numerically, and symbolically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Latent Class Analysis\n\n\n\njulia\n\n\nBayes\n\n\nMCMC\n\n\nEM\n\n\n\nLatent class analysis using Gibbs sampling and Expectation-Maximization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulated Annealing\n\n\n\njulia\n\n\noptimization\n\n\nalgorithms\n\n\n\nSolving the Travleing Salesman Problem (TSP) using Simulated Annealing (SA) in julia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLattice Boltzmann Method (LBM)\n\n\n\njulia\n\n\nphysics\n\n\nCFD\n\n\n\nLattice Boltzmann Method (LBM) for 2-dimensional flow past cylinder in Julia\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nFor a nice tutorial on how to create them visit https://ucsb-meds.github.io/creating-quarto-websites/"
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html",
    "href": "posts/LatticeBoltzmannMethod/index.html",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "",
    "text": "The Lattice Boltzmann Method (LBM) is a powerful computational technique for simulating fluid dynamics. It is built on the principles of molecular dynamics and the Boltzmann equation but simplifies these concepts to create a more computationally efficient method."
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html#boltzmann-equation-and-the-bhatnagar-gross-krook-bgk-approximation",
    "href": "posts/LatticeBoltzmannMethod/index.html#boltzmann-equation-and-the-bhatnagar-gross-krook-bgk-approximation",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "Boltzmann Equation and the Bhatnagar-Gross-Krook (BGK) approximation",
    "text": "Boltzmann Equation and the Bhatnagar-Gross-Krook (BGK) approximation\nMolecular dynamics (MD) models the motion of individual particles based on Newton’s laws, providing detailed insights into microscopic properties. However, MD is computationally expensive for simulating large-scale fluid flows.\nThe Boltzmann equation offers a bridge between microscopic particle dynamics and macroscopic fluid properties. Instead of following each particle, the Boltzmann equation looks at the evolution of the so-called particle distribution function \\(f(x, \\xi, t)\\), which describes the probability of having a particle with position \\(x\\) and molecular velocity \\(\\xi\\) at time \\(t\\).\nKnowledge of the particle distribution function allows computation of macroscopic variables like density\n\\[\n\\int f(\\xi, x, t) d^3\\xi = \\rho(t)\n\\]\nand momentum\n\\[\n\\int \\xi f(\\xi, x, t) d^3\\xi = u \\rho(t)\n\\]\nThe Boltzmann equation describes how the particle distribution function evolves over time\n\\[\n\\frac{d}{dt}f(\\xi,x,t) = \\partial_t f + \\frac{d\\xi}{dt} \\partial_\\xi f + \\frac{dx}{dt} \\partial_x f = \\Omega(f)\n\\]\nHere \\(d\\xi/dt\\) describes external forces and the collision operator \\(\\Omega(f)\\) represents the effect of collisions between particles.\nIn the Bhatnagar-Gross-Krook (BGK) approximation, the collision operator is defined as\n\\[\n\\Omega(f) = - \\frac{1}{\\tau}(f - f^{eq})\n\\]\ni.e. the particle distribution relaxes towards its equilibrium distribution \\(f^{eq}\\) at the relaxation time scale \\(\\tau\\).\nIgnoring external forces (the term with \\(d\\xi/dt\\)), the Boltzmann equation in the BGK approximation can then be written as\n\\[\n\\frac{d}{dt}f(\\xi,x,t) = \\partial_t f + \\xi \\partial_x f = - \\frac{1}{\\tau}(f - f^{eq})\n\\]\nThe change of the particle distribution function within a cell is, therefore, due to two mechanisms - streaming of particles into and out of the cell - collision of particles within the cell, which leads to a relaxation towards the equilibrium condition.\nTODO: Eulerian vs Lagrangian perspective\nThe equilibrium distribution \\(f^{eq}\\) is given by the Maxwell-Boltzmann distribution\n\\[\nf_{eq}(\\mathbf{v}, \\mathbf{x}, t) = \\rho \\left( \\frac{1}{2\\pi RT} \\right)^{3/2} e^{-\\frac{v^2}{2RT}}\n                                     = \\rho \\left( \\frac{1}{2\\pi RT} \\right)^{3/2} e^{\\frac{-|\\mathbf{\\xi} - \\mathbf{u}|^2}{2RT}}\n\\]\nHere \\(\\mathbf{u}\\) is the macroscopic velocity, \\(\\xi\\) the absolute molecular velocity, and \\(\\mathbf{v}=\\xi-\\mathbf{u}\\) the relative molecular velocity, \\(R\\) is the kinetic gas constant and \\(T\\) the temperature.\nNote: While the BGK approximation is good enough to recover Navier-Stokes behaviour, it is not really a good approximation to the Boltzmann equation."
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html#lattice-boltzmann-method",
    "href": "posts/LatticeBoltzmannMethod/index.html#lattice-boltzmann-method",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "Lattice Boltzmann method",
    "text": "Lattice Boltzmann method\nTo solve the Boltzmann equation numerically, it needs to be discretized in space, time, and velocity. Time is discretized into time steps \\(\\Delta t\\) and space is discretized into a lattice with spacing \\(\\Delta x\\). The trick in the discretation of the velocity space is to choose a finite number of velocities in such a way that in a single time step \\(\\Delta t\\), particles can either - stay in their cell or - move to one of the neighbouring cells.\nThe D2Q9 scheme is a two-dimensional, nine-velocity model used in the LBM. The term “D2Q9” stands for two dimensions (D2) and nine discrete velocities (Q9). The scheme consists of a square lattice where each lattice node is connected to its neighbors via nine possible velocity vectors, including the zero vector (rest particle).\nThe nine discrete velocities in the D2Q9 scheme are typically represented as follows:\n\\[\n\\mathbf{e}_i =\n\\begin{cases}\n(0, 0) & \\text{for } i = 1 \\\\\n(0, 1), (1, 0), (0, -1), (-1, 0) & \\text{for } i = 2, 4, 6, 8 \\\\\n(1, 1), (1, -1), (-1, -1), (-11, 1) & \\text{for } i = 3, 5, 7, 9\n\\end{cases}\n\\]\nThese velocities correspond to the rest particle (i = 1), the particles moving to the nearest neighbors (i even), and the particles moving to the next-nearest neighbors (i odd).\nIn the LBM, each velocity vector is associated with a weight that reflects the probability of a particle moving along that vector. The weights must satisfy certain constraints to ensure mass and momentum conservation. The weights can be derived by requiring that the moments of the equilibrium distribution function and the discretized distribution functions agree.\nFor the D2Q9 scheme, these weights are\n\\[\nw_i =\n\\begin{cases}\n4/9 & \\text{for } i = 1 \\\\\n1/9 & \\text{for } i = 2, 4, 6, 8 \\\\\n1/36 & \\text{for } i = 3, 5, 7, 9\n\\end{cases}\n\\]\nIn LBM, the equilibrium distribution function \\(f_i^{eq}\\) is often approximated by a second-order Taylor expansion of the Maxwell-Boltzmann distribution\n\\[\nf_i^{\\text{eq}} = w_i \\rho \\left( 1 + \\frac{\\mathbf{e}_i \\cdot \\mathbf{u}}{c_s^2} + \\frac{(\\mathbf{e}_i \\cdot \\mathbf{u})^2}{2c_s^4} - \\frac{\\mathbf{u} \\cdot \\mathbf{u}}{2c_s^2} \\right)\n\\]\nwhere \\(w_i\\) are the weights and \\(c_s = \\sqrt{RT} = \\frac{1}{\\sqrt{3}} \\frac{\\Delta x}{\\Delta t}\\) is the speed of sound in the lattice. Our choice of velocities implies \\(\\frac{\\Delta x}{\\Delta t} = 1\\) and, therefore, \\(c_s = 1/3\\). The equilibrium particle distribution function can thus be written as\n\\[\nf_i^{\\text{eq}} = w_i \\rho \\left( 1 + 3 \\mathbf{e}_i \\cdot \\mathbf{u} + \\frac{9}{2} (\\mathbf{e}_i \\cdot \\mathbf{u})^2 - \\frac{3}{2}\\mathbf{u} \\cdot \\mathbf{u} \\right)\n\\]"
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html#no-slip-boundary-condition-bounce-back",
    "href": "posts/LatticeBoltzmannMethod/index.html#no-slip-boundary-condition-bounce-back",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "No-slip boundary condition (bounce back)",
    "text": "No-slip boundary condition (bounce back)\nAt the boundary surfaces, we impose a no-slip condition, i.e. the velocity at the boundary should be \\(0\\). In the LBM this can be achieved by setting the particle distribution function within the boundary to 0. In the streaming step at time \\(t\\), those discrete velocities that have a component towards the boundary are streamed into the boundary. Now bounce-back is applied within the boundary cells, i.e. all discrete velocity components are flipped. At time \\(t + \\Delta t\\), the flipped discrete velocities are streamed back out of the boundary. This leads to all discrete velocities that have a component towards to boundary to be approximately 0."
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html#julia-implementation",
    "href": "posts/LatticeBoltzmannMethod/index.html#julia-implementation",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "Julia implementation",
    "text": "Julia implementation\n\nusing CairoMakie\nset_theme!()\n\nconst Nx = 400\nconst Ny = 100\n\nrho0 = 100\ntau = 0.6\n\n\n## D2Q9 lattice\nconst Nl = 9\n\ncx = Int64[0, 0, 1, 1, 1, 0,-1,-1,-1]\ncy = Int64[0, 1, 1, 0,-1,-1,-1, 0, 1]\nw = Float32[4/9,1/9,1/36,1/9,1/36,1/9,1/36,1/9,1/36]\n\nflipdir_idx = [1, 6, 7, 8, 9, 2, 3, 4, 5]\n\ncx3d = reshape(cx, 1, 1, :)\ncy3d = reshape(cy, 1, 1, :)\n\n## cylinder geometry\nX = (1:Nx) * ones(Ny)'\nY = ones(Nx) * (1:Ny)'\ncylinder = (X .- Nx/4).^2 .+ (Y .- Ny/2).^2 .&lt; (Ny/6)^2\n    \n## initialise distributions\nF = ones(Float32, (Nx, Ny, Nl))  # discretized \nF .+= 0.01 * rand(Nx, Ny, Nl)    # add some noise\nF[:,:,4] .+= 1.3\n\nrho = sum(F; dims = 3)\nF .= F ./ rho * rho0\n\nF[cylinder, :] .= 0.0\n\nFcache = zeros(Float32, Nx, Ny, 1)\n;\n\nF contains the distribution function for the discretized velocities\nF[ix, iy, l] corresponds to cell (ix, iy) and velocity (cx[l], cy[l]).\nWhen streaming, the velocity F[ix, iy, l] is moved to F[ix + cx[l], iy + cy[l], l].\nLet’s draw the initial velocities around the cylinder\n\n## macroscopic variables\nrho = sum(F, dims = 3)\nux = sum(F .* cx3d, dims = 3) ./ rho\nuy = sum(F .* cy3d, dims = 3) ./ rho\n\nux[cylinder, 1] .= NaN\nuy[cylinder, 1] .= NaN\n\n\nfig = Figure(size = (800, 200))\nAxis(fig[1, 1])\narrows!(1:10:Nx, 1:10:Ny, \n        ux[1:10:Nx,1:10:Ny,1], \n        uy[1:10:Nx,1:10:Ny,1], arrowsize = 7, lengthscale = 50)\nfig\n\n\n\n\n\n\n\n\n\n\"\"\"\nstream distributions to neighbouring cells\n\"\"\"\nfunction stream!(F, cx, cy, Fcache)\n    Nx, Ny, Nl = size(F)\n    for l in 1:Nl\n        for ix in 1:Nx\n            nix = (ix - 1 + cx[l] + Nx) % Nx + 1\n            for iy in 1:Ny\n                niy = (iy - 1 + cy[l] + Ny) % Ny + 1\n                Fcache[nix, niy, 1] = F[ix, iy, l]\n            end\n        end\n        F[:,:,l] = Fcache\n    end \n    return nothing\nend\n\n\"\"\"\ncollide step within each cell to relax towords equilibrium distribution\n\"\"\"\nfunction collide!(F, cx, cy, Fcache)\n    Nx, Ny, Nl = size(F)\n    cx3d = reshape(cx, 1, 1, :)\n    cy3d = reshape(cy, 1, 1, :)\n\n    ## macroscopic variables\n    rho = sum(F, dims = 3)\n    ux = sum(F .* cx3d, dims = 3) ./ rho\n    uy = sum(F .* cy3d, dims = 3) ./ rho\n\n\n    ## F_eq\n    for l in 1:Nl\n        Fcache[:,:,1] = rho .* w[l] .* ( 1 .+ 3 .* (cx[l] .* ux .+ cy[l] .* uy)  .+ 9 .* (cx[l] .* ux .+ cy[l] .* uy).^2 ./ 2 .- 3*(ux.^2 .+ uy.^2) ./ 2 )\n\n        F[:,:,l] .+= 1/tau .* (Fcache .- F[:,:,l])\n    end\n    \n    return nothing\nend\n\ncollide!\n\n\n\nfunction simulate!(F, cx, cy, cylinder, Fcache, Niter)\n\n    for it in 1:Niter\n\n        # absorbing boundaries\n        # replace cx = -1\n        F[end, :, [7, 8, 9]] .= F[end-1, :, [7,8,9]]\n        # replace cx = +1\n        F[1, :, [3, 4, 5]] .= F[2, :, [3,4,5]]\n\n        stream!(F, cx, cy, Fcache)\n\n        ## no-slip boundary conditions (bounce-back)\n        ## flip directions within boundaries\n        boundaryF = F[cylinder, flipdir_idx]\n\n        collide!(F, cx, cy, Fcache)\n\n        F[cylinder,:] = boundaryF\n\n    end\n\n    return nothing\nend\n\nsimulate! (generic function with 1 method)\n\n\n\nsimulate!(F, cx, cy, cylinder, Fcache, 250)\n\n## macroscopic variables\nrho = sum(F, dims = 3)\nux = sum(F .* cx3d, dims = 3) ./ rho\nuy = sum(F .* cy3d, dims = 3) ./ rho\n\nux[cylinder, 1] .= 0\nuy[cylinder, 1] .= 0\n\nu = sqrt.(ux.^2 .+ uy .^ 2)[:,:,1]\n\nfig = Figure(;size = (800, 200))\na, p = heatmap(fig[1,1], u)\nColorbar(fig[1,2], p)\nfig\n\n\n\n\n\n\n\n\nCreate animation\n\nf = Figure(;size = (800, 200))\nax1 = Axis(f[1,1])\n\nrecord(f, \"latticeboltzmann.gif\", 1:100, framerate = 15) do i\n\n    simulate!(F, cx, cy, cylinder, Fcache, 100)\n\n    ## compute macroscopic variables\n    rho = sum(F, dims = 3)\n    ux = sum(F .* cx3d, dims = 3) ./ rho\n    uy = sum(F .* cy3d, dims = 3) ./ rho\n\n    ux[cylinder, 1] .= 0\n    uy[cylinder, 1] .= 0\n\n    u = sqrt.(ux.^2 .+ uy .^ 2)[:,:,1]\n    empty!(ax1)\n    heatmap!(ax1, u)\nend\n\n\"latticeboltzmann.gif\"\n\n\n\n\n\nLBM 2d flow around cylinder\n\n\n\nReferences\n\nhttps://github.com/pmocz/latticeboltzmann-python/\nhttps://github.com/Ceyron/machine-learning-and-simulation/blob/main/english/simulation_scripts/lattice_boltzmann_method_python_jax.py\nhttps://filelist.tudelft.nl/TNW/Afdelingen/Radiation%20Science%20and%20Technology/Research%20Groups/RPNM/Publications/BSc_Suzanne_Wetstein.pdf\nMora et al.(2019), A concise python implementation of the lattice Boltzmann method on HPC for geo-fluid flow URL"
  },
  {
    "objectID": "posts/BayesianLCA/index.html",
    "href": "posts/BayesianLCA/index.html",
    "title": "Bayesian Latent Class Analysis",
    "section": "",
    "text": "Bayesian Latent Class Analysis (BLCA) is a powerful statistical method used to classify subjects into unobserved (latent) groups based on their responses to observed variables. The method relies on the Bayesian framework to incorporate prior knowledge and manage uncertainty, making it robust and flexible for various applications, such as in social sciences, medicine, and marketing.\n\nBasic Concepts of Latent Class Analysis (LCA)\nLCA models assume that there is an underlying categorical variable (latent class) that explains the patterns of responses observed in the data. Each subject belongs to one of these latent classes, and the probability of each observed response depends on the latent class membership.\n\n\nBayesian Framework\nIn the Bayesian approach, we introduce prior distributions for the model parameters and update these priors with the observed data to obtain posterior distributions. This incorporation of prior knowledge can help guide the analysis, especially when data is sparse or noisy.\n\n\nGenerative model\n\n\n\n\n\n\n\nG\n\n\ncluster_N\n\n\n\ncluster_J\n\n\n\n\nalpha\n\nu\nc\n\n\n\npi\n\nπ\n\n\n\nalpha-&gt;pi\n\n\n\n\n\nbeta\n\nα\njk\n, \nβ\njk\n\n\n\ntheta\n\nθ\njk\n\n\n\nbeta-&gt;theta\n\n\n\n\n\nc\n\nc\ni\n\n\n\npi-&gt;c\n\n\n\n\n\ny\n\ny\nik\n\n\n\ntheta-&gt;y\n\n\n\n\n\nc-&gt;y\n\n\n\n\n\nN_label\ni = 1 .. N subjects\n\n\n\nJ_label\nk = 1 .. K questions\n\n\n\n\n\n\n\n\nFrom the diagram we can see that the joint distribution can be decomposed into the following factors\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) =\n    P(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) P(\\mathbf{c}|\\pi)\n    P(\\pi|u) P(\\mathbf{\\Theta}| \\alpha, \\beta)\n\\]\nAssume \\(N\\) subjects are distributed over \\(C\\) classes.\nThe Dirichlet distribution is the conjugate prior of the Categorical and Multinomial distributions. The mean is \\(\\pi_j = u_j / \\sum_j u_j\\) while the mode is \\((u_j - 1)/\\sum_j(u_j - 1)\\) for \\(u_j &gt; 1\\). It is the appropriate prior when we need to make a \\(C\\)-way choice. The prior probabilities for the class membership is assumed to be Dirichlet\n\\[\n\\pi \\sim \\mathrm{Dirichlet}(u_1, \\ldots, u_C) \\propto \\prod_{j=1}^C \\pi_j^{u_j - 1}\n\\]\nThe probability that subject \\(i\\) is in class \\(j\\) is given by\n\\[\nP(c_{i} = j | \\pi) = \\pi_j\n\\]\nGiven \\(\\pi\\), the class assignments for the \\(N\\) subjects are independent. Therefore,\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{i=1}^N P(c_i|\\pi)\n\\]\nIf we let \\(N_j\\) denote the number of subjects in class \\(j\\), this expression can be simplified to\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{j=1}^C \\pi_j^{N_j}\n\\]\nLet \\(\\Theta_{jk}\\) be the probability that a subject of class \\(j\\) answers question \\(k\\) positive. Then\n\\[\nP(y_{ik} = 1 | c_{i} = j, \\mathbf{\\Theta}) = \\Theta_{jk}\n\\]\nand \\[\nP(y_{ik} = 0 | c_{i} = j, \\mathbf{\\Theta}) = 1 - \\Theta_{jk}\n\\]\nAnswering a yes/no question is a two-way choice (or Bernoulli experiment). The Beta distribution is an appropriate prior for a two-way choice, and we assume \\(\\Theta_{jk}\\) to be Beta distributed\n\\[\nP(\\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K \\mathrm{Beta}(\\alpha_{jk}, \\beta_{jk}) \\propto \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n\\]\nLooking at the plate diagram, we can see that the \\(K\\) questions the \\(N\\) subjects answer are independent given it classes \\(\\mathbf{c}\\) and parameters \\(\\mathbf{\\Theta}\\). Therefore,\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n  \\prod_{i=1}^N \\prod_{k=1}^K  \\Theta_{c_i k}^{y_{ik}}(1-\\Theta_{c_i k})^{1-y_{ik}}\n\\]\nThis expression can be simplified by counting how often the factors \\(\\Theta_{jk}\\) and \\((1-\\Theta_{jk})\\) occur.\nLet \\(N_{jk}\\) denote number of times the question \\(k\\) was answered positive for members of class \\(j\\), then \\(N_j - N_{jk}\\) the number of times it was answered negative.\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j - N_{jk}}\n\\]\nPutting everything together, we end up with the following expression for the joint probability\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j -N_{jk}} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{N_j} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{u_j - 1} \\right)\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K\n        \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n    \\right)\n\\]\nSome rearrangement yields\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n\\left( \\prod_{j=1}^C  \\pi_j^{N_j + u_j - 1} \\right)\n\\left(\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk} + \\alpha_{jk} - 1} (1 - \\Theta_{jk})^{N_j - N_{jk} + \\beta_{jk} - 1}\n\\right)\n\\]\nThis shows that the posterior joint distribution factors into a Dirichlet posterior for \\(\\pi\\) and a product of Beta posteriors for \\(\\Theta\\).\nWe can generate data according to the above model using the following Julia code\n\nusing Distributions\n\nC = 2       # number of classes, j = 1 .. C\nK = 3       # number of questions, k = 1 .. K\nN = 1000     # number of subjects, i = 1 .. N\n\nπ = [0.2, 0.8]\nΘ = [0.1 0.3 0.7;\n     0.5 0.8 0.1]   \n\nc = rand(Categorical(π), N)\n\ngen = (;π, Θ, c)\n\ny = rand.(Bernoulli.(Θ[c, :]));\n\n\nusing CairoMakie\n\nf = Figure(;size=(600, 400))\n\nfor j in 1:K\n    hist(f[1,j], y[:, j]; axis = (;title = \"Question $j\"))\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nGibbs sampling\nGibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to approximate the posterior distributions of the model parameters. It iteratively samples from the conditional distributions of each parameter given the current values of the others.\nAbove we had derived an expression for the joint probability \\(P(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}, \\mathbf{\\Theta})\\). As \\(\\mathbf{y}\\) is observed, we only need to create conditional samples for \\(\\mathbf{c}\\), \\(\\mathbf{\\pi}\\), and \\(\\mathbf{\\Theta}\\).\n\\[\nP(\\mathbf{\\pi} |  \\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\mathrm{Dirichlet}(\\ldots, u_j + N_j, \\ldots)\n\\]\n\\[\nP(\\Theta_{jk} | \\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}) =\n    \\mathrm{Beta}(\\alpha_{jk} + N_{jk}, \\beta_{jk} + N_j -  N_{jk})\n\\]\nTo draw a sample for \\(c_i\\), we have to compute \\[\nP(c_i | \\mathbf{y}, \\mathbf{c}_{-i}, \\mathbf{\\Theta}, \\pi)\n\\]\nwhere \\(\\mathbf{c}_{-i}\\) denotes the vector of class membership for all subjects except \\(i\\). From the plate diagram, we can see that conditioned on \\(\\pi\\) and \\(\\mathrm{\\Theta}\\), \\(c_i\\) only depends on \\(\\pi\\), \\(\\mathrm{\\Theta}\\), and \\(y_{ik}\\). We have\n\\[\nP(c_i, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    P(c_i|\\pi) \\prod_{k=1}^K P(y_{ik}|c_i, \\mathrm{\\Theta})\n\\]\n\\[\nP(c_i = j, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    \\pi_j \\prod_{k=1}^K \\Theta_{jk}^{y_{ik}}(1 - \\Theta_{jk})^{(1-y_{ik})}\n\\]\nThe Gibbs sampling algorithm can be summarized as follows:\n\nInitialize \\(\\mathbf{\\Theta}\\), \\(\\pi\\), \\(\\mathbf{c}\\)\ncompute \\(N_j\\), \\(N_{jk}\\)\nfor iter in 1:max_iter\n\nfor i in 1:N\n\nsample \\(c_i\\), and update \\(N_j\\) and \\(N_{jk}\\)\n\nsample \\(\\mathbf{\\pi}\\) using \\(N_j\\)\nsample \\(\\mathbf{\\Theta}\\) using \\(N_{jk}\\)\n\n\n\nusing DataFrames\nusing StatsBase: sample, Weights\nusing StatsFuns: logsumexp\n\n# Initialise\nu = ones(C)\nα = ones(C,K)\nβ = ones(C,K)\n\nπ = rand(Dirichlet(u))\nΘ = rand.(Beta.(α, β))\nc = rand(Categorical(π), N)\n\nN_j = zeros(C)      # number of subjects in class\nN_jk = zeros(C, K)  # number of positive answers for class j and question k\n\nfor i in 1:N\n    j = c[i]\n    N_j[j] += 1\n    for k in 1:K\n        N_jk[j, k] += y[i,k]\n    end\nend\n\nmax_iter = 11000\nburnin = 1000\nthinning = 10\n\nsamples = DataFrame()\n\nfor iter in 1:max_iter\n    # sample c[i]\n    for i in 1:N\n        # remove c[i]from N_j and N_jk\n        N_j[c[i]] -= 1\n        for k in 1:K\n            N_jk[c[i], k] -= y[i,k]\n        end\n        # compute p(c_i = j)\n        log_p_c = zeros(Float64, C)\n        for j in 1:C\n            log_p_c[j] += log(π[j])\n            for k in 1:K\n                log_p_c[j] += y[i,k]*log(Θ[j,k]) + (1.0 - y[i,k])*log(1.0 - Θ[j,k])\n            end\n        end\n        p_c = exp.(log_p_c .- logsumexp(log_p_c))\n        # and sample c[i]\n        c[i] = sample(1:K, Weights(p_c))\n        # add c[i] to N_j and N_jk\n        N_j[c[i]] += 1\n        for k in 1:K\n            N_jk[c[i], k] += y[i,k]\n        end\n    end\n\n    # sample \\pi\n    π = rand(Dirichlet(u .+ N_j))\n    \n    # sample \\Theta\n    Θ = rand.(Beta.(α .+ N_jk, β .+ N_j .- N_jk))\n\n    if iter &gt; burnin && (iter - burnin) % thinning == 0\n        push!(samples, (;π, Θ, c = copy(c)))\n    end\n\nend\n\nLet’s look at trace plots of the sampled parameters:\n\nf = Figure()\n\nlines(f[1:2,1], [x[1] for x in samples.π]; axis = (;title = \"π\"))\nlines!(f[1:2,1], [x[2] for x in samples.π])\nylims!(0,1)\n\nfor j in 1:C\n    for k in 1:K\n        lines(f[j,k+1], [x[j, k] for x in samples.Θ]; axis = (;title = \"Θ[$j,$k]\"))\n        ylims!(0,1)\n    end\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nExpectation-Maximization (EM)\nThe Expectation-Maximization method results in point estimates of the parameters \\(\\mathbf{\\Theta}\\) and \\(\\pi\\), and probability distributions of latent variables \\(\\mathbf{c}\\).\nSimilar to the Gibbs sampling, we compute for each subject the class membership probabilities \\(P(\\mathbf{c}|\\mathbf{\\Theta}, \\pi)\\). EM uses the joint class distributions to compute the MLE of parameters \\(\\pi\\) and \\(\\mathbf{\\Theta}\\).\n\\[\n\\pi_j = \\frac{\\sum_{i=1}^N P(c_i = j|\\mathbf{\\Theta}, \\pi)}{N} = \\frac{N_j}{N}\n\\]\n\\[\n\\Theta_{jk} = \\frac{\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi) y_{ik}}\n                    {\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi)} =\n                    \\frac{N_{jk}}{N_j}\n\\]\nIf one also incorporates prior counts (\\(u\\) and \\(\\alpha_{jk}\\), \\(\\beta_{jk]}\\) above), the MLE is replaced by a MAP estimate.\n\\[\n\\pi_j = \\frac{N_j + u_j - 1}{N + \\sum_{j=1}^C (u_j - 1)}\n\\]\n\\[\n\\Theta_{jk} = \\frac{N_{jk} + \\alpha_{jk} - 1}{N_j + \\alpha_{jk} + \\beta_{jk} - 2}\n\\]\n\n\nReferences\n\nResnik et al.(2010), Gibbs sampling for the uninitiated, http://users.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf\nLi et al.(2019), Bayesian Latent Class Analysis Tutorial https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6364555/, but here the notation seems seriously flawed"
  },
  {
    "objectID": "posts/TensorNotation/index.html",
    "href": "posts/TensorNotation/index.html",
    "title": "Tensor notation",
    "section": "",
    "text": "Vectors\nIn tensor notation, a vector is written as\n\\[\n\\mathbf{u} = u_i\n\\]\nwhere \\(i = 1,2,3\\), corresponding to the \\(x\\), \\(y\\), and \\(z\\) component, respectively.\n\\[\n\\mathbf{u} + \\mathbf{v} = u_i + v_i\n\\]\n\n\nEinstein summation\nAny index appearing twice is automatically summed from 1 to 3. This is called Einstein summation. Any index can at most appear twice.\n\n\nDot (inner) product\nUsing Einstein summation, the dot (inner) product of two vectors is\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_i v_i = u_1 v_1 + u_2 v_2 + u_3 v_3\n\\]\n\n\nDyadic (outer) product\nThe dyadic (outer) product of two vectors is\n\\[\n\\mathbf{u} \\otimes \\mathbf{v} = u_i v_j =\n\\left[\n\\begin{matrix}\nu_1 v_1 & u_1 v_2 & u_1 v_3 \\cr\nu_2 v_1 & u_2 v_2 & u_2 v_3 \\cr\nu_3 v_1 & u_3 v_2 & u_3 v_3\n\\end{matrix}\n\\right]\n\\]\n\n\nDifferentiation with respect to time\nDifferentiation with respect to time can be written as\n\\[\n\\frac{\\text{d}\\mathbf{x}}{\\text{d}t} = \\mathbf{\\dot{x}} = \\dot{x_i} = x_{i|t}\n\\]\n\n\nDifferentiation with respect to space\nThe gradient of a scalar function is given by\n\\[\n\\nabla \\phi = \\partial_i \\phi = \\phi_{|i}\n\\]\nThe component-wise spatial derivative of a vector\n\\[\n\\frac{\\partial \\mathbf{u}}{\\partial x_j} = u_{i|j}\n\\]\nThe divergence of a vector is\n\\[\n\\text{div} \\ \\mathbf{u} = \\nabla \\cdot \\mathbf{u} = u_{i|i}\n\\]\n\n\nKronecker delta\nThe Kronecker delta is defined as\n\\[\n\\delta_{ij} = \\begin{cases}\n\\begin{matrix}\n1 & i = j \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nKronecker delta is also often called the substitution operator, as\n\\[\nu_i \\delta_{ij} = u_j\n\\]\nThe Kronecker delta often occurs when one deals with spatial derivatives of position\n\\[\n\\frac{\\partial x_i}{\\partial x_j} = \\partial_j x_i = x_{i|j} = \\delta_{ij}\n\\]\n\n\nLevi-Civita alternating tensor\n\\[\n\\epsilon_{ijk} = \\begin{cases}\n\\begin{matrix}\n1 & ijk \\in \\{123, 231, 312\\} \\cr\n-1 & ijk \\in \\{132, 213, 321\\} \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nSometimes the following equality can be useful to simplify expressions:\n\\[\n\\epsilon_{ijk} \\epsilon_{imn} = \\delta_{jm}\\delta_{kn}-\\delta_{jn}\\delta_{km}\n\\]\n\n\nRotation or curl of a vector\n\\[\n\\text{rot} \\ \\mathbf{u} = \\nabla \\times \\mathbf{u} = \\epsilon_{ijk} u_{k|j}\n\\]\nReferences: [https://www.continuummechanics.org]"
  },
  {
    "objectID": "posts/DoublePendulum/index.html",
    "href": "posts/DoublePendulum/index.html",
    "title": "Double pendulum",
    "section": "",
    "text": "The double pendulum in Julia.\nFirst, lets define the Lagrangian \\(L = T - V\\), where \\(T\\) is the kinetic energy of the system and \\(V\\) the potential energy.\nusing CairoMakie\nusing Symbolics\n\nset_theme!()\nDefine all the variables needed to specify the problem\n@variables g, t, m₁, m₂, l₁, l₂, Θ₁(t), Θ₂(t);\nThe kinetic and potential energy are easier to express in cartesian coordinates.\nx₁ = l₁ * sin(Θ₁)\ny₁ = - l₁ * cos(Θ₁)\nx₂ = x₁ + l₂ * sin(Θ₂)\ny₂ = y₁ - l₂ * cos(Θ₂);\nDefine kinetic and potential energy in cartesian coordinates\nDt = Differential(t)\n\nẋ₁ = Dt(x₁)\nẏ₁ = Dt(y₁)\nẋ₂ = Dt(x₂)\nẏ₂ = Dt(y₂)\n\nT₁ = 1/2 * m₁ * (ẋ₁^2 + ẏ₁^2)\nT₂ = 1/2 * m₂ * (ẋ₂^2 + ẏ₂^2)\nT = T₁ + T₂\n\nV₁ = m₁ * g * y₁\nV₂ = m₂ * g * y₂\nV = V₁ + V₂\n\nL = T - V;\nsimplify(expand_derivatives(L))\n\n\\[ \\begin{equation}\ng l_1 m_1 \\cos\\left( \\Theta_1\\left( t \\right) \\right) - g \\left(  - l_1 \\cos\\left( \\Theta_1\\left( t \\right) \\right) - l_2 \\cos\\left( \\Theta_2\\left( t \\right) \\right) \\right) m_2 + 0.5 \\left( \\left( \\frac{\\mathrm{d} \\Theta_1\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} \\cos^{2}\\left( \\Theta_1\\left( t \\right) \\right) l_1^{2} + \\sin^{2}\\left( \\Theta_1\\left( t \\right) \\right) \\left( \\frac{\\mathrm{d} \\Theta_1\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} l_1^{2} \\right) m_1 + 0.5 \\left( \\left( l_1 \\cos\\left( \\Theta_1\\left( t \\right) \\right) \\frac{\\mathrm{d} \\Theta_1\\left( t \\right)}{\\mathrm{d}t} + l_2 \\cos\\left( \\Theta_2\\left( t \\right) \\right) \\frac{\\mathrm{d} \\Theta_2\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} + \\left( l_1 \\frac{\\mathrm{d} \\Theta_1\\left( t \\right)}{\\mathrm{d}t} \\sin\\left( \\Theta_1\\left( t \\right) \\right) + l_2 \\frac{\\mathrm{d} \\Theta_2\\left( t \\right)}{\\mathrm{d}t} \\sin\\left( \\Theta_2\\left( t \\right) \\right) \\right)^{2} \\right) m_2\n\\end{equation}\n\\]\nThe equations of motion are obtained from the Lagrange eqautions for \\(\\Theta_1\\) and \\(\\Theta_2\\)\n\\[\n\\frac{\\partial L}{\\partial \\Theta_1} - \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\partial L}{\\partial \\dot{\\Theta}_1} = 0\n\\]\n\\[\n\\frac{\\partial L}{\\partial \\Theta_2} - \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\partial L}{\\partial \\dot{\\Theta}_2} = 0\n\\]\nΘ̇₁ = Dt(Θ₁)\nΘ̇₂ = Dt(Θ₂)\n\nΘ̈₁ = Dt(Θ̇₁)\nΘ̈₂ = Dt(Θ̇₂)\n\nDΘ₁ = Differential(Θ₁)\nDΘ₂ = Differential(Θ₂)\n\nDΘ̇₁ = Differential(Θ̇₁)\nDΘ̇₂ = Differential(Θ̇₂)\n\nL = simplify(expand_derivatives(L))\n\nLE₁ = simplify(expand_derivatives(DΘ₁(L) - Dt(DΘ̇₁(L))); expand = true)\nLE₂ = simplify(expand_derivatives(DΘ₂(L) - Dt(DΘ̇₂(L))); expand = true);\nWe need to run expand_derivatives for LE1 and LE2 to be able to solve the equation! Otherwise we get a singular exception!\nNow solve for \\(\\ddot{\\Theta}_1\\) and \\(\\ddot{\\Theta}_2\\)\ndu1, du2 = simplify.(symbolic_linear_solve([LE₁ ~ 0, LE₂ ~ 0], [Θ̈₁, Θ̈₂]))\n\n2-element Vector{SymbolicUtils.BasicSymbolic{Real}}:\n (-g*m₁*(cos(Θ₂(t))^2)*sin(Θ₁(t)) - g*m₁*sin(Θ₁(t))*(sin(Θ₂(t))^2) + g*m₂*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₂(t)) - g*m₂*(cos(Θ₂(t))^2)*sin(Θ₁(t)) + l₁*m₂*(cos(Θ₁(t))^2)*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*sin(Θ₂(t)) - l₁*m₂*cos(Θ₁(t))*(cos(Θ₂(t))^2)*(Differential(t)(Θ₁(t))^2)*sin(Θ₁(t)) + l₁*m₂*cos(Θ₁(t))*(Differential(t)(Θ₁(t))^2)*sin(Θ₁(t))*(sin(Θ₂(t))^2) - l₁*m₂*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^2)*sin(Θ₂(t)) + l₂*m₂*cos(Θ₁(t))*(cos(Θ₂(t))^2)*(Differential(t)(Θ₂(t))^2)*sin(Θ₂(t)) + l₂*m₂*cos(Θ₁(t))*(Differential(t)(Θ₂(t))^2)*(sin(Θ₂(t))^3) - l₂*m₂*(cos(Θ₂(t))^3)*(Differential(t)(Θ₂(t))^2)*sin(Θ₁(t)) - l₂*m₂*cos(Θ₂(t))*(Differential(t)(Θ₂(t))^2)*sin(Θ₁(t))*(sin(Θ₂(t))^2)) / (l₁*(m₁*(cos(Θ₁(t))^2)*(cos(Θ₂(t))^2) + m₁*(cos(Θ₁(t))^2)*(sin(Θ₂(t))^2) + m₁*(cos(Θ₂(t))^2)*(sin(Θ₁(t))^2) + m₁*(sin(Θ₁(t))^2)*(sin(Θ₂(t))^2) + m₂*(cos(Θ₁(t))^2)*(sin(Θ₂(t))^2) - (2//1)*m₂*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₁(t))*sin(Θ₂(t)) + m₂*(cos(Θ₂(t))^2)*(sin(Θ₁(t))^2)))\n (-g*m₁*(cos(Θ₁(t))^2)*sin(Θ₂(t)) + g*m₁*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₁(t)) - g*m₂*(cos(Θ₁(t))^2)*sin(Θ₂(t)) + g*m₂*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₁(t)) - l₁*m₁*(cos(Θ₁(t))^3)*(Differential(t)(Θ₁(t))^2)*sin(Θ₂(t)) + l₁*m₁*(cos(Θ₁(t))^2)*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*sin(Θ₁(t)) - l₁*m₁*cos(Θ₁(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^2)*sin(Θ₂(t)) + l₁*m₁*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^3) - l₁*m₂*(cos(Θ₁(t))^3)*(Differential(t)(Θ₁(t))^2)*sin(Θ₂(t)) + l₁*m₂*(cos(Θ₁(t))^2)*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*sin(Θ₁(t)) - l₁*m₂*cos(Θ₁(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^2)*sin(Θ₂(t)) + l₁*m₂*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^3) - l₂*m₂*(cos(Θ₁(t))^2)*cos(Θ₂(t))*(Differential(t)(Θ₂(t))^2)*sin(Θ₂(t)) + l₂*m₂*cos(Θ₁(t))*(cos(Θ₂(t))^2)*(Differential(t)(Θ₂(t))^2)*sin(Θ₁(t)) - l₂*m₂*cos(Θ₁(t))*(Differential(t)(Θ₂(t))^2)*sin(Θ₁(t))*(sin(Θ₂(t))^2) + l₂*m₂*cos(Θ₂(t))*(Differential(t)(Θ₂(t))^2)*(sin(Θ₁(t))^2)*sin(Θ₂(t))) / (l₂*m₁*(cos(Θ₁(t))^2)*(cos(Θ₂(t))^2) + l₂*m₁*(cos(Θ₁(t))^2)*(sin(Θ₂(t))^2) + l₂*m₁*(cos(Θ₂(t))^2)*(sin(Θ₁(t))^2) + l₂*m₁*(sin(Θ₁(t))^2)*(sin(Θ₂(t))^2) + l₂*m₂*(cos(Θ₁(t))^2)*(sin(Θ₂(t))^2) - (2//1)*l₂*m₂*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₁(t))*sin(Θ₂(t)) + l₂*m₂*(cos(Θ₂(t))^2)*(sin(Θ₁(t))^2))\nand create a Julia function that can be used in ODEProblem.\nu̇ = [Θ̇₁; Θ̇₂; du1; du2]\n\ndu, du! = build_function(u̇, [Θ₁, Θ₂, Θ̇₁, Θ̇₂],  [m₁, m₂, l₁, l₂, g], t; expression = Val{false});\nNow we can solve the ODE system numerically\nusing DifferentialEquations\n\ntspan = (0, 40)\nparams = [2.0, 1.0, 2.0, 1.0, 9.81]\ninit = [1, -3, -1, 5]\n\nprob = ODEProblem(du, init, tspan, params);\nsol = solve(prob);\nti = range(tspan...; length = 1001)\ntheta1 = [sol(t)[1] for t in ti]\ntheta2 = [sol(t)[2] for t in ti]\n\nf = Figure()\nlines(f[1,1], ti, theta1)\nlines!(f[1,1], ti, theta2)\n\nf"
  },
  {
    "objectID": "posts/DoublePendulum/index.html#animation",
    "href": "posts/DoublePendulum/index.html#animation",
    "title": "Double pendulum",
    "section": "Animation",
    "text": "Animation\n\nl1 = 2.0\nl2 = 1.0\n\nx1 = l1 * sin.(theta1)\ny1 = -l1 * cos.(theta1)\nx2 = x1 .+ l2 * sin.(theta2)\ny2 = y1 .- l2 * cos.(theta2)\n\nf = Figure()\n\nax1 = Axis(f[1,1], xlabel = L\"t\", ylabel = L\"\\Theta\")\nxlims!(ax1, tspan...)\nylims!(ax1, \n    1.1 * minimum(vcat(theta1, theta2)),\n    1.1 * maximum(vcat(theta1, theta2)))\n\nax2 = Axis(f[1,2]; aspect = 1)\nhidedecorations!(ax2)\nhidespines!(ax2)\nl = 1.1*(l1 + l2)\nxlims!(ax2, -l,l)\nylims!(ax2, -l,l)\n\nrecord(f, \"double_pendulum_animation.gif\", 1:length(ti), framerate = 30) do i\n\n    empty!(ax1)\n    lines!(ax1, ti[1:i], theta1[1:i]; color = :blue)\n    lines!(ax1, ti[1:i], theta2[1:i]; color = :red)\n\n    empty!(ax2)\n \n    lines!(ax2, [0, x1[i], x2[i]], [0, y1[i], y2[i]]; color = :blue)\n    lines!(ax2, x2[1:i], y2[1:i]; color = (:red, 0.1))\n    scatter!(ax2, [x1[i], x2[i]], [y1[i], y2[i]]; markersize = 20, color = [:blue, :red])\n\nend;\n\n\n\n\nDouble pendulum animation"
  },
  {
    "objectID": "posts/DoublePendulum/index.html#references",
    "href": "posts/DoublePendulum/index.html#references",
    "title": "Double pendulum",
    "section": "References",
    "text": "References\nsee double-pendulum.jl here: https://www.phys.uconn.edu/~rozman/Courses/P3101_22F/downloads/\nsee https://cooperrc.github.io/Julia-learning/day_06.html"
  },
  {
    "objectID": "posts/TuringIntro/index.html",
    "href": "posts/TuringIntro/index.html",
    "title": "Introduction to Turing.jl",
    "section": "",
    "text": "Turing.jl is a probabilistic proramming language that let’s us define a generative model and does inference."
  },
  {
    "objectID": "posts/TuringIntro/index.html#coin-example",
    "href": "posts/TuringIntro/index.html#coin-example",
    "title": "Introduction to Turing.jl",
    "section": "1 Coin example",
    "text": "1 Coin example\nLet’s flip a biased coin a hundred times.\n\nusing Turing\nusing DataFrames\ncoin_flips = rand(Bernoulli(0.7), 100);\n\nIn order to do inference, we need to specify a model.\n\\[\n\\begin{align*}\np & \\sim Beta(1,1) \\\\\ncoinflip & \\sim Bernoulli(p)\n\\end{align*}\n\\]\n\n@model function coin(data)\n  p ~ Beta(1,1)\n  for i in eachindex(data) \n    data[i] ~ Bernoulli(p)\n  end\nend\n\ncoin (generic function with 2 methods)\n\n\n\n1.1 Sample from prior\n\nprior = sample(coin(coin_flips), Prior(), 1000)\n\nSampling:   1%|▍                                        |  ETA: 0:00:10Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00\n\n\n\nChains MCMC chain (1000×2×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 1.28 seconds\nCompute duration  = 1.28 seconds\nparameters        = p\ninternals         = lp\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk    ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64    Float64     Float64   Float64    ⋯\n           p    0.4968    0.2901    0.0097   907.9966   1119.3737    1.0007    ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           p    0.0279    0.2520    0.4982    0.7485    0.9760\n\n\n\n\nAn overview of the chain can be obtained using summarystats or quantile. Or describe, which returns both.\n\nsummarystats(prior)\n\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk    ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64    Float64     Float64   Float64    ⋯\n           p    0.4968    0.2901    0.0097   907.9966   1119.3737    1.0007    ⋯\n                                                                1 column omitted\n\n\n\n\n\nDataFrame(prior)[1:10, :]\n\n\n10×4 DataFrame\n\n\n\nRow\niteration\nchain\np\nlp\n\n\n\nInt64\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n1\n1\n0.0714829\n0.0\n\n\n2\n2\n1\n0.544822\n0.0\n\n\n3\n3\n1\n0.388708\n0.0\n\n\n4\n4\n1\n0.515891\n0.0\n\n\n5\n5\n1\n0.947001\n0.0\n\n\n6\n6\n1\n0.221275\n0.0\n\n\n7\n7\n1\n0.56722\n0.0\n\n\n8\n8\n1\n0.93412\n0.0\n\n\n9\n9\n1\n0.714414\n0.0\n\n\n10\n10\n1\n0.991216\n0.0\n\n\n\n\n\n\n\n\n\n1.2 Sample from posterior\n\nposterior = sample(coin(coin_flips), NUTS(), 1000)\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\nChains MCMC chain (1000×13×1 Array{Float64, 3}):\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 2.58 seconds\nCompute duration  = 2.58 seconds\nparameters        = p\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.7523    0.0431    0.0022   395.3742   640.6825    1.0024     ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           p    0.6591    0.7244    0.7545    0.7828    0.8306\n\n\n\n\n\nsummarystats(posterior)\n\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.7523    0.0431    0.0022   395.3742   640.6825    1.0024     ⋯\n                                                                1 column omitted\n\n\n\n\n\nDataFrame(posterior)\n\n\n1000×15 DataFrame975 rows omitted\n\n\n\nRow\niteration\nchain\np\nlp\nn_steps\nis_accept\nacceptance_rate\nlog_density\nhamiltonian_energy\nhamiltonian_energy_error\nmax_hamiltonian_energy_error\ntree_depth\nnumerical_error\nstep_size\nnom_step_size\n\n\n\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n501\n1\n0.759026\n-56.807\n1.0\n1.0\n1.0\n-56.807\n57.156\n-0.472585\n-0.472585\n1.0\n0.0\n1.69751\n1.69751\n\n\n2\n502\n1\n0.762644\n-56.8191\n1.0\n1.0\n0.993468\n-56.8191\n56.8221\n0.00655318\n0.00655318\n1.0\n0.0\n1.69751\n1.69751\n\n\n3\n503\n1\n0.743272\n-56.8388\n3.0\n1.0\n0.862735\n-56.8388\n57.0165\n-0.00115495\n0.257566\n2.0\n0.0\n1.69751\n1.69751\n\n\n4\n504\n1\n0.721926\n-57.0858\n1.0\n1.0\n0.865705\n-57.0858\n57.119\n0.144211\n0.144211\n1.0\n0.0\n1.69751\n1.69751\n\n\n5\n505\n1\n0.736832\n-56.8895\n1.0\n1.0\n1.0\n-56.8895\n57.0332\n-0.115711\n-0.115711\n1.0\n0.0\n1.69751\n1.69751\n\n\n6\n506\n1\n0.732667\n-56.9334\n3.0\n1.0\n0.988378\n-56.9334\n56.9504\n0.0258694\n0.0258694\n2.0\n0.0\n1.69751\n1.69751\n\n\n7\n507\n1\n0.732667\n-56.9334\n1.0\n1.0\n0.00201923\n-56.9334\n60.7974\n0.0\n6.20504\n1.0\n0.0\n1.69751\n1.69751\n\n\n8\n508\n1\n0.772381\n-56.8894\n3.0\n1.0\n0.89873\n-56.8894\n57.0725\n-0.00419038\n0.193076\n2.0\n0.0\n1.69751\n1.69751\n\n\n9\n509\n1\n0.750871\n-56.8068\n3.0\n1.0\n0.877653\n-56.8068\n57.0243\n-0.0636158\n0.218064\n2.0\n0.0\n1.69751\n1.69751\n\n\n10\n510\n1\n0.78196\n-57.0152\n3.0\n1.0\n0.651191\n-57.0152\n57.4571\n0.174659\n0.646505\n2.0\n0.0\n1.69751\n1.69751\n\n\n11\n511\n1\n0.750547\n-56.8075\n3.0\n1.0\n0.779474\n-56.8075\n57.275\n-0.173653\n0.460218\n2.0\n0.0\n1.69751\n1.69751\n\n\n12\n512\n1\n0.781637\n-57.01\n3.0\n1.0\n0.660978\n-57.01\n57.4344\n0.168905\n0.620116\n2.0\n0.0\n1.69751\n1.69751\n\n\n13\n513\n1\n0.725931\n-57.0225\n3.0\n1.0\n0.700705\n-57.0225\n57.5386\n-0.0868621\n0.720277\n2.0\n0.0\n1.69751\n1.69751\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n989\n1489\n1\n0.758519\n-56.8059\n1.0\n1.0\n1.0\n-56.8059\n56.9475\n-0.155596\n-0.155596\n1.0\n0.0\n1.69751\n1.69751\n\n\n990\n1490\n1\n0.644345\n-59.688\n1.0\n1.0\n0.175018\n-59.688\n59.7404\n1.74286\n1.74286\n1.0\n0.0\n1.69751\n1.69751\n\n\n991\n1491\n1\n0.719935\n-57.1201\n1.0\n1.0\n1.0\n-57.1201\n58.4524\n-1.64894\n-1.64894\n1.0\n0.0\n1.69751\n1.69751\n\n\n992\n1492\n1\n0.785131\n-57.0698\n3.0\n1.0\n1.0\n-57.0698\n57.0874\n-0.0355982\n-0.161325\n2.0\n0.0\n1.69751\n1.69751\n\n\n993\n1493\n1\n0.730561\n-56.9589\n3.0\n1.0\n0.993926\n-56.9589\n57.0753\n-0.0747626\n-0.0747626\n2.0\n0.0\n1.69751\n1.69751\n\n\n994\n1494\n1\n0.747573\n-56.8169\n3.0\n1.0\n1.0\n-56.8169\n56.8768\n-0.0821619\n-0.0853442\n2.0\n0.0\n1.69751\n1.69751\n\n\n995\n1495\n1\n0.786545\n-57.0963\n3.0\n1.0\n0.610468\n-57.0963\n57.5842\n0.236467\n0.715627\n2.0\n0.0\n1.69751\n1.69751\n\n\n996\n1496\n1\n0.83268\n-58.7953\n3.0\n1.0\n0.580151\n-58.7953\n58.9109\n0.79187\n1.24669\n2.0\n0.0\n1.69751\n1.69751\n\n\n997\n1497\n1\n0.770938\n-56.8754\n1.0\n1.0\n1.0\n-56.8754\n57.8867\n-0.931971\n-0.931971\n1.0\n0.0\n1.69751\n1.69751\n\n\n998\n1498\n1\n0.794306\n-57.2662\n1.0\n1.0\n0.81897\n-57.2662\n57.3073\n0.199708\n0.199708\n1.0\n0.0\n1.69751\n1.69751\n\n\n999\n1499\n1\n0.836934\n-59.0468\n1.0\n1.0\n0.445731\n-59.0468\n59.2279\n0.80804\n0.80804\n1.0\n0.0\n1.69751\n1.69751\n\n\n1000\n1500\n1\n0.791469\n-57.1993\n3.0\n1.0\n0.866502\n-57.1993\n58.3543\n-0.84452\n-1.11814\n2.0\n0.0\n1.69751\n1.69751\n\n\n\n\n\n\n\n\n\n1.3 Prior predictive check\nHere we pass in a vector containing missing. Sampling then generates observations.\n\nobservations = Vector{Missing}(missing, length(coin_flips))\nprior_check = predict(coin(observations), prior)\n\n\nChains MCMC chain (1000×100×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = data[1], data[2], data[3], data[4], data[5], data[6], data[7], data[8], data[9], data[10], data[11], data[12], data[13], data[14], data[15], data[16], data[17], data[18], data[19], data[20], data[21], data[22], data[23], data[24], data[25], data[26], data[27], data[28], data[29], data[30], data[31], data[32], data[33], data[34], data[35], data[36], data[37], data[38], data[39], data[40], data[41], data[42], data[43], data[44], data[45], data[46], data[47], data[48], data[49], data[50], data[51], data[52], data[53], data[54], data[55], data[56], data[57], data[58], data[59], data[60], data[61], data[62], data[63], data[64], data[65], data[66], data[67], data[68], data[69], data[70], data[71], data[72], data[73], data[74], data[75], data[76], data[77], data[78], data[79], data[80], data[81], data[82], data[83], data[84], data[85], data[86], data[87], data[88], data[89], data[90], data[91], data[92], data[93], data[94], data[95], data[96], data[97], data[98], data[99], data[100]\ninternals         = \nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk   ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64     Float64    Float64   Float64    ⋯\n     data[1]    0.4880    0.5001    0.0167    899.6356        NaN    1.0000    ⋯\n     data[2]    0.4760    0.4997    0.0168    881.3036        NaN    1.0006    ⋯\n     data[3]    0.4930    0.5002    0.0155   1044.2915        NaN    0.9999    ⋯\n     data[4]    0.4960    0.5002    0.0157   1015.6422        NaN    0.9991    ⋯\n     data[5]    0.5120    0.5001    0.0163    937.3570        NaN    0.9990    ⋯\n     data[6]    0.5090    0.5002    0.0176    810.2290        NaN    1.0064    ⋯\n     data[7]    0.5070    0.5002    0.0170    862.2568        NaN    0.9992    ⋯\n     data[8]    0.4750    0.4996    0.0150   1102.2497        NaN    0.9990    ⋯\n     data[9]    0.5000    0.5003    0.0164    930.8721        NaN       NaN    ⋯\n    data[10]    0.4920    0.5002    0.0161    967.3134        NaN    0.9996    ⋯\n    data[11]    0.4870    0.5001    0.0155   1043.3817        NaN    0.9991    ⋯\n    data[12]    0.4970    0.5002    0.0165    915.8056        NaN    1.0028    ⋯\n    data[13]    0.4990    0.5002    0.0161    965.0103        NaN    1.0004    ⋯\n    data[14]    0.5130    0.5001    0.0154   1056.3191        NaN    1.0002    ⋯\n    data[15]    0.4910    0.5002    0.0155   1047.9404        NaN    0.9995    ⋯\n    data[16]    0.5040    0.5002    0.0158   1001.1275        NaN    1.0006    ⋯\n    data[17]    0.5060    0.5002    0.0166    905.9282        NaN    0.9991    ⋯\n    data[18]    0.5110    0.5001    0.0159    989.0216        NaN    0.9991    ⋯\n    data[19]    0.4990    0.5002    0.0176    810.9312        NaN    1.0028    ⋯\n    data[20]    0.4700    0.4993    0.0179    779.7506        NaN    0.9996    ⋯\n    data[21]    0.5010    0.5002    0.0186    727.0646        NaN    0.9997    ⋯\n    data[22]    0.4990    0.5002    0.0161    964.4288        NaN    1.0034    ⋯\n    data[23]    0.4810    0.4999    0.0152   1085.0622        NaN    0.9992    ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮          ⋮          ⋮       ⋱\n                                                    1 column and 77 rows omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n     data[1]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[2]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[3]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[4]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[5]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[6]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[7]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[8]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[9]    0.0000    0.0000    0.5000    1.0000    1.0000\n    data[10]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[11]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[12]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[13]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[14]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[15]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[16]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[17]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[18]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[19]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[20]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[21]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[22]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[23]    0.0000    0.0000    0.0000    1.0000    1.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                 77 rows omitted\n\n\n\n\nprior_check is a Chains instance. To convert to an Array, use\n\nArray(prior_check, [:parameters])\n\n1000×100 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 1.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0     1.0  1.0  0.0  1.0  0.0  0.0  1.0\n 0.0  1.0  0.0  0.0  1.0  1.0  0.0  0.0     1.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0     1.0  0.0  1.0  1.0  0.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     0.0  1.0  1.0  0.0  1.0  1.0  1.0\n 1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  …  1.0  0.0  0.0  1.0  0.0  0.0  1.0\n 1.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0     1.0  1.0  1.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  0.0  1.0  0.0  1.0  1.0  0.0     0.0  0.0  1.0  0.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0     0.0  1.0  1.0  0.0  1.0  1.0  1.0\n 0.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0     1.0  1.0  1.0  1.0  0.0  1.0  1.0\n ⋮                        ⋮              ⋱            ⋮                   \n 0.0  1.0  1.0  0.0  0.0  0.0  1.0  1.0     0.0  0.0  1.0  0.0  0.0  0.0  1.0\n 1.0  0.0  1.0  0.0  1.0  0.0  1.0  0.0     0.0  1.0  1.0  1.0  1.0  1.0  1.0\n 0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  …  0.0  0.0  1.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 1.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  0.0  0.0  1.0  1.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  1.0  0.0  0.0  0.0  1.0\n 0.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0     0.0  1.0  1.0  0.0  0.0  1.0  1.0\n 1.0  1.0  1.0  0.0  0.0  0.0  1.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  1.0\n\n\n\n\n1.4 Posterior predictive check\n\nobservations = Vector{Missing}(missing, length(coin_flips))\nposterior_check = predict(coin(observations), posterior)\n\n\nChains MCMC chain (1000×100×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = data[1], data[2], data[3], data[4], data[5], data[6], data[7], data[8], data[9], data[10], data[11], data[12], data[13], data[14], data[15], data[16], data[17], data[18], data[19], data[20], data[21], data[22], data[23], data[24], data[25], data[26], data[27], data[28], data[29], data[30], data[31], data[32], data[33], data[34], data[35], data[36], data[37], data[38], data[39], data[40], data[41], data[42], data[43], data[44], data[45], data[46], data[47], data[48], data[49], data[50], data[51], data[52], data[53], data[54], data[55], data[56], data[57], data[58], data[59], data[60], data[61], data[62], data[63], data[64], data[65], data[66], data[67], data[68], data[69], data[70], data[71], data[72], data[73], data[74], data[75], data[76], data[77], data[78], data[79], data[80], data[81], data[82], data[83], data[84], data[85], data[86], data[87], data[88], data[89], data[90], data[91], data[92], data[93], data[94], data[95], data[96], data[97], data[98], data[99], data[100]\ninternals         = \nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk   ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64     Float64    Float64   Float64    ⋯\n     data[1]    0.7460    0.4355    0.0156    779.3470        NaN    0.9995    ⋯\n     data[2]    0.7430    0.4372    0.0153    818.6527        NaN    1.0018    ⋯\n     data[3]    0.7530    0.4315    0.0139    961.4171        NaN    0.9994    ⋯\n     data[4]    0.7530    0.4315    0.0137    987.1635        NaN    0.9993    ⋯\n     data[5]    0.7300    0.4442    0.0147    908.3041        NaN    1.0015    ⋯\n     data[6]    0.7680    0.4223    0.0138    930.5262        NaN    1.0008    ⋯\n     data[7]    0.7400    0.4389    0.0139    998.6619        NaN    0.9990    ⋯\n     data[8]    0.7410    0.4383    0.0136   1037.4990        NaN    0.9994    ⋯\n     data[9]    0.7590    0.4279    0.0130   1083.8300        NaN    1.0002    ⋯\n    data[10]    0.7690    0.4217    0.0124   1153.2398        NaN    0.9995    ⋯\n    data[11]    0.7500    0.4332    0.0143    919.9873        NaN    1.0011    ⋯\n    data[12]    0.7510    0.4327    0.0134   1044.7622        NaN    1.0002    ⋯\n    data[13]    0.7630    0.4255    0.0162    691.1437        NaN    0.9993    ⋯\n    data[14]    0.7500    0.4332    0.0130   1105.7124        NaN    0.9992    ⋯\n    data[15]    0.7420    0.4378    0.0143    934.5392        NaN    0.9991    ⋯\n    data[16]    0.7520    0.4321    0.0140    950.1475        NaN    1.0007    ⋯\n    data[17]    0.7530    0.4315    0.0153    794.7635        NaN    0.9993    ⋯\n    data[18]    0.7450    0.4361    0.0142    937.9138        NaN    1.0018    ⋯\n    data[19]    0.7510    0.4327    0.0149    839.4231        NaN    1.0009    ⋯\n    data[20]    0.7470    0.4349    0.0137   1002.9777        NaN    0.9993    ⋯\n    data[21]    0.7300    0.4442    0.0139   1016.5668        NaN    0.9997    ⋯\n    data[22]    0.7500    0.4332    0.0144    909.8565        NaN    0.9998    ⋯\n    data[23]    0.7660    0.4236    0.0147    825.2510        NaN    1.0028    ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮          ⋮          ⋮       ⋱\n                                                    1 column and 77 rows omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n     data[1]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[2]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[3]    0.0000    1.0000    1.0000    1.0000    1.0000\n     data[4]    0.0000    1.0000    1.0000    1.0000    1.0000\n     data[5]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[6]    0.0000    1.0000    1.0000    1.0000    1.0000\n     data[7]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[8]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[9]    0.0000    1.0000    1.0000    1.0000    1.0000\n    data[10]    0.0000    1.0000    1.0000    1.0000    1.0000\n    data[11]    0.0000    0.7500    1.0000    1.0000    1.0000\n    data[12]    0.0000    1.0000    1.0000    1.0000    1.0000\n    data[13]    0.0000    1.0000    1.0000    1.0000    1.0000\n    data[14]    0.0000    0.7500    1.0000    1.0000    1.0000\n    data[15]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[16]    0.0000    1.0000    1.0000    1.0000    1.0000\n    data[17]    0.0000    1.0000    1.0000    1.0000    1.0000\n    data[18]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[19]    0.0000    1.0000    1.0000    1.0000    1.0000\n    data[20]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[21]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[22]    0.0000    0.7500    1.0000    1.0000    1.0000\n    data[23]    0.0000    1.0000    1.0000    1.0000    1.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                 77 rows omitted\n\n\n\n\n\n\n1.5 Chains\nThe results of sampling runs are MCMCChains.Chains instances.\n\nvalue = rand(500, 2, 3)\nchn = Chains(value, [:a, :b])\nchn2 = Chains(value, [\"A[1]\", \"A[2]\"])\n\n\nChains MCMC chain (500×2×3 Array{Float64, 3}):\nIterations        = 1:1:500\nNumber of chains  = 3\nSamples per chain = 500\nparameters        = A[1], A[2]\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n        A[1]    0.4954    0.2869    0.0073   1525.7353   1387.6389    1.0007   ⋯\n        A[2]    0.4886    0.2903    0.0073   1569.6827   1312.5552    1.0018   ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n        A[1]    0.0306    0.2417    0.4929    0.7388    0.9729\n        A[2]    0.0214    0.2380    0.4807    0.7357    0.9716\n\n\n\n\nNames of variables stored in a chain can be retrieved using\n\nnames(prior)\n\n2-element Vector{Symbol}:\n :p\n :lp\n\n\nOften, names are organized in to groups, i.e. data[1], data[2], … To retrieve the names within a group one can use namesingroup(chn, :data). group(chn, :data)\nNames can be organized into sections. Turings sample returns a Chains instance with sections :parameters and :internals.\n\nsections(prior)\n\n2-element Vector{Symbol}:\n :parameters\n :internals\n\n\nThe mapping of names into sections is stored in prior.name_map as a NamedTuple.\nTo retrieve the names for a specific section only, use e.g.\n\nnames(prior, :parameters)\n\n1-element Vector{Symbol}:\n :p\n\n\nMany functions can be restricted to specific sections. E.g. summarystats(prior, :parameters), Array(prior, :parameters).\nA Chains instance has the following fields: - value: AxisArray object of size :iter x :var x :chain - logevidence - name_map (to define sections, NamedTuple section -&gt; names, default section is :parameters) - info\nChains can be indexed, - prior[:p] returns an AxisArray for :p - prior[[:p, :lp]] returns a Chain restricted to [:p, :lp] - prior[1:10, [:p], 1] restricts chain to iterations 1:10, :p, and the first chain\nAn AxisArray has fields :axes and :data. It is like an Array but is aware of dimension names.\nFor example, our prior_check from above has axes [:iter, :var, :chain].\nCan use prior_check.value[var]\nI.e. to get only a slice of a dimension :dim, one can use A[dim = idx], so in the case of a MCMC sample, we might use prior.value[var = :p], or prior.value[chain = 1]\nTo convert a chain to a DataFrame containing the :parameters section only, one can do\n\nDataFrame(posterior[names(posterior, :parameters)]);\n\nTo extract the parameters as an Arrayone can use\n\nArray(prior_check, :parameters);"
  },
  {
    "objectID": "posts/TuringIntro/index.html#linear-regression",
    "href": "posts/TuringIntro/index.html#linear-regression",
    "title": "Introduction to Turing.jl",
    "section": "2 Linear regression",
    "text": "2 Linear regression\nLinear regression models a continuous (dependent) variable as a linear combination of independent predictors.\n\\[\ny = X \\beta + \\alpha + \\epsilon\n\\]\nThe corresponding Bayesian formulation can be written as\n\\[\n\\begin{align*}\n  \\mathbf{y} & \\sim N(\\alpha + \\mathbf{X} \\mathbf{\\beta}, \\sigma) \\\\\n  \\alpha & \\sim N(\\mu_{\\alpha}, \\sigma_{\\alpha}) \\\\\n  \\beta & \\sim N(\\mu_{\\beta}, \\sigma_{\\beta}) \\\\\n  \\sigma & \\sim Exp(\\lambda_{\\sigma})\n\\end{align*}\n\\]\nThe purpose of inference is to obtain \\(P(\\alpha, \\beta, \\sigma | \\mathbf{y}, \\mathbf{X})\\).\nIn Turing.jl, we first need to specify the generative model\n\nusing LinearAlgebra: I\nusing StatsBase\nusing Turing\n\n@model function linreg(X, y; n = size(X, 2))\n\n  α ~ Normal(mean(y), 2.5 * std(y))\n  β ~ filldist(Normal(0, 2), n)\n  σ ~ Exponential(1)\n\n  return y ~ MvNormal(X*β .+ α, I * σ^2)\nend\n\nlinreg (generic function with 2 methods)\n\n\n\nusing CSV\nusing DataFrames\n\nkidiq = CSV.read(\"kidiq.csv\", DataFrame)\ndescribe(kidiq)\n\n\n4×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\nkid_score\n86.7972\n20\n90.0\n144\n0\nInt64\n\n\n2\nmom_hs\n0.785714\n0\n1.0\n1\n0\nInt64\n\n\n3\nmom_iq\n100.0\n71.0374\n97.9153\n138.893\n0\nFloat64\n\n\n4\nmom_age\n22.7857\n17\n23.0\n29\n0\nInt64\n\n\n\n\n\n\n\n\nX = Matrix(select(kidiq, Not(:kid_score)))\ny = kidiq[:, :kid_score]\nmodel = linreg(X, y)\n\nnames(select(kidiq, Not(:kid_score)))\n\n3-element Vector{String}:\n \"mom_hs\"\n \"mom_iq\"\n \"mom_age\"\n\n\n\nchain = sample(model, NUTS(), 1000)\nsummarystats(chain)\n\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\nSampling:   3%|█▏                                       |  ETA: 0:00:04Sampling:   5%|██▏                                      |  ETA: 0:00:05Sampling:   7%|██▉                                      |  ETA: 0:00:05Sampling:  22%|█████████                                |  ETA: 0:00:02Sampling:  37%|███████████████▏                         |  ETA: 0:00:01Sampling:  50%|████████████████████▌                    |  ETA: 0:00:01Sampling:  64%|██████████████████████████▍              |  ETA: 0:00:00Sampling:  79%|████████████████████████████████▎        |  ETA: 0:00:00Sampling:  92%|█████████████████████████████████████▋   |  ETA: 0:00:00Sampling: 100%|█████████████████████████████████████████| Time: 0:00:01\n\n\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           α   20.5350    8.2946    0.4234   381.4727   472.8968    1.0064     ⋯\n        β[1]    2.5365    1.4630    0.0540   739.9959   579.0300    1.0014     ⋯\n        β[2]    0.5804    0.0578    0.0023   653.2618   540.4519    0.9999     ⋯\n        β[3]    0.2741    0.3129    0.0162   376.7357   382.7267    1.0060     ⋯\n           σ   17.8141    0.5788    0.0190   914.3251   653.7774    1.0013     ⋯\n                                                                1 column omitted"
  },
  {
    "objectID": "posts/TuringIntro/index.html#logistic-regression",
    "href": "posts/TuringIntro/index.html#logistic-regression",
    "title": "Introduction to Turing.jl",
    "section": "3 Logistic regression",
    "text": "3 Logistic regression\n\nusing CSV\nusing DataFrames\n\nwells = CSV.read(\"wells.csv\", DataFrame)\ndescribe(wells)\n\n\n5×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nFloat64\nReal\nFloat64\nReal\nInt64\nDataType\n\n\n\n\n1\nswitch\n0.575166\n0\n1.0\n1\n0\nInt64\n\n\n2\narsenic\n1.65693\n0.51\n1.3\n9.65\n0\nFloat64\n\n\n3\ndist\n48.3319\n0.387\n36.7615\n339.531\n0\nFloat64\n\n\n4\nassoc\n0.422848\n0\n0.0\n1\n0\nInt64\n\n\n5\neduc\n4.82848\n0\n5.0\n17\n0\nInt64\n\n\n\n\n\n\n\n\nusing Turing\n\n@model function logreg(X, y; predictors=size(X, 2))\n    #priors\n    α ~ Normal(0, 2.5)\n    β ~ filldist(TDist(3), predictors)\n\n    #likelihood\n    return y .~  BernoulliLogit.(α .+ X * β)\nend;\n\nfunction logistic(x)\n    return 1 / (1 + exp(-x))\nend\n\n@model function logreg2(X, y; predictors=size(X, 2))\n    #priors\n    α ~ Normal(0, 2.5)\n    β ~ filldist(TDist(3), predictors)\n\n    #likelihood\n    p = logistic.(α .+ X * β)\n    return y .~  Bernoulli.(p)\nend;\n\n\nX = Matrix(select(wells, Not(:switch)))\ny = wells[:, :switch]\nmodel = logreg2(X, y);\n\n\nchain = sample(model, NUTS(), 1000)\nsummarystats(chain)\n\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\nSampling:   3%|█▏                                       |  ETA: 0:00:04Sampling:   5%|██▏                                      |  ETA: 0:00:07Sampling:   6%|██▌                                      |  ETA: 0:00:08Sampling:   7%|██▉                                      |  ETA: 0:00:11Sampling:   8%|███▌                                     |  ETA: 0:00:10Sampling:  10%|████                                     |  ETA: 0:00:10Sampling:  11%|████▋                                    |  ETA: 0:00:09Sampling:  13%|█████▏                                   |  ETA: 0:00:09Sampling:  14%|█████▊                                   |  ETA: 0:00:09Sampling:  16%|██████▌                                  |  ETA: 0:00:08Sampling:  18%|███████▎                                 |  ETA: 0:00:08Sampling:  19%|███████▉                                 |  ETA: 0:00:08Sampling:  21%|████████▋                                |  ETA: 0:00:07Sampling:  23%|█████████▍                               |  ETA: 0:00:07Sampling:  25%|██████████▏                              |  ETA: 0:00:07Sampling:  27%|██████████▉                              |  ETA: 0:00:06Sampling:  28%|███████████▋                             |  ETA: 0:00:06Sampling:  31%|████████████▋                            |  ETA: 0:00:06Sampling:  33%|█████████████▍                           |  ETA: 0:00:05Sampling:  34%|██████████████▏                          |  ETA: 0:00:05Sampling:  36%|██████████████▉                          |  ETA: 0:00:05Sampling:  39%|███████████████▉                         |  ETA: 0:00:05Sampling:  41%|████████████████▋                        |  ETA: 0:00:04Sampling:  43%|█████████████████▋                       |  ETA: 0:00:04Sampling:  46%|██████████████████▊                      |  ETA: 0:00:04Sampling:  48%|███████████████████▌                     |  ETA: 0:00:04Sampling:  50%|████████████████████▌                    |  ETA: 0:00:04Sampling:  52%|█████████████████████▍                   |  ETA: 0:00:03Sampling:  55%|██████████████████████▍                  |  ETA: 0:00:03Sampling:  56%|███████████████████████▏                 |  ETA: 0:00:03Sampling:  58%|███████████████████████▉                 |  ETA: 0:00:03Sampling:  61%|████████████████████████▉                |  ETA: 0:00:03Sampling:  63%|█████████████████████████▉               |  ETA: 0:00:02Sampling:  65%|██████████████████████████▊              |  ETA: 0:00:02Sampling:  68%|███████████████████████████▊             |  ETA: 0:00:02Sampling:  70%|████████████████████████████▌            |  ETA: 0:00:02Sampling:  72%|█████████████████████████████▌           |  ETA: 0:00:02Sampling:  74%|██████████████████████████████▍          |  ETA: 0:00:02Sampling:  76%|███████████████████████████████▏         |  ETA: 0:00:02Sampling:  77%|███████████████████████████████▊         |  ETA: 0:00:01Sampling:  79%|████████████████████████████████▌        |  ETA: 0:00:01Sampling:  81%|█████████████████████████████████▎       |  ETA: 0:00:01Sampling:  84%|██████████████████████████████████▎      |  ETA: 0:00:01Sampling:  85%|███████████████████████████████████      |  ETA: 0:00:01Sampling:  88%|████████████████████████████████████     |  ETA: 0:00:01Sampling:  90%|████████████████████████████████████▊    |  ETA: 0:00:01Sampling:  91%|█████████████████████████████████████▌   |  ETA: 0:00:01Sampling:  94%|██████████████████████████████████████▌  |  ETA: 0:00:00Sampling:  96%|███████████████████████████████████████▍ |  ETA: 0:00:00Sampling:  98%|████████████████████████████████████████▍|  ETA: 0:00:00Sampling: 100%|█████████████████████████████████████████| Time: 0:00:06\n\n\n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk   ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64     Float64    Float64   Float64    ⋯\n           α   -0.1589    0.1054    0.0053    395.7466   437.7457    1.0060    ⋯\n        β[1]    0.4689    0.0403    0.0020    401.5135   500.8058    1.0087    ⋯\n        β[2]   -0.0090    0.0010    0.0000   1102.0522   843.2654    1.0046    ⋯\n        β[3]   -0.1230    0.0780    0.0033    546.2323   662.5340    1.0042    ⋯\n        β[4]    0.0429    0.0092    0.0004    623.2406   578.5058    0.9996    ⋯\n                                                                1 column omitted"
  },
  {
    "objectID": "posts/TuringIntro/index.html#references",
    "href": "posts/TuringIntro/index.html#references",
    "title": "Introduction to Turing.jl",
    "section": "4 References",
    "text": "4 References\n\nBayesian Statistics using Julia and Turing, https://storopoli.io/Bayesian-Julia/\nTuring.jl tutorials, https://turing.ml/v0.22/tutorials/"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html",
    "href": "posts/TimeDependentSchroedingerEquation/index.html",
    "title": "Time-dependent Schrödinger equation",
    "section": "",
    "text": "The one-dimensional time-dependent Schrödinger equation (TDSE) for a potential \\(V(x)\\) is given by\n\\[\ni\\hbar \\frac{\\partial \\psi(x,t)}{\\partial t} = -\\frac{\\hbar^2}{2m} \\frac{\\partial^2 \\psi(x,t)}{\\partial x^2} + V(x) \\psi(x,t)\n\\]\nTo specify a problem, we need to specify"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html#solve-using-finite-difference-method",
    "href": "posts/TimeDependentSchroedingerEquation/index.html#solve-using-finite-difference-method",
    "title": "Time-dependent Schrödinger equation",
    "section": "Solve using finite difference method",
    "text": "Solve using finite difference method\nUsing finite differences, we can discretize the TDSE as\n\\[\n\\psi_i^{n+1} = \\psi_i^n +\n   \\frac{i \\hbar \\Delta t}{2m (\\Delta x)^2} \\left( \\psi_{i+1}^n - 2\\psi_i^n + \\psi_{i-1}^n \\right) -\n   \\frac{i \\Delta t}{\\hbar} V(x_i) \\psi_i^n\n\\]\nAssuming a potential of the form\n\\[\nV(x) = - 10^4 \\exp\\left(-\\frac{(x - L/2)^2}{2(L/20)^2} \\right)\n\\]\n\nusing CairoMakie\nusing LaTeXStrings\n\nset_theme!()\n\nnx = 301\nnt = 100001\nL = 1.0\nT = 0.01\ndx = L / (nx - 1)\ndt = T / (nt - 1)\nx = range(0.0, L; length = nx)\n\nV = @. -10000 * exp(-(x - L/2.)^2 / 2 / (L/20.)^2)\n\nlines(x, V; axis = (;ylabel = \"V(x)\", xlabel = \"x/L\"))\n\n\n\n\n\n\n\n\nAs initial condition we use\n\\[\n\\psi_0 = \\sqrt{2} \\sin(\\pi x)\n\\]\n\nΨ₀ = sqrt(2.) * sin.(π*x)\n\nlines(x, Ψ₀; axis = (;ylabel = L\"\\psi_0(x)\", xlabel = L\"x/L\"))\n\n\n\n\n\n\n\n\nwhich is normalized to 1\n\\[\n\\int_0^1 |\\psi_0(x)|^2 \\mathrm{d}x = 1\n\\]\nas can be confirmed numerically\n\nsum(Ψ₀.^2 * dx)\n\n1.0000000000000002\n\n\n\nΨ = zeros(ComplexF64, nt, nx)\nΨ[1,:] .= Ψ₀\n\nfunction evolve!(psi)\n    for t in 1:(nt-1)\n        for i in 2:(nx-1)\n            psi[t+1, i] = psi[t, i] + \n                im/2 * dt/dx^2 * (psi[t, i+1] - 2*psi[t, i] + psi[t, i-1]) - \n                im*dt*V[i]*psi[t, i]\n        end\n        \n        normal = sum(abs.(psi[t+1,:]).^2)*dx\n        for i in 1:(nx-1)\n            psi[t+1,i] = psi[t+1,i]/normal\n        end\n    end\nend\n\nevolve!(Ψ)\n\n\nf,a,p = lines(x, abs2.(Ψ[1,:]); \n            label = L\"t = 0\", \n            axis = (;xlabel = L\"x\",\n                     ylabel = L\"\\Psi(5000, x)\"))\n\nlines!(a, x, abs2.(Ψ[5000,:]); label = L\"t = 5000\")\naxislegend(a)\nf"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html#solve-using-eigenstate-evolution",
    "href": "posts/TimeDependentSchroedingerEquation/index.html#solve-using-eigenstate-evolution",
    "title": "Time-dependent Schrödinger equation",
    "section": "Solve using eigenstate evolution",
    "text": "Solve using eigenstate evolution\nThe TDSE can also be solved using the eigenstate evolution. Here we first solve the time-independent Schrödinger equation (TISE),\n\\[\n-\\frac{\\hbar^2}{2m} \\frac{\\partial^2 \\psi(x)}{\\partial x^2} + V(x) \\psi(x) = E \\psi(x)\n\\]\nto obtain the eigenstates \\(\\psi_j(x)\\) and their energy levels \\(E_j\\).\nThen we can express the time-dependent solutions of the TDSE as\n\\[\n\\psi(x, t) = \\sum_{j=0}^\\infty a_j \\psi_j(x) \\exp(-i E_j t)\n\\]\nwhere the \\(a_j\\) are obtained from the initial condition \\(\\psi_0(x)\\) through\n\\[\na_j = \\int_{-\\infty}^{\\infty} \\psi_0(x) \\psi_j^*(x) \\mathrm{d}x\n\\]\nTo solve the TISE numerically, we can discretize the space and convert the differential equation into a matrix equation.\n\nDivide the spatial domain into \\(N\\) points with spacing \\(\\Delta x\\). Let \\(x_i = x_0 + i\\Delta x\\) for \\(i = 0, 1, 2, \\ldots, N-1\\).\nApproximate the second derivative using the central difference method: \\[\n\\frac{d^2 \\psi}{dx^2} \\bigg|_{x=x_i} \\approx \\frac{\\psi(x_{i+1}) - 2\\psi(x_i) + \\psi(x_{i-1})}{(\\Delta x)^2}\n\\]\n\nWe substitute this finite difference approximation into the Schrödinger equation\n\\[\n-\\frac{\\hbar^2}{2m} \\frac{\\psi(x_{i+1}) - 2\\psi(x_i) + \\psi(x_{i-1})}{(\\Delta x)^2} + V(x_i) \\psi(x_i) = E \\psi(x_i)\n\\]\nand rearrange terms\n\\[\n-\\frac{\\hbar^2}{2m (\\Delta x)^2} \\psi(x_{i+1}) + \\left( \\frac{\\hbar^2}{m (\\Delta x)^2} + V(x_i) \\right) \\psi(x_i) - \\frac{\\hbar^2}{2m (\\Delta x)^2} \\psi(x_{i-1}) = E \\psi(x_i)\n\\]\nThis can be written as a matrix equation \\(H \\psi = E \\psi\\), where \\(H\\) is a tridiagonal matrix with the following elements:\n\nThe diagonal elements \\[\nH_{ii} = \\frac{\\hbar^2}{m (\\Delta x)^2} + V(x_i)\n\\]\nThe off-diagonal elements \\[\nH_{i, i+1} = H_{i+1, i} = -\\frac{\\hbar^2}{2m (\\Delta x)^2}\n\\]\n\nFor an \\(N\\)-point discretization, the Hamiltonian matrix \\(H\\) in tridiagonal form looks like this:\n\\[\nH = \\begin{bmatrix}\na_1 & b & 0 & 0 & \\cdots & 0 & 0 \\\\\nb & a_2 & b & 0 & \\cdots & 0 & 0 \\\\\n0 & b & a_3 & b & \\cdots & 0 & 0 \\\\\n0 & 0 & b & a_4 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & a_{N-2} & b \\\\\n0 & 0 & 0 & 0 & \\cdots & b & a_{N-1}\n\\end{bmatrix}\n\\]\nwhere: - \\(a_i = \\frac{\\hbar^2}{m (\\Delta x)^2} + V(x_i)\\) (diagonal elements), - \\(b = -\\frac{\\hbar^2}{2m (\\Delta x)^2}\\) (off-diagonal elements).\n\nusing LinearAlgebra\n\na = 1/dx^2 .+ V[2:(end-1)]\nb = -1/(2*dx^2) .* ones(length(a)-1)\n\nF = eigen(SymTridiagonal(a, b));\n\nThe k-th eigenvalue can be accessed using F.values[k], the k-th eigenvector as F.vectors[:,k].\n\nE_j = F.values[1:70]\nψⱼ = vcat(zeros(70)', F.vectors[:,1:70], zeros(70)')\n\ncⱼ = ψⱼ' * Ψ₀\n\nlines(x, abs2.(ψⱼ * (cⱼ .* exp.(-im*E_j*5000*dt))  ))"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html#animation",
    "href": "posts/TimeDependentSchroedingerEquation/index.html#animation",
    "title": "Time-dependent Schrödinger equation",
    "section": "Animation",
    "text": "Animation\n\nt = Observable(0)\ny = @lift abs2.(ψⱼ * (cⱼ .* exp.(-im*E_j*$t*dt)))\n\nf = Figure()\nax = Axis(f[1,1], xlabel = L\"x/L\", ylabel = L\"|\\psi(x)|^2\",\n           title = @lift \"t = $($t) dt\")\nlines!(ax, x, y;\n          color = :blue)\nxlims!(ax, 0.0, 1.0)\nylims!(ax, 0, 20)\n\n\nrecord(f, \"schroedinger_animation.gif\", 0:100:30000, framerate = 30) do i \n  t[] = i  \nend\n\n\"schroedinger_animation.gif\"\n\n\n\n\n\nSchroedinger animation"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html#references",
    "href": "posts/TimeDependentSchroedingerEquation/index.html#references",
    "title": "Time-dependent Schrödinger equation",
    "section": "References",
    "text": "References\nhttps://github.com/lukepolson/youtube_channel/blob/main/Python%20Metaphysics%20Series/vid17.ipynb"
  },
  {
    "objectID": "posts/Pendulum/index.html",
    "href": "posts/Pendulum/index.html",
    "title": "Pendulum",
    "section": "",
    "text": "A pendulum can be modelled as a mass \\(m\\) on a weightless rod of length \\(l\\). The forces acting on the mass are gravity and the tension in the rod.\nWe can use Newton’s second law to derive equations of motion.\nThe component of gravity along the direction of the string is compensated by the tension in the string. Only the force perpendicular to the string is used to accelerate the mass. Here we consider arclength \\(s = l \\Theta\\)\n\\[\nm \\frac{\\mathrm{d}^2s}{\\mathrm{d}t^2} = - m g \\sin(\\Theta)\n\\]\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\sin(\\Theta)\n\\]\nThis is a non-linear ordinary differential equation (ODE) of second order."
  },
  {
    "objectID": "posts/Pendulum/index.html#equations-of-motion",
    "href": "posts/Pendulum/index.html#equations-of-motion",
    "title": "Pendulum",
    "section": "",
    "text": "A pendulum can be modelled as a mass \\(m\\) on a weightless rod of length \\(l\\). The forces acting on the mass are gravity and the tension in the rod.\nWe can use Newton’s second law to derive equations of motion.\nThe component of gravity along the direction of the string is compensated by the tension in the string. Only the force perpendicular to the string is used to accelerate the mass. Here we consider arclength \\(s = l \\Theta\\)\n\\[\nm \\frac{\\mathrm{d}^2s}{\\mathrm{d}t^2} = - m g \\sin(\\Theta)\n\\]\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\sin(\\Theta)\n\\]\nThis is a non-linear ordinary differential equation (ODE) of second order."
  },
  {
    "objectID": "posts/Pendulum/index.html#analytic-solution-of-linearized-ode",
    "href": "posts/Pendulum/index.html#analytic-solution-of-linearized-ode",
    "title": "Pendulum",
    "section": "Analytic solution of linearized ODE",
    "text": "Analytic solution of linearized ODE\nTo solve that equation analytically, it can be linearized using the Taylor expansion\n\\[\n\\sin(\\Theta) = \\Theta - \\frac{\\Theta^3}{3!} + \\frac{\\Theta^5}{5!} + \\ldots\n\\]\n\nusing CairoMakie\nusing LaTeXStrings\n\nset_theme!()\n\nΘ = range(0, π/2; length = 100)\napprox(Θ) = Θ - Θ^3/6 + Θ^5/120\n\nf = Figure()\na, p = lines(f[1,1], Θ, sin.(Θ); \n            linewidth = 7,\n            color = (:blue, 0.2), \n            label = L\"\\sin(Θ)\", \n            axis = (;title = \"Taylor approximation of sin(Θ)\", xlabel = L\"Θ\"))\nlines!(f[1,1], Θ, Θ; label = L\"Θ\")\nlines!(f[1,1], Θ, Θ - Θ .^ 3/6, label = L\"Θ - \\frac{Θ^3}{3!}\")\nlines!(f[1,1], Θ, Θ - Θ .^ 3 / 6 + Θ .^ 5 / 120, label = L\"Θ - \\frac{Θ^3}{3!} + \\frac{Θ^5}{5!}\")\n\naxislegend(a; position = :lt)\n\nf\n\n\n\n\n\n\n\n\nFor small angles (\\(\\Theta \\le 30°\\)) the first order Taylor approximation is reasonable. For larger angles, we overestimate the restoring force and, therefore, the frequency.\nThe resulting linear second-order ODE\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\Theta\n\\]\ncan be solved analytically with solution\n\\[\n\\Theta(t) = A \\cos(\\omega t) + B \\cos(\\omega t)\n\\]\nwith\n\\[\n\\omega = \\sqrt{\\frac{g}{l}}\n\\]\nThe solution to the linear ODE with \\(\\Theta_0 = \\pi/6 = 30°\\) is plotted below\n\ng = 9.81\nl = 1\nω = sqrt(g/l)\nt = range(0, 4*pi/ω, length = 200)\nΘ₀ = π/6\nanalytical = Θ₀ * cos.(ω*t)\n\nf2 = Figure()\n\nlines(f2[1,1], t, analytical * 180/pi;\n      axis = (;xlabel = \"time (s)\", ylabel = L\"$\\theta$ (deg)\"))\n\nf2"
  },
  {
    "objectID": "posts/Pendulum/index.html#numerical-solution-of-non-linear-ode",
    "href": "posts/Pendulum/index.html#numerical-solution-of-non-linear-ode",
    "title": "Pendulum",
    "section": "Numerical solution of non-linear ODE",
    "text": "Numerical solution of non-linear ODE\nThe nonlinear ODE can be solved using DifferentialEquations.jl. First we need to tranform the 2-order ODE into a 1-order ODE.\nThe state \\(u(t)\\) is given as \\[\nu(t) = \\begin{pmatrix}\n    \\Theta(t) \\\\\n    \\dot{\\Theta}(t)\n    \\end{pmatrix}\n\\]\n\\[\n\\dot{u}(t) = \\begin{pmatrix}\n    \\dot{\\Theta}(t) \\\\\n    - \\frac{g}{l} \\sin(\\Theta(t))\n\\end{pmatrix}\n\\]\nNow we need to define a function that provides \\(\\dot{u}(t)\\) given \\(u(t)\\), \\(t\\), and possible parameters.\n\nfunction pendulum(u, params, t)\n    du = zeros(length(u))\n    l = params[1]\n    du[1] = u[2]\n    du[2] = -g/l*sin(u[1])\n    return du\nend;\n\n\nusing DifferentialEquations\n\ntspan = (0, 4π/ω)\nparams = [l]\ninit = [π/6, 0]\nprob = ODEProblem(pendulum, init, tspan, params);\n\nu = solve(prob) returns the solution to the ODEProblem. Note that the time stepping is done in an adaptive manner, but u(t) interpolates the system state for any time \\(t \\in\\) tspan.\n\nu = solve(prob)\n\nlines!(f2[1,1], t, 180/π*[u(ti)[1] for ti in t])\n\nf2\n\n\n\n\n\n\n\n\nAs already stated above, the small-angle approximation overestimates the frequency and underestimates the period."
  },
  {
    "objectID": "posts/Pendulum/index.html#symbolic-solution-using-lagrangian",
    "href": "posts/Pendulum/index.html#symbolic-solution-using-lagrangian",
    "title": "Pendulum",
    "section": "Symbolic solution using Lagrangian",
    "text": "Symbolic solution using Lagrangian\nAbove, we had derived the equations of motion by hand. This is not always feasible. Julia has symbolic computation capabilities through the package Symbolics.jl. This allows us to use Julia as a computer algebra system (CAS) to derive the equations of motion from the Lagrangian, which is easy to specify.\nFirst we define the Lagrangian symbolically\n\nusing Symbolics\n\n@variables t m g l Θ(t)\n\n# define Theta, x, and y as above\nx = l*sin(Θ)\ny = -l*cos(Θ)\n\nDt = Differential(t)\n\n# define kinetic and potential energies\nT = 1/2*m*(Dt(x)^2 + Dt(y)^2)\nV = m*g*y\n\n# and the Lagrangian\nL = T - V;\n\nNote that Julia has not performed any differentiation yet. We need to call expand_derivatives to do so.\n\nexpand_derivatives(L)\n\n\\[ \\begin{equation}\ng l m \\cos\\left( \\Theta\\left( t \\right) \\right) + 0.5 \\left( \\sin^{2}\\left( \\Theta\\left( t \\right) \\right) \\left( \\frac{\\mathrm{d} \\Theta\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} l^{2} + \\cos^{2}\\left( \\Theta\\left( t \\right) \\right) \\left( \\frac{\\mathrm{d} \\Theta\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} l^{2} \\right) m\n\\end{equation}\n\\]\n\n\nTo get the equations of motion from the Lagrangian, we need to compute\n\\[\n\\frac{\\partial L}{\\partial \\Theta} - \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\partial L}{\\partial \\dot{\\Theta}}\n\\]\nand solve for \\(\\ddot{\\Theta}\\). This can be done using symbolic_linear_solve.\n\nΘ̇ = Dt(Θ)\nΘ̈ = Dt(Θ̇)\nDΘ = Differential(Θ)\nDΘ̇ = Differential(Θ̇)\n\nLE = simplify(expand_derivatives(DΘ(L) -  Dt(DΘ̇(L))))\n\nu̇ = [Θ̇; simplify(symbolic_linear_solve(LE, Θ̈))]\n\n# define a function du(u, params, t) to be used in ODEProblem\ndu, du! = build_function(u̇, [Θ, Θ̇],  [l, g], t; expression = Val{false});\n\nThese functions can now be used to solve the DE numerically using DifferentialEquations.jl as above.\n\ntspan = (0, 4π/ω)\nparams = [1.0, 9.81]\ninit = [π/6, 0]\nprob = ODEProblem(du, init, tspan, params);\n\n\nsol = solve(prob)\n\nt = range(0, 4*pi/ω, length = 200)\nlines!(f2[1,1], t, 180/π*[sol(ti)[1] for ti in t])\n\nf2"
  },
  {
    "objectID": "posts/Pendulum/index.html#animation",
    "href": "posts/Pendulum/index.html#animation",
    "title": "Pendulum",
    "section": "Animation",
    "text": "Animation\nWe can use the record function in CairoMakie.jl to create an animation.\n\n\nu1 = [u(ti)[1] for ti in t]\n\nf = Figure()\n\nax1 = Axis(f[1,1], xlabel = L\"t\", ylabel = L\"Θ\")\nxlims!(ax1, tspan...)\nylims!(ax1, -1,1)\n\nax2 = Axis(f[1,2]; aspect = 1)\nhidedecorations!(ax2)\nhidespines!(ax2)\nxlims!(ax2, -12,12)\nylims!(ax2, -12,12)\n\nr = 10.0\n\nrecord(f, \"incremental_plot_animation.gif\", 1:length(t), framerate = 30) do i\n\n    empty!(ax1)\n    lines!(ax1, t[1:i], u1[1:i]; color = :blue)\n    lines!(ax1, t[1:i], analytical[1:i]; color = :red)\n\n    empty!(ax2)\n    y = -r*cos(u1[i])\n    x = r*sin(u1[i])\n    lines!(ax2, [0,x], [0,y]; color = :blue)\n    scatter!(ax2, x, y; markersize = 20, color = :blue)\n\n    y = -r*cos(analytical[i])\n    x = r*sin(analytical[i])\n    lines!(ax2, [0,x], [0,y]; color = :red)\n    scatter!(ax2, x, y; markersize = 20, color = :red)\nend;\n\n\n\n\nAnimation"
  },
  {
    "objectID": "posts/Pendulum/index.html#references",
    "href": "posts/Pendulum/index.html#references",
    "title": "Pendulum",
    "section": "References",
    "text": "References\n\nhttps://cooperrc.github.io/Julia-learning/day_05.html"
  },
  {
    "objectID": "posts/SimulatedAnnealing/index.html",
    "href": "posts/SimulatedAnnealing/index.html",
    "title": "Simulated Annealing",
    "section": "",
    "text": "The Traveling Salesperson Problem (TSP) is a classic optimization problem: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the starting city?\nThis problem becomes incredibly challenging as the number of cities increases. It’s a prime example of an NP-hard problem, meaning that finding the absolute best solution becomes computationally infeasible with just a moderate number of cities. As such, we often resort to heuristic methods to find good enough solutions in a reasonable time."
  },
  {
    "objectID": "posts/SimulatedAnnealing/index.html#the-traveling-salesperson-problem-tsp",
    "href": "posts/SimulatedAnnealing/index.html#the-traveling-salesperson-problem-tsp",
    "title": "Simulated Annealing",
    "section": "",
    "text": "The Traveling Salesperson Problem (TSP) is a classic optimization problem: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the starting city?\nThis problem becomes incredibly challenging as the number of cities increases. It’s a prime example of an NP-hard problem, meaning that finding the absolute best solution becomes computationally infeasible with just a moderate number of cities. As such, we often resort to heuristic methods to find good enough solutions in a reasonable time."
  },
  {
    "objectID": "posts/SimulatedAnnealing/index.html#simulated-annealing-sa",
    "href": "posts/SimulatedAnnealing/index.html#simulated-annealing-sa",
    "title": "Simulated Annealing",
    "section": "Simulated Annealing (SA)",
    "text": "Simulated Annealing (SA)\nSimulated Annealing is a probabilistic metaheuristic that draws inspiration from the process of annealing in metallurgy. When metals are heated and then slowly cooled, they reach a state of minimum energy, forming a very strong and ordered crystalline structure. If they are cooled too quickly, they can end up in a less ordered, higher energy state. SA mimics this process to find good solutions in optimization problems.\n\nStart with a random configuration: In our case, a random ordering of cities.\nDefine an “energy” function: In our case, this will be the total distance of the route. We want to minimize this energy.\nStart with a high temperature: The “temperature” is a parameter that controls the probability of accepting worse solutions. At high temperatures, the algorithm is more likely to accept moves that increase the energy.\nIterate and perturb:\n\nRandomly “perturb” the current solution. For TSP, a perturbation could be swapping the positions of two cities.\nCalculate the change in “energy” (total route distance) that would result from accepting this move.\nIf the proposed change decreases the energy (shorter route), accept the move.\nIf the proposed change increases the energy (longer route), accept it with a probability that depends on the temperature:\n\nThe probability is calculated using the Boltzmann probability equation: exp(-delta_energy / temperature). This means the higher the delta_energy, the lower the acceptance probability. Also, higher temperatures means more probability of accepting bad steps.\n\n\nCool the system: Gradually decrease the temperature. As the temperature decreases, the probability of accepting moves that increase the energy decreases. The algorithm becomes more “greedy” towards better solutions and will likely converge to a good (but not necessarily optimal) solution.\nRepeat steps 4-5 until a stopping criteria is met (e.g., fixed number of steps, temperature reaches a minimum).\n\nBy accepting bad solutions with a probability that decreases with temperature, the algorithm can avoid getting stuck in local minima (a suboptimal solution) and explore different parts of the solution space."
  },
  {
    "objectID": "posts/SimulatedAnnealing/index.html#implementing-sa-in-julia",
    "href": "posts/SimulatedAnnealing/index.html#implementing-sa-in-julia",
    "title": "Simulated Annealing",
    "section": "Implementing SA in Julia",
    "text": "Implementing SA in Julia\nThe implementation fo simulated annealing (SA) requires:\n\nAn Objective Function: This function takes the problem data (e.g., the cities in TSP) and a set of parameters (e.g., a route in TSP), and returns a numerical value representing the “energy” or “cost” of the solution. The goal of the optimization is to minimize this value.\nA Proposal Function: This function takes the problem data and the current set of parameters and proposes a new set of parameters. In TSP, this could be a function that swaps two cities in the route. This is the way we “perturb” the current solution.\n\nWith these two components, the core SA algorithm can remain the same regardless of the specific optimization problem.\n\nfunction simulated_annealing(data, initial_params, objective_function, proposal_function;\n                              initial_temperature=1000.0,\n                              cooling_rate=0.99,\n                              iterations = 10000)\n    current_params = initial_params\n    current_energy = objective_function(data, current_params)\n    temperature = initial_temperature\n    best_params = copy(current_params)\n    best_energy = current_energy\n\n    energies = [current_energy] # keep track of energies during SA\n\n    for i in 1:iterations\n        proposed_params = proposal_function(data, current_params)\n        proposed_energy = objective_function(data, proposed_params)\n\n        delta_energy = proposed_energy - current_energy\n        if delta_energy &lt; 0\n           current_params = proposed_params\n           current_energy = proposed_energy\n            if proposed_energy &lt; best_energy\n                best_energy = proposed_energy\n                best_params = copy(proposed_params)\n            end\n        else\n           acceptance_probability = exp(-delta_energy / temperature)\n           if rand() &lt; acceptance_probability\n               current_params = proposed_params\n               current_energy = proposed_energy\n           end\n       end\n       temperature *= cooling_rate\n       push!(energies, current_energy)\n    end\n    return best_params, best_energy, energies\nend\n\nsimulated_annealing (generic function with 1 method)"
  },
  {
    "objectID": "posts/SimulatedAnnealing/index.html#solving-the-traveling-salesperson-problem-using-simulated-annealing",
    "href": "posts/SimulatedAnnealing/index.html#solving-the-traveling-salesperson-problem-using-simulated-annealing",
    "title": "Simulated Annealing",
    "section": "Solving the Traveling Salesperson Problem using Simulated Annealing",
    "text": "Solving the Traveling Salesperson Problem using Simulated Annealing\nIn order to solve TSP using SA, we need to implement the problem-specific objective function (calculate_distance) and the proposal function (perturb_route).\n\nusing Random\nusing StatsBase: sample\nusing CairoMakie\n\nfunction calculate_distance(cities, route)\n    total_distance = 0.0\n    for i in 1:(length(route)-1)\n        city1 = route[i]\n        city2 = route[i+1]\n        total_distance += sqrt((cities[city1][1] - cities[city2][1])^2 + (cities[city1][2] - cities[city2][2])^2)\n    end\n   # Complete the route back to the starting point\n   total_distance += sqrt((cities[route[end]][1] - cities[route[1]][1])^2 + (cities[route[end]][2] - cities[route[1]][2])^2)\n\n    return total_distance\nend\n\nfunction perturb_route(cities, route)\n    n = length(route)\n    i, j = sample(1:n, 2, replace=false) # select 2 random cities\n    route[i], route[j] = route[j], route[i] # swap them\n    return route\nend\n\nperturb_route (generic function with 1 method)\n\n\n\n# Set up cities \ncities = Dict(\n    1 =&gt; [1.0, 1.0],\n    2 =&gt; [2.0, 2.0],\n    3 =&gt; [4.0, 1.0],\n    4 =&gt; [4.0, 3.0],\n    5 =&gt; [3.0, 5.0],\n    6 =&gt; [1.0, 4.0],\n    7 =&gt; [5.0, 5.0]\n);\n\n\n# Initialize a random route\ninitial_route = collect(1:length(cities))\nshuffle!(initial_route)\n\n\n# Run the generalized SA algorithm for TSP\nbest_route, best_distance, distances = simulated_annealing(\n    cities,                     # problem data\n    initial_route,              # initial parameters (route)\n    calculate_distance,       # objective function\n    perturb_route,                # proposal function\n    iterations = 10000,           # additional parameters\n    cooling_rate = 0.99\n)\n\n\n\n\nprintln(\"Best Route: \", best_route)\nprintln(\"Best Distance: \", best_distance)\n\nplot(distances, axis = (;title=\"Distance during Simulated Annealing\", xlabel=\"Iteration\", ylabel=\"Total Distance\"))\n\nBest Route: [6, 5, 7, 4, 3, 2, 1]\nBest Distance: 15.122417494872465\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/Q6F2P/src/scenes.jl:238"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]