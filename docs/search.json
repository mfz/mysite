[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Notes",
    "section": "",
    "text": "Tensor notation\n\n\nIntroduction to tensor, index, or Einstein notation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Latent Class Analysis\n\n\nLatent class analysis using Gibbs sampling and Expectation-Maximization\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nFor a nice tutorial on how to create them visit https://ucsb-meds.github.io/creating-quarto-websites/"
  },
  {
    "objectID": "posts/TensorNotation/index.html",
    "href": "posts/TensorNotation/index.html",
    "title": "Tensor notation",
    "section": "",
    "text": "Vectors\nIn tensor notation, a vector is written as\n\\[\n\\mathbf{u} = u_i\n\\]\nwhere \\(i = 1,2,3\\), corresponding to the \\(x\\), \\(y\\), and \\(z\\) component, respectively.\n\\[\n\\mathbf{u} + \\mathbf{v} = u_i + v_i\n\\]\n\n\nEinstein summation\nAny index appearing twice is automatically summed from 1 to 3. This is called Einstein summation. Any index can at most appear twice.\n\n\nDot (inner) product\nUsing Einstein summation, the dot (inner) product of two vectors is\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_i v_i = u_1 v_1 + u_2 v_2 + u_3 v_3\n\\]\n\n\nDyadic (outer) product\nThe dyadic (outer) product of two vectors is\n\\[\n\\mathbf{u} \\otimes \\mathbf{v} = u_i v_j =\n\\left[\n\\begin{matrix}\nu_1 v_1 & u_1 v_2 & u_1 v_3 \\cr\nu_2 v_1 & u_2 v_2 & u_2 v_3 \\cr\nu_3 v_1 & u_3 v_2 & u_3 v_3\n\\end{matrix}\n\\right]\n\\]\n\n\nDifferentiation with respect to time\nDifferentiation with respect to time can be written as\n\\[\n\\frac{\\text{d}\\mathbf{x}}{\\text{d}t} = \\mathbf{\\dot{x}} = \\dot{x_i} = x_{i|t}\n\\]\n\n\nDifferentiation with respect to space\nThe gradient of a scalar function is given by\n\\[\n\\nabla \\phi = \\partial_i \\phi = \\phi_{|i}\n\\]\nThe component-wise spatial derivative of a vector\n\\[\n\\frac{\\partial \\mathbf{u}}{\\partial x_j} = u_{i|j}\n\\]\nThe divergence of a vector is\n\\[\n\\text{div} \\ \\mathbf{u} = \\nabla \\cdot \\mathbf{u} = u_{i|i}\n\\]\n\n\nKronecker delta\nThe Kronecker delta is defined as\n\\[\n\\delta_{ij} = \\begin{cases}\n\\begin{matrix}\n1 & i = j \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nKronecker delta is also often called the substitution operator, as\n\\[\nu_i \\delta_{ij} = u_j\n\\]\nThe Kronecker delta often occurs when one deals with spatial derivatives of position\n\\[\n\\frac{\\partial x_i}{\\partial x_j} = \\partial_j x_i = x_{i|j} = \\delta_{ij}\n\\]\n\n\nLevi-Civita alternating tensor\n\\[\n\\epsilon_{ijk} = \\begin{cases}\n\\begin{matrix}\n1 & ijk \\in \\{123, 231, 312\\} \\cr\n-1 & ijk \\in \\{132, 213, 321\\} \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nSometimes the following equality can be useful to simplify expressions:\n\\[\n\\epsilon_{ijk} \\epsilon_{imn} = \\delta_{jm}\\delta_{kn}-\\delta_{jn}\\delta_{km}\n\\]\n\n\nRotation or curl of a vector\n\\[\n\\text{rot} \\ \\mathbf{u} = \\nabla \\times \\mathbf{u} = \\epsilon_{ijk} u_{k|j}\n\\]\nReferences: [https://www.continuummechanics.org]"
  },
  {
    "objectID": "posts/BayesianLCA/index.html",
    "href": "posts/BayesianLCA/index.html",
    "title": "Bayesian Latent Class Analysis",
    "section": "",
    "text": "Bayesian Latent Class Analysis (BLCA) is a powerful statistical method used to classify subjects into unobserved (latent) groups based on their responses to observed variables. The method relies on the Bayesian framework to incorporate prior knowledge and manage uncertainty, making it robust and flexible for various applications, such as in social sciences, medicine, and marketing.\n\nBasic Concepts of Latent Class Analysis (LCA)\nLCA models assume that there is an underlying categorical variable (latent class) that explains the patterns of responses observed in the data. Each subject belongs to one of these latent classes, and the probability of each observed response depends on the latent class membership.\n\n\nBayesian Framework\nIn the Bayesian approach, we introduce prior distributions for the model parameters and update these priors with the observed data to obtain posterior distributions. This incorporation of prior knowledge can help guide the analysis, especially when data is sparse or noisy.\n\n\nGenerative model\n\n\n\n\n\n\n\nG\n\n\ncluster_N\n\n\n\ncluster_J\n\n\n\n\nalpha\n\nu\nc\n\n\n\npi\n\nπ\n\n\n\nalpha-&gt;pi\n\n\n\n\n\nbeta\n\nα\njk\n, \nβ\njk\n\n\n\ntheta\n\nθ\njk\n\n\n\nbeta-&gt;theta\n\n\n\n\n\nc\n\nc\ni\n\n\n\npi-&gt;c\n\n\n\n\n\ny\n\ny\nik\n\n\n\ntheta-&gt;y\n\n\n\n\n\nc-&gt;y\n\n\n\n\n\nN_label\ni = 1 .. N subjects\n\n\n\nJ_label\nk = 1 .. K questions\n\n\n\n\n\n\n\n\nFrom the diagram we can see that the joint distribution can be decomposed into the following factors\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) =\n    P(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) P(\\mathbf{c}|\\pi)\n    P(\\pi|u) P(\\mathbf{\\Theta}| \\alpha, \\beta)\n\\]\nAssume \\(N\\) subjects are distributed over \\(C\\) classes.\nThe Dirichlet distribution is the conjugate prior of the Categorical and Multinomial distributions. The mean is \\(\\pi_j = u_j / \\sum_j u_j\\) while the mode is \\((u_j - 1)/\\sum_j(u_j - 1)\\) for \\(u_j &gt; 1\\). It is the appropriate prior when we need to make a \\(C\\)-way choice. The prior probabilities for the class membership is assumed to be Dirichlet\n\\[\n\\pi \\sim \\mathrm{Dirichlet}(u_1, \\ldots, u_C) \\propto \\prod_{j=1}^C \\pi_j^{u_j - 1}\n\\]\nThe probability that subject \\(i\\) is in class \\(j\\) is given by\n\\[\nP(c_{i} = j | \\pi) = \\pi_j\n\\]\nGiven \\(\\pi\\), the class assignments for the \\(N\\) subjects are independent. Therefore,\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{i=1}^N P(c_i|\\pi)\n\\]\nIf we let \\(N_j\\) denote the number of subjects in class \\(j\\), this expression can be simplified to\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{j=1}^C \\pi_j^{N_j}\n\\]\nLet \\(\\Theta_{jk}\\) be the probability that a subject of class \\(j\\) answers question \\(k\\) positive. Then\n\\[\nP(y_{ik} = 1 | c_{i} = j, \\mathbf{\\Theta}) = \\Theta_{jk}\n\\]\nand \\[\nP(y_{ik} = 0 | c_{i} = j, \\mathbf{\\Theta}) = 1 - \\Theta_{jk}\n\\]\nAnswering a yes/no question is a two-way choice (or Bernoulli experiment). The Beta distribution is an appropriate prior for a two-way choice, and we assume \\(\\Theta_{jk}\\) to be Beta distributed\n\\[\nP(\\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K \\mathrm{Beta}(\\alpha_{jk}, \\beta_{jk}) \\propto \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n\\]\nLooking at the plate diagram, we can see that the \\(K\\) questions the \\(N\\) subjects answer are independent given it classes \\(\\mathbf{c}\\) and parameters \\(\\mathbf{\\Theta}\\). Therefore,\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n  \\prod_{i=1}^N \\prod_{k=1}^K  \\Theta_{c_i k}^{y_{ik}}(1-\\Theta_{c_i k})^{1-y_{ik}}\n\\]\nThis expression can be simplified by counting how often the factors \\(\\Theta_{jk}\\) and \\((1-\\Theta_{jk})\\) occur.\nLet \\(N_{jk}\\) denote number of times the question \\(k\\) was answered positive for members of class \\(j\\), then \\(N_j - N_{jk}\\) the number of times it was answered negative.\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j - N_{jk}}\n\\]\nPutting everything together, we end up with the following expression for the joint probability\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j -N_{jk}} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{N_j} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{u_j - 1} \\right)\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K\n        \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n    \\right)\n\\]\nSome rearrangement yields\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n\\left( \\prod_{j=1}^C  \\pi_j^{N_j + u_j - 1} \\right)\n\\left(\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk} + \\alpha_{jk} - 1} (1 - \\Theta_{jk})^{N_j - N_{jk} + \\beta_{jk} - 1}\n\\right)\n\\]\nThis shows that the posterior joint distribution factors into a Dirichlet posterior for \\(\\pi\\) and a product of Beta posteriors for \\(\\Theta\\).\nWe can generate data according to the above model using the following Julia code\n\nusing Distributions\n\nC = 2       # number of classes, j = 1 .. C\nK = 3       # number of questions, k = 1 .. K\nN = 1000     # number of subjects, i = 1 .. N\n\nπ = [0.2, 0.8]\nΘ = [0.1 0.3 0.7;\n     0.5 0.8 0.1]   \n\nc = rand(Categorical(π), N)\n\ngen = (;π, Θ, c)\n\ny = rand.(Bernoulli.(Θ[c, :]));\n\n\nusing CairoMakie\n\nf = Figure(;size=(600, 400))\n\nfor j in 1:K\n    hist(f[1,j], y[:, j]; axis = (;title = \"Question $j\"))\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nGibbs sampling\nGibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to approximate the posterior distributions of the model parameters. It iteratively samples from the conditional distributions of each parameter given the current values of the others.\nAbove we had derived an expression for the joint probability \\(P(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}, \\mathbf{\\Theta})\\). As \\(\\mathbf{y}\\) is observed, we only need to create conditional samples for \\(\\mathbf{c}\\), \\(\\mathbf{\\pi}\\), and \\(\\mathbf{\\Theta}\\).\n\\[\nP(\\mathbf{\\pi} |  \\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\mathrm{Dirichlet}(\\ldots, u_j + N_j, \\ldots)\n\\]\n\\[\nP(\\Theta_{jk} | \\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}) =\n    \\mathrm{Beta}(\\alpha_{jk} + N_{jk}, \\beta_{jk} + N_j -  N_{jk})\n\\]\nTo draw a sample for \\(c_i\\), we have to compute \\[\nP(c_i | \\mathbf{y}, \\mathbf{c}_{-i}, \\mathbf{\\Theta}, \\pi)\n\\]\nwhere \\(\\mathbf{c}_{-i}\\) denotes the vector of class membership for all subjects except \\(i\\). From the plate diagram, we can see that conditioned on \\(\\pi\\) and \\(\\mathrm{\\Theta}\\), \\(c_i\\) only depends on \\(\\pi\\), \\(\\mathrm{\\Theta}\\), and \\(y_{ik}\\). We have\n\\[\nP(c_i, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    P(c_i|\\pi) \\prod_{k=1}^K P(y_{ik}|c_i, \\mathrm{\\Theta})\n\\]\n\\[\nP(c_i = j, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    \\pi_j \\prod_{k=1}^K \\Theta_{jk}^{y_{ik}}(1 - \\Theta_{jk})^{(1-y_{ik})}\n\\]\nThe Gibbs sampling algorithm can be summarized as follows:\n\nInitialize \\(\\mathbf{\\Theta}\\), \\(\\pi\\), \\(\\mathbf{c}\\)\ncompute \\(N_j\\), \\(N_{jk}\\)\nfor iter in 1:max_iter\n\nfor i in 1:N\n\nsample \\(c_i\\), and update \\(N_j\\) and \\(N_{jk}\\)\n\nsample \\(\\mathbf{\\pi}\\) using \\(N_j\\)\nsample \\(\\mathbf{\\Theta}\\) using \\(N_{jk}\\)\n\n\n\nusing DataFrames\nusing StatsBase: sample, Weights\nusing StatsFuns: logsumexp\n\n# Initialise\nu = ones(C)\nα = ones(C,K)\nβ = ones(C,K)\n\nπ = rand(Dirichlet(u))\nΘ = rand.(Beta.(α, β))\nc = rand(Categorical(π), N)\n\nN_j = zeros(C)      # number of subjects in class\nN_jk = zeros(C, K)  # number of positive answers for class j and question k\n\nfor i in 1:N\n    j = c[i]\n    N_j[j] += 1\n    for k in 1:K\n        N_jk[j, k] += y[i,k]\n    end\nend\n\nmax_iter = 11000\nburnin = 1000\nthinning = 10\n\nsamples = DataFrame()\n\nfor iter in 1:max_iter\n    # sample c[i]\n    for i in 1:N\n        # remove c[i]from N_j and N_jk\n        N_j[c[i]] -= 1\n        for k in 1:K\n            N_jk[c[i], k] -= y[i,k]\n        end\n        # compute p(c_i = j)\n        log_p_c = zeros(Float64, C)\n        for j in 1:C\n            log_p_c[j] += log(π[j])\n            for k in 1:K\n                log_p_c[j] += y[i,k]*log(Θ[j,k]) + (1.0 - y[i,k])*log(1.0 - Θ[j,k])\n            end\n        end\n        p_c = exp.(log_p_c .- logsumexp(log_p_c))\n        # and sample c[i]\n        c[i] = sample(1:K, Weights(p_c))\n        # add c[i] to N_j and N_jk\n        N_j[c[i]] += 1\n        for k in 1:K\n            N_jk[c[i], k] += y[i,k]\n        end\n    end\n\n    # sample \\pi\n    π = rand(Dirichlet(u .+ N_j))\n    \n    # sample \\Theta\n    Θ = rand.(Beta.(α .+ N_jk, β .+ N_j .- N_jk))\n\n    if iter &gt; burnin && (iter - burnin) % thinning == 0\n        push!(samples, (;π, Θ, c = copy(c)))\n    end\n\nend\n\nLet’s look at trace plots of the sampled parameters:\n\nf = Figure()\n\nlines(f[1:2,1], [x[1] for x in samples.π]; axis = (;title = \"π\"))\nlines!(f[1:2,1], [x[2] for x in samples.π])\nylims!(0,1)\n\nfor j in 1:C\n    for k in 1:K\n        lines(f[j,k+1], [x[j, k] for x in samples.Θ]; axis = (;title = \"Θ[$j,$k]\"))\n        ylims!(0,1)\n    end\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nExpectation-Maximization (EM)\nThe Expectation-Maximization method results in point estimates of the parameters \\(\\mathbf{\\Theta}\\) and \\(\\pi\\), and probability distributions of latent variables \\(\\mathbf{c}\\).\nSimilar to the Gibbs sampling, we compute for each subject the class membership probabilities \\(P(\\mathbf{c}|\\mathbf{\\Theta}, \\pi)\\). EM uses the joint class distributions to compute the MLE of parameters \\(\\pi\\) and \\(\\mathbf{\\Theta}\\).\n\\[\n\\pi_j = \\frac{\\sum_{i=1}^N P(c_i = j|\\mathbf{\\Theta}, \\pi)}{N} = \\frac{N_j}{N}\n\\]\n\\[\n\\Theta_{jk} = \\frac{\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi) y_{ik}}\n                    {\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi)} =\n                    \\frac{N_{jk}}{N_j}\n\\]\nIf one also incorporates prior counts (\\(u\\) and \\(\\alpha_{jk}\\), \\(\\beta_{jk]}\\) above), the MLE is replaced by a MAP estimate.\n\\[\n\\pi_j = \\frac{N_j + u_j - 1}{N + \\sum_{j=1}^C (u_j - 1)}\n\\]\n\\[\n\\Theta_{jk} = \\frac{N_{jk} + \\alpha_{jk} - 1}{N_j + \\alpha_{jk} + \\beta_{jk} - 2}\n\\]\n\n\nReferences\n\nResnik et al.(2010), Gibbs sampling for the uninitiated, http://users.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf\nLi et al.(2019), Bayesian Latent Class Analysis Tutorial https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6364555/, but here the notation seems seriously flawed"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]