[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Notes",
    "section": "",
    "text": "Introduction to Turing.jl\n\n\nA quick introduction to probabilistic programming language Turing.jl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensor notation\n\n\nIntroduction to tensor, index, or Einstein notation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Latent Class Analysis\n\n\nLatent class analysis using Gibbs sampling and Expectation-Maximization\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "posts/TensorNotation/index.html",
    "href": "posts/TensorNotation/index.html",
    "title": "Tensor notation",
    "section": "",
    "text": "Vectors\nIn tensor notation, a vector is written as\n\\[\n\\mathbf{u} = u_i\n\\]\nwhere \\(i = 1,2,3\\), corresponding to the \\(x\\), \\(y\\), and \\(z\\) component, respectively.\n\\[\n\\mathbf{u} + \\mathbf{v} = u_i + v_i\n\\]\n\n\nEinstein summation\nAny index appearing twice is automatically summed from 1 to 3. This is called Einstein summation. Any index can at most appear twice.\n\n\nDot (inner) product\nUsing Einstein summation, the dot (inner) product of two vectors is\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_i v_i = u_1 v_1 + u_2 v_2 + u_3 v_3\n\\]\n\n\nDyadic (outer) product\nThe dyadic (outer) product of two vectors is\n\\[\n\\mathbf{u} \\otimes \\mathbf{v} = u_i v_j =\n\\left[\n\\begin{matrix}\nu_1 v_1 & u_1 v_2 & u_1 v_3 \\cr\nu_2 v_1 & u_2 v_2 & u_2 v_3 \\cr\nu_3 v_1 & u_3 v_2 & u_3 v_3\n\\end{matrix}\n\\right]\n\\]\n\n\nDifferentiation with respect to time\nDifferentiation with respect to time can be written as\n\\[\n\\frac{\\text{d}\\mathbf{x}}{\\text{d}t} = \\mathbf{\\dot{x}} = \\dot{x_i} = x_{i|t}\n\\]\n\n\nDifferentiation with respect to space\nThe gradient of a scalar function is given by\n\\[\n\\nabla \\phi = \\partial_i \\phi = \\phi_{|i}\n\\]\nThe component-wise spatial derivative of a vector\n\\[\n\\frac{\\partial \\mathbf{u}}{\\partial x_j} = u_{i|j}\n\\]\nThe divergence of a vector is\n\\[\n\\text{div} \\ \\mathbf{u} = \\nabla \\cdot \\mathbf{u} = u_{i|i}\n\\]\n\n\nKronecker delta\nThe Kronecker delta is defined as\n\\[\n\\delta_{ij} = \\begin{cases}\n\\begin{matrix}\n1 & i = j \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nKronecker delta is also often called the substitution operator, as\n\\[\nu_i \\delta_{ij} = u_j\n\\]\nThe Kronecker delta often occurs when one deals with spatial derivatives of position\n\\[\n\\frac{\\partial x_i}{\\partial x_j} = \\partial_j x_i = x_{i|j} = \\delta_{ij}\n\\]\n\n\nLevi-Civita alternating tensor\n\\[\n\\epsilon_{ijk} = \\begin{cases}\n\\begin{matrix}\n1 & ijk \\in \\{123, 231, 312\\} \\cr\n-1 & ijk \\in \\{132, 213, 321\\} \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nSometimes the following equality can be useful to simplify expressions:\n\\[\n\\epsilon_{ijk} \\epsilon_{imn} = \\delta_{jm}\\delta_{kn}-\\delta_{jn}\\delta_{km}\n\\]\n\n\nRotation or curl of a vector\n\\[\n\\text{rot} \\ \\mathbf{u} = \\nabla \\times \\mathbf{u} = \\epsilon_{ijk} u_{k|j}\n\\]\nReferences: [https://www.continuummechanics.org]"
  },
  {
    "objectID": "posts/TuringIntro/index.html",
    "href": "posts/TuringIntro/index.html",
    "title": "Introduction to Turing.jl",
    "section": "",
    "text": "Turing.jl is a probabilistic proramming language that let’s us define a generative model and does inference."
  },
  {
    "objectID": "posts/TuringIntro/index.html#coin-example",
    "href": "posts/TuringIntro/index.html#coin-example",
    "title": "Introduction to Turing.jl",
    "section": "1 Coin example",
    "text": "1 Coin example\nLet’s flip a biased coin a hundred times.\n\nusing Turing\nusing DataFrames\ncoin_flips = rand(Bernoulli(0.7), 100);\n\nIn order to do inference, we need to specify a model.\n\\[ p \\sim Beta(1,1) \\]\n\\[ coin flip \\sim Bernoulli(p) \\]\n\n@model function coin(data)\n  p ~ Beta(1,1)\n  for i in eachindex(data) \n    data[i] ~ Bernoulli(p)\n  end\nend\n\ncoin (generic function with 2 methods)\n\n\n\n1.1 Sample from prior\n\nprior = sample(coin(coin_flips), Prior(), 1000)\n\nSampling:  40%|████████████████▋                        |  ETA: 0:00:00Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00\n\n\n\nChains MCMC chain (1000×2×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 1.16 seconds\nCompute duration  = 1.16 seconds\nparameters        = p\ninternals         = lp\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.5016    0.2885    0.0093   964.7822   940.1033    1.0021     ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           p    0.0376    0.2501    0.4939    0.7540    0.9760\n\n\n\n\n\nsummarystats(prior)\n\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.5016    0.2885    0.0093   964.7822   940.1033    1.0021     ⋯\n                                                                1 column omitted\n\n\n\n\n\nDataFrame(prior) \n\n\n1000×4 DataFrame975 rows omitted\n\n\n\nRow\niteration\nchain\np\nlp\n\n\n\nInt64\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n1\n1\n0.0461556\n0.0\n\n\n2\n2\n1\n0.594452\n0.0\n\n\n3\n3\n1\n0.159078\n0.0\n\n\n4\n4\n1\n0.536642\n0.0\n\n\n5\n5\n1\n0.14759\n0.0\n\n\n6\n6\n1\n0.982922\n0.0\n\n\n7\n7\n1\n0.342207\n0.0\n\n\n8\n8\n1\n0.352616\n0.0\n\n\n9\n9\n1\n0.600124\n0.0\n\n\n10\n10\n1\n0.914474\n0.0\n\n\n11\n11\n1\n0.889923\n0.0\n\n\n12\n12\n1\n0.201666\n0.0\n\n\n13\n13\n1\n0.228706\n0.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n989\n989\n1\n0.803291\n0.0\n\n\n990\n990\n1\n0.216907\n0.0\n\n\n991\n991\n1\n0.98858\n0.0\n\n\n992\n992\n1\n0.501805\n0.0\n\n\n993\n993\n1\n0.676283\n0.0\n\n\n994\n994\n1\n0.204099\n0.0\n\n\n995\n995\n1\n0.982323\n0.0\n\n\n996\n996\n1\n0.859361\n0.0\n\n\n997\n997\n1\n0.233004\n0.0\n\n\n998\n998\n1\n0.982051\n0.0\n\n\n999\n999\n1\n0.893269\n0.0\n\n\n1000\n1000\n1\n0.570736\n0.0\n\n\n\n\n\n\n\n\n\n1.2 Sample from posterior\n\nposterior = sample(coin(coin_flips), NUTS(), 1000)\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\nChains MCMC chain (1000×13×1 Array{Float64, 3}):\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 2.53 seconds\nCompute duration  = 2.53 seconds\nparameters        = p\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.7082    0.0461    0.0019   590.2686   677.9923    1.0008     ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           p    0.6128    0.6798    0.7105    0.7378    0.7954\n\n\n\n\n\nsummarystats(prior)\n\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.5016    0.2885    0.0093   964.7822   940.1033    1.0021     ⋯\n                                                                1 column omitted\n\n\n\n\n\nDataFrame(posterior)\n\n\n1000×15 DataFrame975 rows omitted\n\n\n\nRow\niteration\nchain\np\nlp\nn_steps\nis_accept\nacceptance_rate\nlog_density\nhamiltonian_energy\nhamiltonian_energy_error\nmax_hamiltonian_energy_error\ntree_depth\nnumerical_error\nstep_size\nnom_step_size\n\n\n\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n501\n1\n0.717998\n-61.828\n1.0\n1.0\n0.757963\n-61.828\n62.1002\n0.0\n0.27712\n1.0\n0.0\n1.5545\n1.5545\n\n\n2\n502\n1\n0.740088\n-62.0934\n3.0\n1.0\n0.894529\n-62.0934\n62.1186\n0.142159\n0.185425\n2.0\n0.0\n1.5545\n1.5545\n\n\n3\n503\n1\n0.740088\n-62.0934\n1.0\n1.0\n0.138025\n-62.0934\n64.4317\n0.0\n1.98032\n1.0\n0.0\n1.5545\n1.5545\n\n\n4\n504\n1\n0.740088\n-62.0934\n1.0\n1.0\n0.312093\n-62.0934\n63.4736\n0.0\n1.16445\n1.0\n0.0\n1.5545\n1.5545\n\n\n5\n505\n1\n0.662678\n-62.2271\n3.0\n1.0\n0.767041\n-62.2271\n62.5924\n-0.00127818\n0.524955\n2.0\n0.0\n1.5545\n1.5545\n\n\n6\n506\n1\n0.655636\n-62.3765\n1.0\n1.0\n0.913276\n-62.3765\n62.6489\n0.0907169\n0.0907169\n1.0\n0.0\n1.5545\n1.5545\n\n\n7\n507\n1\n0.609538\n-63.8566\n3.0\n1.0\n0.368152\n-63.8566\n64.3468\n0.927639\n1.18267\n2.0\n0.0\n1.5545\n1.5545\n\n\n8\n508\n1\n0.641929\n-62.7267\n1.0\n1.0\n1.0\n-62.7267\n63.8323\n-0.714576\n-0.714576\n1.0\n0.0\n1.5545\n1.5545\n\n\n9\n509\n1\n0.68078\n-61.9414\n1.0\n1.0\n1.0\n-61.9414\n62.3927\n-0.473854\n-0.473854\n1.0\n0.0\n1.5545\n1.5545\n\n\n10\n510\n1\n0.68078\n-61.9414\n3.0\n1.0\n0.123939\n-61.9414\n63.9924\n0.0\n2.85382\n2.0\n0.0\n1.5545\n1.5545\n\n\n11\n511\n1\n0.68078\n-61.9414\n1.0\n1.0\n0.185385\n-61.9414\n63.2305\n0.0\n1.68532\n1.0\n0.0\n1.5545\n1.5545\n\n\n12\n512\n1\n0.626875\n-63.1998\n3.0\n1.0\n0.668567\n-63.1998\n63.2018\n0.77562\n0.77562\n2.0\n0.0\n1.5545\n1.5545\n\n\n13\n513\n1\n0.669213\n-62.1074\n3.0\n1.0\n0.960222\n-62.1074\n62.8031\n-0.671604\n-0.671604\n2.0\n0.0\n1.5545\n1.5545\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n989\n1489\n1\n0.721976\n-61.8564\n1.0\n1.0\n0.970231\n-61.8564\n61.8636\n0.0302207\n0.0302207\n1.0\n0.0\n1.5545\n1.5545\n\n\n990\n1490\n1\n0.721976\n-61.8564\n1.0\n1.0\n0.0751755\n-61.8564\n63.6342\n0.0\n2.58793\n1.0\n0.0\n1.5545\n1.5545\n\n\n991\n1491\n1\n0.686017\n-61.8859\n3.0\n1.0\n0.992161\n-61.8859\n61.9016\n0.0154505\n0.0154505\n2.0\n0.0\n1.5545\n1.5545\n\n\n992\n1492\n1\n0.686017\n-61.8859\n1.0\n1.0\n0.867892\n-61.8859\n62.0547\n0.0\n0.141688\n1.0\n0.0\n1.5545\n1.5545\n\n\n993\n1493\n1\n0.686017\n-61.8859\n1.0\n1.0\n0.143066\n-61.8859\n63.309\n0.0\n1.94445\n1.0\n0.0\n1.5545\n1.5545\n\n\n994\n1494\n1\n0.686017\n-61.8859\n1.0\n1.0\n0.296269\n-61.8859\n62.8227\n0.0\n1.21649\n1.0\n0.0\n1.5545\n1.5545\n\n\n995\n1495\n1\n0.690538\n-61.8481\n1.0\n1.0\n1.0\n-61.8481\n61.8991\n-0.0219391\n-0.0219391\n1.0\n0.0\n1.5545\n1.5545\n\n\n996\n1496\n1\n0.714896\n-61.8115\n3.0\n1.0\n0.884779\n-61.8115\n61.9816\n-0.0106187\n0.211995\n2.0\n0.0\n1.5545\n1.5545\n\n\n997\n1497\n1\n0.769881\n-62.9041\n1.0\n1.0\n0.564167\n-62.9041\n62.9198\n0.572406\n0.572406\n1.0\n0.0\n1.5545\n1.5545\n\n\n998\n1498\n1\n0.821201\n-65.8279\n3.0\n1.0\n0.499564\n-65.8279\n65.836\n1.05356\n1.89714\n2.0\n0.0\n1.5545\n1.5545\n\n\n999\n1499\n1\n0.785157\n-63.5502\n3.0\n1.0\n0.850635\n-63.5502\n65.5869\n-0.976817\n-1.6987\n2.0\n0.0\n1.5545\n1.5545\n\n\n1000\n1500\n1\n0.781857\n-63.3961\n3.0\n1.0\n0.980302\n-63.3961\n64.2692\n-0.0706388\n-0.0827444\n2.0\n0.0\n1.5545\n1.5545\n\n\n\n\n\n\n\n\n\n1.3 Prior predictive check\nHere we pass in a vector containing missing. Sampling then generates observations.\n\nobservations = Vector{Missing}(missing, length(coin_flips))\nprior_check = predict(coin(observations), prior)\n\n\nChains MCMC chain (1000×100×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = data[1], data[2], data[3], data[4], data[5], data[6], data[7], data[8], data[9], data[10], data[11], data[12], data[13], data[14], data[15], data[16], data[17], data[18], data[19], data[20], data[21], data[22], data[23], data[24], data[25], data[26], data[27], data[28], data[29], data[30], data[31], data[32], data[33], data[34], data[35], data[36], data[37], data[38], data[39], data[40], data[41], data[42], data[43], data[44], data[45], data[46], data[47], data[48], data[49], data[50], data[51], data[52], data[53], data[54], data[55], data[56], data[57], data[58], data[59], data[60], data[61], data[62], data[63], data[64], data[65], data[66], data[67], data[68], data[69], data[70], data[71], data[72], data[73], data[74], data[75], data[76], data[77], data[78], data[79], data[80], data[81], data[82], data[83], data[84], data[85], data[86], data[87], data[88], data[89], data[90], data[91], data[92], data[93], data[94], data[95], data[96], data[97], data[98], data[99], data[100]\ninternals         = \nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk   ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64     Float64    Float64   Float64    ⋯\n     data[1]    0.5230    0.4997    0.0147   1157.0934        NaN    0.9995    ⋯\n     data[2]    0.4970    0.5002    0.0162    957.8177        NaN    0.9990    ⋯\n     data[3]    0.4790    0.4998    0.0171    852.2803        NaN    0.9995    ⋯\n     data[4]    0.5000    0.5003    0.0158   1000.9674        NaN       NaN    ⋯\n     data[5]    0.4930    0.5002    0.0158    996.2614        NaN    0.9990    ⋯\n     data[6]    0.4960    0.5002    0.0168    882.8375        NaN    1.0017    ⋯\n     data[7]    0.4920    0.5002    0.0160    976.4481        NaN    0.9993    ⋯\n     data[8]    0.4940    0.5002    0.0161    967.4698        NaN    0.9990    ⋯\n     data[9]    0.5130    0.5001    0.0156   1028.7241        NaN    0.9991    ⋯\n    data[10]    0.5070    0.5002    0.0162    953.9188        NaN    0.9992    ⋯\n    data[11]    0.4850    0.5000    0.0157   1008.2859        NaN    0.9997    ⋯\n    data[12]    0.5070    0.5002    0.0158   1003.4037        NaN    1.0015    ⋯\n    data[13]    0.5010    0.5002    0.0160    972.1132        NaN    0.9993    ⋯\n    data[14]    0.4910    0.5002    0.0155   1044.3628        NaN    0.9991    ⋯\n    data[15]    0.5040    0.5002    0.0159    990.3171        NaN    1.0000    ⋯\n    data[16]    0.5110    0.5001    0.0160    977.4628        NaN    0.9991    ⋯\n    data[17]    0.5150    0.5000    0.0151   1103.7482        NaN    0.9990    ⋯\n    data[18]    0.4930    0.5002    0.0156   1023.7561        NaN    0.9999    ⋯\n    data[19]    0.5070    0.5002    0.0162    957.7321        NaN    0.9993    ⋯\n    data[20]    0.4880    0.5001    0.0154   1051.4843        NaN    0.9990    ⋯\n    data[21]    0.5080    0.5002    0.0160    978.7747        NaN    0.9991    ⋯\n    data[22]    0.4950    0.5002    0.0155   1038.6265        NaN    1.0004    ⋯\n    data[23]    0.4920    0.5002    0.0163    943.3135        NaN    1.0013    ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮          ⋮          ⋮       ⋱\n                                                    1 column and 77 rows omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n     data[1]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[2]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[3]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[4]    0.0000    0.0000    0.5000    1.0000    1.0000\n     data[5]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[6]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[7]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[8]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[9]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[10]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[11]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[12]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[13]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[14]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[15]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[16]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[17]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[18]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[19]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[20]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[21]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[22]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[23]    0.0000    0.0000    0.0000    1.0000    1.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                 77 rows omitted\n\n\n\n\nprior_check is a Chains instance. To convert to an Array, use\n\nArray(prior_check, [:parameters])\n\n1000×100 Matrix{Float64}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  1.0  1.0  0.0\n 0.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0     0.0  1.0  1.0  0.0  1.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  1.0  1.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0     1.0  1.0  1.0  1.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0     1.0  0.0  0.0  1.0  0.0  0.0  1.0\n 1.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  1.0  1.0  0.0  1.0  1.0  1.0  0.0     1.0  0.0  0.0  1.0  1.0  1.0  0.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  …  1.0  1.0  0.0  1.0  1.0  0.0  1.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0     0.0  1.0  0.0  1.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  1.0  1.0  0.0  0.0  0.0  0.0\n ⋮                        ⋮              ⋱            ⋮                   \n 0.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 1.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0     1.0  0.0  0.0  0.0  0.0  1.0  1.0\n 1.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0     1.0  0.0  0.0  1.0  1.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0     0.0  0.0  0.0  1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  1.0  0.0  0.0  0.0  1.0  0.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  0.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  0.0  1.0  0.0  1.0  0.0  1.0     0.0  1.0  1.0  1.0  1.0  0.0  1.0\n\n\n\n\n1.4 Posterior predictive check\n\nobservations = Vector{Missing}(missing, length(coin_flips))\nposterior_check = predict(coin(observations), posterior)\n\n\nChains MCMC chain (1000×100×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = data[1], data[2], data[3], data[4], data[5], data[6], data[7], data[8], data[9], data[10], data[11], data[12], data[13], data[14], data[15], data[16], data[17], data[18], data[19], data[20], data[21], data[22], data[23], data[24], data[25], data[26], data[27], data[28], data[29], data[30], data[31], data[32], data[33], data[34], data[35], data[36], data[37], data[38], data[39], data[40], data[41], data[42], data[43], data[44], data[45], data[46], data[47], data[48], data[49], data[50], data[51], data[52], data[53], data[54], data[55], data[56], data[57], data[58], data[59], data[60], data[61], data[62], data[63], data[64], data[65], data[66], data[67], data[68], data[69], data[70], data[71], data[72], data[73], data[74], data[75], data[76], data[77], data[78], data[79], data[80], data[81], data[82], data[83], data[84], data[85], data[86], data[87], data[88], data[89], data[90], data[91], data[92], data[93], data[94], data[95], data[96], data[97], data[98], data[99], data[100]\ninternals         = \nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk   ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64     Float64    Float64   Float64    ⋯\n     data[1]    0.7430    0.4372    0.0134   1072.3149        NaN    0.9996    ⋯\n     data[2]    0.6910    0.4623    0.0154    906.1264        NaN    0.9991    ⋯\n     data[3]    0.7180    0.4502    0.0148    924.7892        NaN    0.9993    ⋯\n     data[4]    0.7060    0.4558    0.0146    975.4260        NaN    0.9991    ⋯\n     data[5]    0.6880    0.4635    0.0147    992.9012        NaN    0.9990    ⋯\n     data[6]    0.6890    0.4631    0.0143   1043.4965        NaN    1.0003    ⋯\n     data[7]    0.7060    0.4558    0.0151    914.4906        NaN    0.9990    ⋯\n     data[8]    0.6970    0.4598    0.0150    935.9213        NaN    1.0011    ⋯\n     data[9]    0.7100    0.4540    0.0170    714.6554        NaN    1.0018    ⋯\n    data[10]    0.7240    0.4472    0.0137   1068.6520        NaN    0.9992    ⋯\n    data[11]    0.6760    0.4682    0.0151    962.1669        NaN    1.0002    ⋯\n    data[12]    0.7010    0.4580    0.0153    895.2473        NaN    1.0001    ⋯\n    data[13]    0.7010    0.4580    0.0142   1038.8307        NaN    0.9994    ⋯\n    data[14]    0.7040    0.4567    0.0150    928.0940        NaN    0.9991    ⋯\n    data[15]    0.7170    0.4507    0.0144    983.5115        NaN    0.9990    ⋯\n    data[16]    0.6990    0.4589    0.0139   1095.3970        NaN    0.9996    ⋯\n    data[17]    0.7110    0.4535    0.0142   1023.9128        NaN    0.9996    ⋯\n    data[18]    0.7210    0.4487    0.0145    957.3748        NaN    0.9992    ⋯\n    data[19]    0.6650    0.4722    0.0158    890.8334        NaN    0.9990    ⋯\n    data[20]    0.7290    0.4447    0.0146    922.2537        NaN    0.9992    ⋯\n    data[21]    0.6900    0.4627    0.0151    934.9079        NaN    0.9993    ⋯\n    data[22]    0.6970    0.4598    0.0159    833.4933        NaN    1.0001    ⋯\n    data[23]    0.7110    0.4535    0.0149    924.5675        NaN    0.9996    ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮          ⋮          ⋮       ⋱\n                                                    1 column and 77 rows omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n     data[1]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[2]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[3]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[4]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[5]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[6]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[7]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[8]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[9]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[10]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[11]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[12]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[13]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[14]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[15]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[16]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[17]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[18]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[19]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[20]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[21]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[22]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[23]    0.0000    0.0000    1.0000    1.0000    1.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                 77 rows omitted\n\n\n\n\n\n\n1.5 Chains\nThe results of sampling runs are MCMCChains.Chains instances.\nA Chains instance has the following fields:\n\nvalue: AxisArray object :iter x :var x :chain\nlogevidence\nname_map (to define sections, NamedTuple section -&gt; names, default section is :parameters)\ninfo\n\n\nvalue = rand(500, 2, 3)\nchn = Chains(value, [:a, :b])\nchn2 = Chains(value, [\"A[1]\", \"A[2]\"])\n\n\nChains MCMC chain (500×2×3 Array{Float64, 3}):\nIterations        = 1:1:500\nNumber of chains  = 3\nSamples per chain = 500\nparameters        = A[1], A[2]\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n        A[1]    0.4948    0.2824    0.0070   1587.1995   1410.8704    0.9998   ⋯\n        A[2]    0.4918    0.2864    0.0075   1466.7437   1366.1185    0.9998   ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n        A[1]    0.0228    0.2608    0.4955    0.7333    0.9676\n        A[2]    0.0290    0.2362    0.4924    0.7415    0.9770\n\n\n\n\nOne can restrict the output (an AxisArray) to some parameters by using chn[:a], for example. To get a Chains instance with only parameter a, do chn[[:a]]. Weird!\nAn AxisArray has fields :axes and :data. It has advanced indexing. I.e. to get only a slice of a dimension :dim, one can use A[dim = idx], so in the case of a MCMC sample, we might use chn.value[var = :a], or chn.value[chain = 2]\nsummarystats(chn; sections = :parameters)\nquantile(chn; sections = :parameters)\nor describe(chn; sections = :parameters) to get both at once\nTurings sample returns a Chains instance with sections :parameters and :internals.\n\nposterior.name_map\n\n(parameters = [:p], internals = [:lp, :n_steps, :is_accept, :acceptance_rate, :log_density, :hamiltonian_energy, :hamiltonian_energy_error, :max_hamiltonian_energy_error, :tree_depth, :numerical_error, :step_size, :nom_step_size])\n\n\nTo convert a chain to a DataFrame containing the :parameters section only, one can do\n\nDataFrame(posterior[posterior.name_map.parameters])\n\n# or DataFrame(MCMCchains.get_sections(posterior, :parameters))\n\n\n1000×3 DataFrame975 rows omitted\n\n\n\nRow\niteration\nchain\np\n\n\n\nInt64\nInt64\nFloat64\n\n\n\n\n1\n501\n1\n0.717998\n\n\n2\n502\n1\n0.740088\n\n\n3\n503\n1\n0.740088\n\n\n4\n504\n1\n0.740088\n\n\n5\n505\n1\n0.662678\n\n\n6\n506\n1\n0.655636\n\n\n7\n507\n1\n0.609538\n\n\n8\n508\n1\n0.641929\n\n\n9\n509\n1\n0.68078\n\n\n10\n510\n1\n0.68078\n\n\n11\n511\n1\n0.68078\n\n\n12\n512\n1\n0.626875\n\n\n13\n513\n1\n0.669213\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n989\n1489\n1\n0.721976\n\n\n990\n1490\n1\n0.721976\n\n\n991\n1491\n1\n0.686017\n\n\n992\n1492\n1\n0.686017\n\n\n993\n1493\n1\n0.686017\n\n\n994\n1494\n1\n0.686017\n\n\n995\n1495\n1\n0.690538\n\n\n996\n1496\n1\n0.714896\n\n\n997\n1497\n1\n0.769881\n\n\n998\n1498\n1\n0.821201\n\n\n999\n1499\n1\n0.785157\n\n\n1000\n1500\n1\n0.781857"
  },
  {
    "objectID": "posts/BayesianLCA/index.html",
    "href": "posts/BayesianLCA/index.html",
    "title": "Bayesian Latent Class Analysis",
    "section": "",
    "text": "Bayesian Latent Class Analysis (BLCA) is a powerful statistical method used to classify subjects into unobserved (latent) groups based on their responses to observed variables. The method relies on the Bayesian framework to incorporate prior knowledge and manage uncertainty, making it robust and flexible for various applications, such as in social sciences, medicine, and marketing.\n\nBasic Concepts of Latent Class Analysis (LCA)\nLCA models assume that there is an underlying categorical variable (latent class) that explains the patterns of responses observed in the data. Each subject belongs to one of these latent classes, and the probability of each observed response depends on the latent class membership.\n\n\nBayesian Framework\nIn the Bayesian approach, we introduce prior distributions for the model parameters and update these priors with the observed data to obtain posterior distributions. This incorporation of prior knowledge can help guide the analysis, especially when data is sparse or noisy.\n\n\nGenerative model\nThe plate diagram for the model is shown below:\n\n\n\n\n\n\n\nG\n\n\ncluster_N\n\n\n\ncluster_J\n\n\n\n\nalpha\n\nu\nc\n\n\n\npi\n\nπ\n\n\n\nalpha-&gt;pi\n\n\n\n\n\nbeta\n\nα\njk\n, \nβ\njk\n\n\n\ntheta\n\nθ\njk\n\n\n\nbeta-&gt;theta\n\n\n\n\n\nc\n\nc\ni\n\n\n\npi-&gt;c\n\n\n\n\n\ny\n\ny\nik\n\n\n\ntheta-&gt;y\n\n\n\n\n\nc-&gt;y\n\n\n\n\n\nN_label\ni = 1 .. N subjects\n\n\n\nJ_label\nk = 1 .. K questions\n\n\n\n\n\n\n\n\nFrom the diagram we can see that the joint distribution can be decomposed into the following factors\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) =\n    P(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) P(\\mathbf{c}|\\pi)\n    P(\\pi|u) P(\\mathbf{\\Theta}| \\alpha, \\beta)\n\\]\nAssume \\(N\\) subjects are distributed over \\(C\\) classes.\nThe Dirichlet distribution is the conjugate prior of the Categorical and Multinomial distributions. The mean is \\(\\pi_j = u_j / \\sum_j u_j\\) while the mode is \\((u_j - 1)/\\sum_j(u_j - 1)\\) for \\(u_j &gt; 1\\). It is the appropriate prior when we need to make a \\(C\\)-way choice. The prior probabilities for the class membership is assumed to be Dirichlet\n\\[\n\\pi \\sim \\mathrm{Dirichlet}(u_1, \\ldots, u_C) \\propto \\prod_{j=1}^C \\pi_j^{u_j - 1}\n\\]\nThe probability that subject \\(i\\) is in class \\(j\\) is given by\n\\[\nP(c_{i} = j | \\pi) = \\pi_j\n\\]\nGiven \\(\\pi\\), the class assignments for the \\(N\\) subjects are independent. Therefore,\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{i=1}^N P(c_i|\\pi)\n\\]\nIf we let \\(N_j\\) denote the number of subjects in class \\(j\\), this expression can be simplified to\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{j=1}^C \\pi_j^{N_j}\n\\]\nLet \\(\\Theta_{jk}\\) be the probability that a subject of class \\(j\\) answers question \\(k\\) positive. Then\n\\[\nP(y_{ik} = 1 | c_{i} = j, \\mathbf{\\Theta}) = \\Theta_{jk}\n\\]\nand \\[\nP(y_{ik} = 0 | c_{i} = j, \\mathbf{\\Theta}) = 1 - \\Theta_{jk}\n\\]\nAnswering a yes/no question is a two-way choice (or Bernoulli experiment). The Beta distribution is an appropriate prior for a two-way choice, and we assume \\(\\Theta_{jk}\\) to be Beta distributed\n\\[\nP(\\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K \\mathrm{Beta}(\\alpha_{jk}, \\beta_{jk}) \\propto \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n\\]\nLooking at the plate diagram, we can see that the \\(K\\) questions the \\(N\\) subjects answer are independent given it classes \\(\\mathbf{c}\\) and parameters \\(\\mathbf{\\Theta}\\). Therefore,\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n  \\prod_{i=1}^N \\prod_{k=1}^K  \\Theta_{c_i k}^{y_{ik}}(1-\\Theta_{c_i k})^{1-y_{ik}}\n\\]\nThis expression can be simplified by counting how often the factors \\(\\Theta_{jk}\\) and \\((1-\\Theta_{jk})\\) occur.\nLet \\(N_{jk}\\) denote number of times the question \\(k\\) was answered positive for members of class \\(j\\), then \\(N_j - N_{jk}\\) the number of times it was answered negative.\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j - N_{jk}}\n\\]\nPutting everything together, we end up with the following expression for the joint probability\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j -N_{jk}} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{N_j} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{u_j - 1} \\right)\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K\n        \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n    \\right)\n\\]\nSome rearrangement yields\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n\\left( \\prod_{j=1}^C  \\pi_j^{N_j + u_j - 1} \\right)\n\\left(\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk} + \\alpha_{jk} - 1} (1 - \\Theta_{jk})^{N_j - N_{jk} + \\beta_{jk} - 1}\n\\right)\n\\]\nThis shows that the posterior joint distribution factors into a Dirichlet posterior for \\(\\pi\\) and a product of Beta posteriors for \\(\\Theta\\).\nWe can generate data according to the above model using the following Julia code\n\nusing Distributions\n\nC = 2       # number of classes, j = 1 .. C\nK = 3       # number of questions, k = 1 .. K\nN = 1000     # number of subjects, i = 1 .. N\n\nπ = [0.2, 0.8]\nΘ = [0.1 0.3 0.7;\n     0.5 0.8 0.1]   \n\nc = rand(Categorical(π), N)\n\ngen = (;π, Θ, c)\n\ny = rand.(Bernoulli.(Θ[c, :]));\n\n\nusing CairoMakie\n\nf = Figure(;size=(600, 400))\n\nfor j in 1:K\n    hist(f[1,j], y[:, j]; axis = (;title = \"Question $j\"))\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nGibbs sampling\nGibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to approximate the posterior distributions of the model parameters. It iteratively samples from the conditional distributions of each parameter given the current values of the others.\nAbove we had derived an expression for the joint probability \\(P(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}, \\mathbf{\\Theta})\\). As \\(\\mathbf{y}\\) is observed, we only need to create conditional samples for \\(\\mathbf{c}\\), \\(\\mathbf{\\pi}\\), and \\(\\mathbf{\\Theta}\\).\n\\[\nP(\\mathbf{\\pi} |  \\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\mathrm{Dirichlet}(\\ldots, u_j + N_j, \\ldots)\n\\]\n\\[\nP(\\Theta_{jk} | \\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}) =\n    \\mathrm{Beta}(\\alpha_{jk} + N_{jk}, \\beta_{jk} + N_j -  N_{jk})\n\\]\nTo draw a sample for \\(c_i\\), we have to compute \\[\nP(c_i | \\mathbf{y}, \\mathbf{c}_{-i}, \\mathbf{\\Theta}, \\pi)\n\\]\nwhere \\(\\mathbf{c}_{-i}\\) denotes the vector of class membership for all subjects except \\(i\\). From the plate diagram, we can see that conditioned on \\(\\pi\\) and \\(\\mathrm{\\Theta}\\), \\(c_i\\) only depends on \\(\\pi\\), \\(\\mathrm{\\Theta}\\), and \\(y_{ik}\\). We have\n\\[\nP(c_i, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    P(c_i|\\pi) \\prod_{k=1}^K P(y_{ik}|c_i, \\mathrm{\\Theta})\n\\]\n\\[\nP(c_i = j, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    \\pi_j \\prod_{k=1}^K \\Theta_{jk}^{y_{ik}}(1 - \\Theta_{jk})^{(1-y_{ik})}\n\\]\nThe Gibbs sampling algorithm can be summarized as follows:\n\nInitialize \\(\\mathbf{\\Theta}\\), \\(\\pi\\), \\(\\mathbf{c}\\)\ncompute \\(N_j\\), \\(N_{jk}\\)\nfor iter in 1:max_iter\n\nfor i in 1:N\n\nsample \\(c_i\\), and update \\(N_j\\) and \\(N_{jk}\\)\n\nsample \\(\\mathbf{\\pi}\\) using \\(N_j\\)\nsample \\(\\mathbf{\\Theta}\\) using \\(N_{jk}\\)\n\n\n\nusing DataFrames\nusing StatsBase: sample, Weights\nusing StatsFuns: logsumexp\n\n# Initialise\nu = ones(C)\nα = ones(C,K)\nβ = ones(C,K)\n\nπ = rand(Dirichlet(u))\nΘ = rand.(Beta.(α, β))\nc = rand(Categorical(π), N)\n\nN_j = zeros(C)      # number of subjects in class\nN_jk = zeros(C, K)  # number of positive answers for class j and question k\n\nfor i in 1:N\n    j = c[i]\n    N_j[j] += 1\n    for k in 1:K\n        N_jk[j, k] += y[i,k]\n    end\nend\n\nmax_iter = 11000\nburnin = 1000\nthinning = 10\n\nsamples = DataFrame()\n\nfor iter in 1:max_iter\n    # sample c[i]\n    for i in 1:N\n        # remove c[i]from N_j and N_jk\n        N_j[c[i]] -= 1\n        for k in 1:K\n            N_jk[c[i], k] -= y[i,k]\n        end\n        # compute p(c_i = j)\n        log_p_c = zeros(Float64, C)\n        for j in 1:C\n            log_p_c[j] += log(π[j])\n            for k in 1:K\n                log_p_c[j] += y[i,k]*log(Θ[j,k]) + (1.0 - y[i,k])*log(1.0 - Θ[j,k])\n            end\n        end\n        p_c = exp.(log_p_c .- logsumexp(log_p_c))\n        # and sample c[i]\n        c[i] = sample(1:K, Weights(p_c))\n        # add c[i] to N_j and N_jk\n        N_j[c[i]] += 1\n        for k in 1:K\n            N_jk[c[i], k] += y[i,k]\n        end\n    end\n\n    # sample \\pi\n    π = rand(Dirichlet(u .+ N_j))\n    \n    # sample \\Theta\n    Θ = rand.(Beta.(α .+ N_jk, β .+ N_j .- N_jk))\n\n    if iter &gt; burnin && (iter - burnin) % thinning == 0\n        push!(samples, (;π, Θ, c = copy(c)))\n    end\n\nend\n\nLet’s look at trace plots of the sampled parameters:\n\nf = Figure()\n\nlines(f[1:2,1], [x[1] for x in samples.π]; axis = (;title = \"π\"))\nlines!(f[1:2,1], [x[2] for x in samples.π])\nylims!(0,1)\n\nfor j in 1:C\n    for k in 1:K\n        lines(f[j,k+1], [x[j, k] for x in samples.Θ]; axis = (;title = \"Θ[$j,$k]\"))\n        ylims!(0,1)\n    end\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nExpectation-Maximization (EM)\nThe Expectation-Maximization method results in point estimates of the parameters \\(\\mathbf{\\Theta}\\) and \\(\\pi\\), and probability distributions of latent variables \\(\\mathbf{c}\\).\nSimilar to the Gibbs sampling, we compute for each subject the class membership probabilities \\(P(\\mathbf{c}|\\mathbf{\\Theta}, \\pi)\\). EM uses the joint class distributions to compute the MLE of parameters \\(\\pi\\) and \\(\\mathbf{\\Theta}\\).\n\\[\n\\pi_j = \\frac{\\sum_{i=1}^N P(c_i = j|\\mathbf{\\Theta}, \\pi)}{N} = \\frac{N_j}{N}\n\\]\n\\[\n\\Theta_{jk} = \\frac{\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi) y_{ik}}\n                    {\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi)} =\n                    \\frac{N_{jk}}{N_j}\n\\]\nIf one also incorporates prior counts (\\(u\\) and \\(\\alpha_{jk}\\), \\(\\beta_{jk]}\\) above), the MLE is replaced by a MAP estimate.\n\\[\n\\pi_j = \\frac{N_j + u_j - 1}{N + \\sum_{j=1}^C (u_j - 1)}\n\\]\n\\[\n\\Theta_{jk} = \\frac{N_{jk} + \\alpha_{jk} - 1}{N_j + \\alpha_{jk} + \\beta_{jk} - 2}\n\\]\n\n\nReferences\n\nResnik et al.(2010), Gibbs sampling for the uninitiated, http://users.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf\nLi et al.(2019), Bayesian Latent Class Analysis Tutorial https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6364555/, but here the notation seems seriously flawed"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]