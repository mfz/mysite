[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Notes",
    "section": "",
    "text": "Double pendulum\n\n\n\njulia\n\n\nphysics\n\n\n\nSimulating a double pendulum in Julia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Turing.jl\n\n\nA quick introduction to probabilistic programming language Turing.jl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime-dependent Schrödinger equation\n\n\n\njulia\n\n\nphysics\n\n\n\nSolving the time-dependent Schrödinger equation in Julia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensor notation\n\n\n\nphysics\n\n\n\nIntroduction to tensor, index, or Einstein notation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPendulum\n\n\n\njulia\n\n\nphysics\n\n\n\nSimulating a pendulum in Julia, analytically, numerically, and symbolically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Latent Class Analysis\n\n\n\njulia\n\n\nBayes\n\n\nMCMC\n\n\nEM\n\n\n\nLatent class analysis using Gibbs sampling and Expectation-Maximization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLattice Boltzmann Method (LBM)\n\n\n\njulia\n\n\nphysics\n\n\nCFD\n\n\n\nLattice Boltzmann Method (LBM) for 2-dimensional flow past cylinder in Julia\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nFor a nice tutorial on how to create them visit https://ucsb-meds.github.io/creating-quarto-websites/"
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html",
    "href": "posts/LatticeBoltzmannMethod/index.html",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "",
    "text": "The Lattice Boltzmann Method (LBM) is a powerful computational technique for simulating fluid dynamics. It is built on the principles of molecular dynamics and the Boltzmann equation but simplifies these concepts to create a more computationally efficient method."
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html#boltzmann-equation-and-the-bhatnagar-gross-krook-bgk-approximation",
    "href": "posts/LatticeBoltzmannMethod/index.html#boltzmann-equation-and-the-bhatnagar-gross-krook-bgk-approximation",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "Boltzmann Equation and the Bhatnagar-Gross-Krook (BGK) approximation",
    "text": "Boltzmann Equation and the Bhatnagar-Gross-Krook (BGK) approximation\nMolecular dynamics (MD) models the motion of individual particles based on Newton’s laws, providing detailed insights into microscopic properties. However, MD is computationally expensive for simulating large-scale fluid flows.\nThe Boltzmann equation offers a bridge between microscopic particle dynamics and macroscopic fluid properties. Instead of following each particle, the Boltzmann equation looks at the evolution of the so-called particle distribution function \\(f(x, \\xi, t)\\), which describes the probability of having a particle with position \\(x\\) and molecular velocity \\(\\xi\\) at time \\(t\\).\nKnowledge of the particle distribution function allows computation of macroscopic variables like density\n\\[\n\\int f(\\xi, x, t) d^3\\xi = \\rho(t)\n\\]\nand momentum\n\\[\n\\int \\xi f(\\xi, x, t) d^3\\xi = u \\rho(t)\n\\]\nThe Boltzmann equation describes how the particle distribution function evolves over time\n\\[\n\\frac{d}{dt}f(\\xi,x,t) = \\partial_t f + \\frac{d\\xi}{dt} \\partial_\\xi f + \\frac{dx}{dt} \\partial_x f = \\Omega(f)\n\\]\nHere \\(d\\xi/dt\\) describes external forces and the collision operator \\(\\Omega(f)\\) represents the effect of collisions between particles.\nIn the Bhatnagar-Gross-Krook (BGK) approximation, the collision operator is defined as\n\\[\n\\Omega(f) = - \\frac{1}{\\tau}(f - f^{eq})\n\\]\ni.e. the particle distribution relaxes towards its equilibrium distribution \\(f^{eq}\\) at the relaxation time scale \\(\\tau\\).\nIgnoring external forces (the term with \\(d\\xi/dt\\)), the Boltzmann equation in the BGK approximation can then be written as\n\\[\n\\frac{d}{dt}f(\\xi,x,t) = \\partial_t f + \\xi \\partial_x f = - \\frac{1}{\\tau}(f - f^{eq})\n\\]\nThe change of the particle distribution function within a cell is, therefore, due to two mechanisms - streaming of particles into and out of the cell - collision of particles within the cell, which leads to a relaxation towards the equilibrium condition.\nTODO: Eulerian vs Lagrangian perspective\nThe equilibrium distribution \\(f^{eq}\\) is given by the Maxwell-Boltzmann distribution\n\\[\nf_{eq}(\\mathbf{v}, \\mathbf{x}, t) = \\rho \\left( \\frac{1}{2\\pi RT} \\right)^{3/2} e^{-\\frac{v^2}{2RT}}\n                                     = \\rho \\left( \\frac{1}{2\\pi RT} \\right)^{3/2} e^{\\frac{-|\\mathbf{\\xi} - \\mathbf{u}|^2}{2RT}}\n\\]\nHere \\(\\mathbf{u}\\) is the macroscopic velocity, \\(\\xi\\) the absolute molecular velocity, and \\(\\mathbf{v}=\\xi-\\mathbf{u}\\) the relative molecular velocity, \\(R\\) is the kinetic gas constant and \\(T\\) the temperature.\nNote: While the BGK approximation is good enough to recover Navier-Stokes behaviour, it is not really a good approximation to the Boltzmann equation."
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html#lattice-boltzmann-method",
    "href": "posts/LatticeBoltzmannMethod/index.html#lattice-boltzmann-method",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "Lattice Boltzmann method",
    "text": "Lattice Boltzmann method\nTo solve the Boltzmann equation numerically, it needs to be discretized in space, time, and velocity. Time is discretized into time steps \\(\\Delta t\\) and space is discretized into a lattice with spacing \\(\\Delta x\\). The trick in the discretation of the velocity space is to choose a finite number of velocities in such a way that in a single time step \\(\\Delta t\\), particles can either - stay in their cell or - move to one of the neighbouring cells.\nThe D2Q9 scheme is a two-dimensional, nine-velocity model used in the LBM. The term “D2Q9” stands for two dimensions (D2) and nine discrete velocities (Q9). The scheme consists of a square lattice where each lattice node is connected to its neighbors via nine possible velocity vectors, including the zero vector (rest particle).\nThe nine discrete velocities in the D2Q9 scheme are typically represented as follows:\n\\[\n\\mathbf{e}_i =\n\\begin{cases}\n(0, 0) & \\text{for } i = 1 \\\\\n(0, 1), (1, 0), (0, -1), (-1, 0) & \\text{for } i = 2, 4, 6, 8 \\\\\n(1, 1), (1, -1), (-1, -1), (-11, 1) & \\text{for } i = 3, 5, 7, 9\n\\end{cases}\n\\]\nThese velocities correspond to the rest particle (i = 1), the particles moving to the nearest neighbors (i even), and the particles moving to the next-nearest neighbors (i odd).\nIn the LBM, each velocity vector is associated with a weight that reflects the probability of a particle moving along that vector. The weights must satisfy certain constraints to ensure mass and momentum conservation. The weights can be derived by requiring that the moments of the equilibrium distribution function and the discretized distribution functions agree.\nFor the D2Q9 scheme, these weights are\n\\[\nw_i =\n\\begin{cases}\n4/9 & \\text{for } i = 1 \\\\\n1/9 & \\text{for } i = 2, 4, 6, 8 \\\\\n1/36 & \\text{for } i = 3, 5, 7, 9\n\\end{cases}\n\\]\nIn LBM, the equilibrium distribution function \\(f_i^{eq}\\) is often approximated by a second-order Taylor expansion of the Maxwell-Boltzmann distribution\n\\[\nf_i^{\\text{eq}} = w_i \\rho \\left( 1 + \\frac{\\mathbf{e}_i \\cdot \\mathbf{u}}{c_s^2} + \\frac{(\\mathbf{e}_i \\cdot \\mathbf{u})^2}{2c_s^4} - \\frac{\\mathbf{u} \\cdot \\mathbf{u}}{2c_s^2} \\right)\n\\]\nwhere \\(w_i\\) are the weights and \\(c_s = \\sqrt{RT} = \\frac{1}{\\sqrt{3}} \\frac{\\Delta x}{\\Delta t}\\) is the speed of sound in the lattice. Our choice of velocities implies \\(\\frac{\\Delta x}{\\Delta t} = 1\\) and, therefore, \\(c_s = 1/3\\). The equilibrium particle distribution function can thus be written as\n\\[\nf_i^{\\text{eq}} = w_i \\rho \\left( 1 + 3 \\mathbf{e}_i \\cdot \\mathbf{u} + \\frac{9}{2} (\\mathbf{e}_i \\cdot \\mathbf{u})^2 - \\frac{3}{2}\\mathbf{u} \\cdot \\mathbf{u} \\right)\n\\]"
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html#no-slip-boundary-condition-bounce-back",
    "href": "posts/LatticeBoltzmannMethod/index.html#no-slip-boundary-condition-bounce-back",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "No-slip boundary condition (bounce back)",
    "text": "No-slip boundary condition (bounce back)\nAt the boundary surfaces, we impose a no-slip condition, i.e. the velocity at the boundary should be \\(0\\). In the LBM this can be achieved by setting the particle distribution function within the boundary to 0. In the streaming step at time \\(t\\), those discrete velocities that have a component towards the boundary are streamed into the boundary. Now bounce-back is applied within the boundary cells, i.e. all discrete velocity components are flipped. At time \\(t + \\Delta t\\), the flipped discrete velocities are streamed back out of the boundary. This leads to all discrete velocities that have a component towards to boundary to be approximately 0."
  },
  {
    "objectID": "posts/LatticeBoltzmannMethod/index.html#julia-implementation",
    "href": "posts/LatticeBoltzmannMethod/index.html#julia-implementation",
    "title": "Lattice Boltzmann Method (LBM)",
    "section": "Julia implementation",
    "text": "Julia implementation\n\nusing CairoMakie\nset_theme!()\n\nconst Nx = 400\nconst Ny = 100\n\nrho0 = 100\ntau = 0.6\n\n\n## D2Q9 lattice\nconst Nl = 9\n\ncx = Int64[0, 0, 1, 1, 1, 0,-1,-1,-1]\ncy = Int64[0, 1, 1, 0,-1,-1,-1, 0, 1]\nw = Float32[4/9,1/9,1/36,1/9,1/36,1/9,1/36,1/9,1/36]\n\nflipdir_idx = [1, 6, 7, 8, 9, 2, 3, 4, 5]\n\ncx3d = reshape(cx, 1, 1, :)\ncy3d = reshape(cy, 1, 1, :)\n\n## cylinder geometry\nX = (1:Nx) * ones(Ny)'\nY = ones(Nx) * (1:Ny)'\ncylinder = (X .- Nx/4).^2 .+ (Y .- Ny/2).^2 .&lt; (Ny/6)^2\n    \n## initialise distributions\nF = ones(Float32, (Nx, Ny, Nl))  # discretized \nF .+= 0.01 * rand(Nx, Ny, Nl)    # add some noise\nF[:,:,4] .+= 1.3\n\nrho = sum(F; dims = 3)\nF .= F ./ rho * rho0\n\nF[cylinder, :] .= 0.0\n\nFcache = zeros(Float32, Nx, Ny, 1)\n;\n\nF contains the distribution function for the discretized velocities\nF[ix, iy, l] corresponds to cell (ix, iy) and velocity (cx[l], cy[l]).\nWhen streaming, the velocity F[ix, iy, l] is moved to F[ix + cx[l], iy + cy[l], l].\nLet’s draw the initial velocities around the cylinder\n\n## macroscopic variables\nrho = sum(F, dims = 3)\nux = sum(F .* cx3d, dims = 3) ./ rho\nuy = sum(F .* cy3d, dims = 3) ./ rho\n\nux[cylinder, 1] .= NaN\nuy[cylinder, 1] .= NaN\n\n\nfig = Figure(size = (800, 200))\nAxis(fig[1, 1])\narrows!(1:10:Nx, 1:10:Ny, \n        ux[1:10:Nx,1:10:Ny,1], \n        uy[1:10:Nx,1:10:Ny,1], arrowsize = 7, lengthscale = 50)\nfig\n\n\n\n\n\n\n\n\n\n\"\"\"\nstream distributions to neighbouring cells\n\"\"\"\nfunction stream!(F, cx, cy, Fcache)\n    Nx, Ny, Nl = size(F)\n    for l in 1:Nl\n        for ix in 1:Nx\n            nix = (ix - 1 + cx[l] + Nx) % Nx + 1\n            for iy in 1:Ny\n                niy = (iy - 1 + cy[l] + Ny) % Ny + 1\n                Fcache[nix, niy, 1] = F[ix, iy, l]\n            end\n        end\n        F[:,:,l] = Fcache\n    end \n    return nothing\nend\n\n\"\"\"\ncollide step within each cell to relax towords equilibrium distribution\n\"\"\"\nfunction collide!(F, cx, cy, Fcache)\n    Nx, Ny, Nl = size(F)\n    cx3d = reshape(cx, 1, 1, :)\n    cy3d = reshape(cy, 1, 1, :)\n\n    ## macroscopic variables\n    rho = sum(F, dims = 3)\n    ux = sum(F .* cx3d, dims = 3) ./ rho\n    uy = sum(F .* cy3d, dims = 3) ./ rho\n\n\n    ## F_eq\n    for l in 1:Nl\n        Fcache[:,:,1] = rho .* w[l] .* ( 1 .+ 3 .* (cx[l] .* ux .+ cy[l] .* uy)  .+ 9 .* (cx[l] .* ux .+ cy[l] .* uy).^2 ./ 2 .- 3*(ux.^2 .+ uy.^2) ./ 2 )\n\n        F[:,:,l] .+= 1/tau .* (Fcache .- F[:,:,l])\n    end\n    \n    return nothing\nend\n\ncollide!\n\n\n\nfunction simulate!(F, cx, cy, cylinder, Fcache, Niter)\n\n    for it in 1:Niter\n\n        # absorbing boundaries\n        # replace cx = -1\n        F[end, :, [7, 8, 9]] .= F[end-1, :, [7,8,9]]\n        # replace cx = +1\n        F[1, :, [3, 4, 5]] .= F[2, :, [3,4,5]]\n\n        stream!(F, cx, cy, Fcache)\n\n        ## no-slip boundary conditions (bounce-back)\n        ## flip directions within boundaries\n        boundaryF = F[cylinder, flipdir_idx]\n\n        collide!(F, cx, cy, Fcache)\n\n        F[cylinder,:] = boundaryF\n\n    end\n\n    return nothing\nend\n\nsimulate! (generic function with 1 method)\n\n\n\nsimulate!(F, cx, cy, cylinder, Fcache, 250)\n\n## macroscopic variables\nrho = sum(F, dims = 3)\nux = sum(F .* cx3d, dims = 3) ./ rho\nuy = sum(F .* cy3d, dims = 3) ./ rho\n\nux[cylinder, 1] .= 0\nuy[cylinder, 1] .= 0\n\nu = sqrt.(ux.^2 .+ uy .^ 2)[:,:,1]\n\nfig = Figure(;size = (800, 200))\na, p = heatmap(fig[1,1], u)\nColorbar(fig[1,2], p)\nfig\n\n\n\n\n\n\n\n\nCreate animation\n\nf = Figure(;size = (800, 200))\nax1 = Axis(f[1,1])\n\nrecord(f, \"latticeboltzmann.gif\", 1:100, framerate = 15) do i\n\n    simulate!(F, cx, cy, cylinder, Fcache, 100)\n\n    ## compute macroscopic variables\n    rho = sum(F, dims = 3)\n    ux = sum(F .* cx3d, dims = 3) ./ rho\n    uy = sum(F .* cy3d, dims = 3) ./ rho\n\n    ux[cylinder, 1] .= 0\n    uy[cylinder, 1] .= 0\n\n    u = sqrt.(ux.^2 .+ uy .^ 2)[:,:,1]\n    empty!(ax1)\n    heatmap!(ax1, u)\nend\n\n\"latticeboltzmann.gif\"\n\n\n\n\n\nLBM 2d flow around cylinder\n\n\n\nReferences\n\nhttps://github.com/pmocz/latticeboltzmann-python/\nhttps://github.com/Ceyron/machine-learning-and-simulation/blob/main/english/simulation_scripts/lattice_boltzmann_method_python_jax.py\nhttps://filelist.tudelft.nl/TNW/Afdelingen/Radiation%20Science%20and%20Technology/Research%20Groups/RPNM/Publications/BSc_Suzanne_Wetstein.pdf\nMora et al.(2019), A concise python implementation of the lattice Boltzmann method on HPC for geo-fluid flow URL"
  },
  {
    "objectID": "posts/Pendulum/index.html",
    "href": "posts/Pendulum/index.html",
    "title": "Pendulum",
    "section": "",
    "text": "A pendulum can be modelled as a mass \\(m\\) on a weightless rod of length \\(l\\). The forces acting on the mass are gravity and the tension in the rod.\nWe can use Newton’s second law to derive equations of motion.\nThe component of gravity along the direction of the string is compensated by the tension in the string. Only the force perpendicular to the string is used to accelerate the mass. Here we consider arclength \\(s = l \\Theta\\)\n\\[\nm \\frac{\\mathrm{d}^2s}{\\mathrm{d}t^2} = - m g \\sin(\\Theta)\n\\]\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\sin(\\Theta)\n\\]\nThis is a non-linear ordinary differential equation (ODE) of second order."
  },
  {
    "objectID": "posts/Pendulum/index.html#equations-of-motion",
    "href": "posts/Pendulum/index.html#equations-of-motion",
    "title": "Pendulum",
    "section": "",
    "text": "A pendulum can be modelled as a mass \\(m\\) on a weightless rod of length \\(l\\). The forces acting on the mass are gravity and the tension in the rod.\nWe can use Newton’s second law to derive equations of motion.\nThe component of gravity along the direction of the string is compensated by the tension in the string. Only the force perpendicular to the string is used to accelerate the mass. Here we consider arclength \\(s = l \\Theta\\)\n\\[\nm \\frac{\\mathrm{d}^2s}{\\mathrm{d}t^2} = - m g \\sin(\\Theta)\n\\]\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\sin(\\Theta)\n\\]\nThis is a non-linear ordinary differential equation (ODE) of second order."
  },
  {
    "objectID": "posts/Pendulum/index.html#analytic-solution-of-linearized-ode",
    "href": "posts/Pendulum/index.html#analytic-solution-of-linearized-ode",
    "title": "Pendulum",
    "section": "Analytic solution of linearized ODE",
    "text": "Analytic solution of linearized ODE\nTo solve that equation analytically, it can be linearized using the Taylor expansion\n\\[\n\\sin(\\Theta) = \\Theta - \\frac{\\Theta^3}{3!} + \\frac{\\Theta^5}{5!} + \\ldots\n\\]\n\nusing CairoMakie\nusing LaTeXStrings\n\nset_theme!()\n\nΘ = range(0, π/2; length = 100)\napprox(Θ) = Θ - Θ^3/6 + Θ^5/120\n\nf = Figure()\na, p = lines(f[1,1], Θ, sin.(Θ); \n            linewidth = 7,\n            color = (:blue, 0.2), \n            label = L\"\\sin(Θ)\", \n            axis = (;title = \"Taylor approximation of sin(Θ)\", xlabel = L\"Θ\"))\nlines!(f[1,1], Θ, Θ; label = L\"Θ\")\nlines!(f[1,1], Θ, Θ - Θ .^ 3/6, label = L\"Θ - \\frac{Θ^3}{3!}\")\nlines!(f[1,1], Θ, Θ - Θ .^ 3 / 6 + Θ .^ 5 / 120, label = L\"Θ - \\frac{Θ^3}{3!} + \\frac{Θ^5}{5!}\")\n\naxislegend(a; position = :lt)\n\nf\n\n\n\n\n\n\n\n\nFor small angles (\\(\\Theta \\le 30°\\)) the first order Taylor approximation is reasonable. For larger angles, we overestimate the restoring force and, therefore, the frequency.\nThe resulting linear second-order ODE\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\Theta\n\\]\ncan be solved analytically with solution\n\\[\n\\Theta(t) = A \\cos(\\omega t) + B \\cos(\\omega t)\n\\]\nwith\n\\[\n\\omega = \\sqrt{\\frac{g}{l}}\n\\]\nThe solution to the linear ODE with \\(\\Theta_0 = \\pi/6 = 30°\\) is plotted below\n\ng = 9.81\nl = 1\nω = sqrt(g/l)\nt = range(0, 4*pi/ω, length = 200)\nΘ₀ = π/6\nanalytical = Θ₀ * cos.(ω*t)\n\nf2 = Figure()\n\nlines(f2[1,1], t, analytical * 180/pi;\n      axis = (;xlabel = \"time (s)\", ylabel = L\"$\\theta$ (deg)\"))\n\nf2"
  },
  {
    "objectID": "posts/Pendulum/index.html#numerical-solution-of-non-linear-ode",
    "href": "posts/Pendulum/index.html#numerical-solution-of-non-linear-ode",
    "title": "Pendulum",
    "section": "Numerical solution of non-linear ODE",
    "text": "Numerical solution of non-linear ODE\nThe nonlinear ODE can be solved using DifferentialEquations.jl. First we need to tranform the 2-order ODE into a 1-order ODE.\nThe state \\(u(t)\\) is given as \\[\nu(t) = \\begin{pmatrix}\n    \\Theta(t) \\\\\n    \\dot{\\Theta}(t)\n    \\end{pmatrix}\n\\]\n\\[\n\\dot{u}(t) = \\begin{pmatrix}\n    \\dot{\\Theta}(t) \\\\\n    - \\frac{g}{l} \\sin(\\Theta(t))\n\\end{pmatrix}\n\\]\nNow we need to define a function that provides \\(\\dot{u}(t)\\) given \\(u(t)\\), \\(t\\), and possible parameters.\n\nfunction pendulum(u, params, t)\n    du = zeros(length(u))\n    l = params[1]\n    du[1] = u[2]\n    du[2] = -g/l*sin(u[1])\n    return du\nend;\n\n\nusing DifferentialEquations\n\ntspan = (0, 4π/ω)\nparams = [l]\ninit = [π/6, 0]\nprob = ODEProblem(pendulum, init, tspan, params);\n\nu = solve(prob) returns the solution to the ODEProblem. Note that the time stepping is done in an adaptive manner, but u(t) interpolates the system state for any time \\(t \\in\\) tspan.\n\nu = solve(prob)\n\nlines!(f2[1,1], t, 180/π*[u(ti)[1] for ti in t])\n\nf2\n\n\n\n\n\n\n\n\nAs already stated above, the small-angle approximation overestimates the frequency and underestimates the period."
  },
  {
    "objectID": "posts/Pendulum/index.html#symbolic-solution-using-lagrangian",
    "href": "posts/Pendulum/index.html#symbolic-solution-using-lagrangian",
    "title": "Pendulum",
    "section": "Symbolic solution using Lagrangian",
    "text": "Symbolic solution using Lagrangian\nAbove, we had derived the equations of motion by hand. This is not always feasible. Julia has symbolic computation capabilities through the package Symbolics.jl. This allows us to use Julia as a computer algebra system (CAS) to derive the equations of motion from the Lagrangian, which is easy to specify.\nFirst we define the Lagrangian symbolically\n\nusing Symbolics\n\n@variables t m g l Θ(t)\n\n# define Theta, x, and y as above\nx = l*sin(Θ)\ny = -l*cos(Θ)\n\nDt = Differential(t)\n\n# define kinetic and potential energies\nT = 1/2*m*(Dt(x)^2 + Dt(y)^2)\nV = m*g*y\n\n# and the Lagrangian\nL = T - V;\n\nNote that Julia has not performed any differentiation yet. We need to call expand_derivatives to do so.\n\nexpand_derivatives(L)\n\n\\[ \\begin{equation}\ng l m \\cos\\left( \\Theta\\left( t \\right) \\right) + 0.5 \\left( \\sin^{2}\\left( \\Theta\\left( t \\right) \\right) \\left( \\frac{\\mathrm{d} \\Theta\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} l^{2} + \\cos^{2}\\left( \\Theta\\left( t \\right) \\right) \\left( \\frac{\\mathrm{d} \\Theta\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} l^{2} \\right) m\n\\end{equation}\n\\]\n\n\nTo get the equations of motion from the Lagrangian, we need to compute\n\\[\n\\frac{\\partial L}{\\partial \\Theta} - \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\partial L}{\\partial \\dot{\\Theta}}\n\\]\nand solve for \\(\\ddot{\\Theta}\\). This can be done using symbolic_linear_solve.\n\nΘ̇ = Dt(Θ)\nΘ̈ = Dt(Θ̇)\nDΘ = Differential(Θ)\nDΘ̇ = Differential(Θ̇)\n\nLE = simplify(expand_derivatives(DΘ(L) -  Dt(DΘ̇(L))))\n\nu̇ = [Θ̇; simplify(symbolic_linear_solve(LE, Θ̈))]\n\n# define a function du(u, params, t) to be used in ODEProblem\ndu, du! = build_function(u̇, [Θ, Θ̇],  [l, g], t; expression = Val{false});\n\nThese functions can now be used to solve the DE numerically using DifferentialEquations.jl as above.\n\ntspan = (0, 4π/ω)\nparams = [1.0, 9.81]\ninit = [π/6, 0]\nprob = ODEProblem(du, init, tspan, params);\n\n\nsol = solve(prob)\n\nt = range(0, 4*pi/ω, length = 200)\nlines!(f2[1,1], t, 180/π*[sol(ti)[1] for ti in t])\n\nf2"
  },
  {
    "objectID": "posts/Pendulum/index.html#animation",
    "href": "posts/Pendulum/index.html#animation",
    "title": "Pendulum",
    "section": "Animation",
    "text": "Animation\nWe can use the record function in CairoMakie.jl to create an animation.\n\n\nu1 = [u(ti)[1] for ti in t]\n\nf = Figure()\n\nax1 = Axis(f[1,1], xlabel = L\"t\", ylabel = L\"Θ\")\nxlims!(ax1, tspan...)\nylims!(ax1, -1,1)\n\nax2 = Axis(f[1,2]; aspect = 1)\nhidedecorations!(ax2)\nhidespines!(ax2)\nxlims!(ax2, -12,12)\nylims!(ax2, -12,12)\n\nr = 10.0\n\nrecord(f, \"incremental_plot_animation.gif\", 1:length(t), framerate = 30) do i\n\n    empty!(ax1)\n    lines!(ax1, t[1:i], u1[1:i]; color = :blue)\n    lines!(ax1, t[1:i], analytical[1:i]; color = :red)\n\n    empty!(ax2)\n    y = -r*cos(u1[i])\n    x = r*sin(u1[i])\n    lines!(ax2, [0,x], [0,y]; color = :blue)\n    scatter!(ax2, x, y; markersize = 20, color = :blue)\n\n    y = -r*cos(analytical[i])\n    x = r*sin(analytical[i])\n    lines!(ax2, [0,x], [0,y]; color = :red)\n    scatter!(ax2, x, y; markersize = 20, color = :red)\nend;\n\n\n\n\nAnimation"
  },
  {
    "objectID": "posts/Pendulum/index.html#references",
    "href": "posts/Pendulum/index.html#references",
    "title": "Pendulum",
    "section": "References",
    "text": "References\n\nhttps://cooperrc.github.io/Julia-learning/day_05.html"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html",
    "href": "posts/TimeDependentSchroedingerEquation/index.html",
    "title": "Time-dependent Schrödinger equation",
    "section": "",
    "text": "The one-dimensional time-dependent Schrödinger equation (TDSE) for a potential \\(V(x)\\) is given by\n\\[\ni\\hbar \\frac{\\partial \\psi(x,t)}{\\partial t} = -\\frac{\\hbar^2}{2m} \\frac{\\partial^2 \\psi(x,t)}{\\partial x^2} + V(x) \\psi(x,t)\n\\]\nTo specify a problem, we need to specify"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html#solve-using-finite-difference-method",
    "href": "posts/TimeDependentSchroedingerEquation/index.html#solve-using-finite-difference-method",
    "title": "Time-dependent Schrödinger equation",
    "section": "Solve using finite difference method",
    "text": "Solve using finite difference method\nUsing finite differences, we can discretize the TDSE as\n\\[\n\\psi_i^{n+1} = \\psi_i^n +\n   \\frac{i \\hbar \\Delta t}{2m (\\Delta x)^2} \\left( \\psi_{i+1}^n - 2\\psi_i^n + \\psi_{i-1}^n \\right) -\n   \\frac{i \\Delta t}{\\hbar} V(x_i) \\psi_i^n\n\\]\nAssuming a potential of the form\n\\[\nV(x) = - 10^4 \\exp\\left(-\\frac{(x - L/2)^2}{2(L/20)^2} \\right)\n\\]\n\nusing CairoMakie\nusing LaTeXStrings\n\nset_theme!()\n\nnx = 301\nnt = 100001\nL = 1.0\nT = 0.01\ndx = L / (nx - 1)\ndt = T / (nt - 1)\nx = range(0.0, L; length = nx)\n\nV = @. -10000 * exp(-(x - L/2.)^2 / 2 / (L/20.)^2)\n\nlines(x, V; axis = (;ylabel = \"V(x)\", xlabel = \"x/L\"))\n\n\n\n\n\n\n\n\nAs initial condition we use\n\\[\n\\psi_0 = \\sqrt{2} \\sin(\\pi x)\n\\]\n\nΨ₀ = sqrt(2.) * sin.(π*x)\n\nlines(x, Ψ₀; axis = (;ylabel = L\"\\psi_0(x)\", xlabel = L\"x/L\"))\n\n\n\n\n\n\n\n\nwhich is normalized to 1\n\\[\n\\int_0^1 |\\psi_0(x)|^2 \\mathrm{d}x = 1\n\\]\nas can be confirmed numerically\n\nsum(Ψ₀.^2 * dx)\n\n1.0000000000000002\n\n\n\nΨ = zeros(ComplexF64, nt, nx)\nΨ[1,:] .= Ψ₀\n\nfunction evolve!(psi)\n    for t in 1:(nt-1)\n        for i in 2:(nx-1)\n            psi[t+1, i] = psi[t, i] + \n                im/2 * dt/dx^2 * (psi[t, i+1] - 2*psi[t, i] + psi[t, i-1]) - \n                im*dt*V[i]*psi[t, i]\n        end\n        \n        normal = sum(abs.(psi[t+1,:]).^2)*dx\n        for i in 1:(nx-1)\n            psi[t+1,i] = psi[t+1,i]/normal\n        end\n    end\nend\n\nevolve!(Ψ)\n\n\nf,a,p = lines(x, abs2.(Ψ[1,:]); \n            label = L\"t = 0\", \n            axis = (;xlabel = L\"x\",\n                     ylabel = L\"\\Psi(5000, x)\"))\n\nlines!(a, x, abs2.(Ψ[5000,:]); label = L\"t = 5000\")\naxislegend(a)\nf"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html#solve-using-eigenstate-evolution",
    "href": "posts/TimeDependentSchroedingerEquation/index.html#solve-using-eigenstate-evolution",
    "title": "Time-dependent Schrödinger equation",
    "section": "Solve using eigenstate evolution",
    "text": "Solve using eigenstate evolution\nThe TDSE can also be solved using the eigenstate evolution. Here we first solve the time-independent Schrödinger equation (TISE),\n\\[\n-\\frac{\\hbar^2}{2m} \\frac{\\partial^2 \\psi(x)}{\\partial x^2} + V(x) \\psi(x) = E \\psi(x)\n\\]\nto obtain the eigenstates \\(\\psi_j(x)\\) and their energy levels \\(E_j\\).\nThen we can express the time-dependent solutions of the TDSE as\n\\[\n\\psi(x, t) = \\sum_{j=0}^\\infty a_j \\psi_j(x) \\exp(-i E_j t)\n\\]\nwhere the \\(a_j\\) are obtained from the initial condition \\(\\psi_0(x)\\) through\n\\[\na_j = \\int_{-\\infty}^{\\infty} \\psi_0(x) \\psi_j^*(x) \\mathrm{d}x\n\\]\nTo solve the TISE numerically, we can discretize the space and convert the differential equation into a matrix equation.\n\nDivide the spatial domain into \\(N\\) points with spacing \\(\\Delta x\\). Let \\(x_i = x_0 + i\\Delta x\\) for \\(i = 0, 1, 2, \\ldots, N-1\\).\nApproximate the second derivative using the central difference method: \\[\n\\frac{d^2 \\psi}{dx^2} \\bigg|_{x=x_i} \\approx \\frac{\\psi(x_{i+1}) - 2\\psi(x_i) + \\psi(x_{i-1})}{(\\Delta x)^2}\n\\]\n\nWe substitute this finite difference approximation into the Schrödinger equation\n\\[\n-\\frac{\\hbar^2}{2m} \\frac{\\psi(x_{i+1}) - 2\\psi(x_i) + \\psi(x_{i-1})}{(\\Delta x)^2} + V(x_i) \\psi(x_i) = E \\psi(x_i)\n\\]\nand rearrange terms\n\\[\n-\\frac{\\hbar^2}{2m (\\Delta x)^2} \\psi(x_{i+1}) + \\left( \\frac{\\hbar^2}{m (\\Delta x)^2} + V(x_i) \\right) \\psi(x_i) - \\frac{\\hbar^2}{2m (\\Delta x)^2} \\psi(x_{i-1}) = E \\psi(x_i)\n\\]\nThis can be written as a matrix equation \\(H \\psi = E \\psi\\), where \\(H\\) is a tridiagonal matrix with the following elements:\n\nThe diagonal elements \\[\nH_{ii} = \\frac{\\hbar^2}{m (\\Delta x)^2} + V(x_i)\n\\]\nThe off-diagonal elements \\[\nH_{i, i+1} = H_{i+1, i} = -\\frac{\\hbar^2}{2m (\\Delta x)^2}\n\\]\n\nFor an \\(N\\)-point discretization, the Hamiltonian matrix \\(H\\) in tridiagonal form looks like this:\n\\[\nH = \\begin{bmatrix}\na_1 & b & 0 & 0 & \\cdots & 0 & 0 \\\\\nb & a_2 & b & 0 & \\cdots & 0 & 0 \\\\\n0 & b & a_3 & b & \\cdots & 0 & 0 \\\\\n0 & 0 & b & a_4 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & a_{N-2} & b \\\\\n0 & 0 & 0 & 0 & \\cdots & b & a_{N-1}\n\\end{bmatrix}\n\\]\nwhere: - \\(a_i = \\frac{\\hbar^2}{m (\\Delta x)^2} + V(x_i)\\) (diagonal elements), - \\(b = -\\frac{\\hbar^2}{2m (\\Delta x)^2}\\) (off-diagonal elements).\n\nusing LinearAlgebra\n\na = 1/dx^2 .+ V[2:(end-1)]\nb = -1/(2*dx^2) .* ones(length(a)-1)\n\nF = eigen(SymTridiagonal(a, b));\n\nThe k-th eigenvalue can be accessed using F.values[k], the k-th eigenvector as F.vectors[:,k].\n\nE_j = F.values[1:70]\nψⱼ = vcat(zeros(70)', F.vectors[:,1:70], zeros(70)')\n\ncⱼ = ψⱼ' * Ψ₀\n\nlines(x, abs2.(ψⱼ * (cⱼ .* exp.(-im*E_j*5000*dt))  ))"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html#animation",
    "href": "posts/TimeDependentSchroedingerEquation/index.html#animation",
    "title": "Time-dependent Schrödinger equation",
    "section": "Animation",
    "text": "Animation\n\nt = Observable(0)\ny = @lift abs2.(ψⱼ * (cⱼ .* exp.(-im*E_j*$t*dt)))\n\nf = Figure()\nax = Axis(f[1,1], xlabel = L\"x/L\", ylabel = L\"|\\psi(x)|^2\",\n           title = @lift \"t = $($t) dt\")\nlines!(ax, x, y;\n          color = :blue)\nxlims!(ax, 0.0, 1.0)\nylims!(ax, 0, 20)\n\n\nrecord(f, \"schroedinger_animation.gif\", 0:100:30000, framerate = 30) do i \n  t[] = i  \nend\n\n\"schroedinger_animation.gif\"\n\n\n\n\n\nSchroedinger animation"
  },
  {
    "objectID": "posts/TimeDependentSchroedingerEquation/index.html#references",
    "href": "posts/TimeDependentSchroedingerEquation/index.html#references",
    "title": "Time-dependent Schrödinger equation",
    "section": "References",
    "text": "References\nhttps://github.com/lukepolson/youtube_channel/blob/main/Python%20Metaphysics%20Series/vid17.ipynb"
  },
  {
    "objectID": "posts/TuringIntro/index.html",
    "href": "posts/TuringIntro/index.html",
    "title": "Introduction to Turing.jl",
    "section": "",
    "text": "Turing.jl is a probabilistic proramming language that let’s us define a generative model and does inference."
  },
  {
    "objectID": "posts/TuringIntro/index.html#coin-example",
    "href": "posts/TuringIntro/index.html#coin-example",
    "title": "Introduction to Turing.jl",
    "section": "1 Coin example",
    "text": "1 Coin example\nLet’s flip a biased coin a hundred times.\n\nusing Turing\nusing DataFrames\ncoin_flips = rand(Bernoulli(0.7), 100);\n\nIn order to do inference, we need to specify a model.\n\\[\np \\sim Beta(1,1)\n\\]\n\\[\ncoin flip \\sim Bernoulli(p)\n\\]\n\n@model function coin(data)\n  p ~ Beta(1,1)\n  for i in eachindex(data) \n    data[i] ~ Bernoulli(p)\n  end\nend\n\ncoin (generic function with 2 methods)\n\n\n\n1.1 Sample from prior\n\nprior = sample(coin(coin_flips), Prior(), 1000)\n\nSampling:   8%|███▌                                     |  ETA: 0:00:01Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00\n\n\n\nChains MCMC chain (1000×2×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 1.22 seconds\nCompute duration  = 1.22 seconds\nparameters        = p\ninternals         = lp\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.4905    0.2940    0.0094   974.7327   939.1381    1.0022     ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           p    0.0248    0.2182    0.4755    0.7408    0.9729\n\n\n\n\n\nsummarystats(prior)\n\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.4905    0.2940    0.0094   974.7327   939.1381    1.0022     ⋯\n                                                                1 column omitted\n\n\n\n\n\nDataFrame(prior) \n\n\n1000×4 DataFrame975 rows omitted\n\n\n\nRow\niteration\nchain\np\nlp\n\n\n\nInt64\nInt64\nFloat64\nFloat64\n\n\n\n\n1\n1\n1\n0.70156\n0.0\n\n\n2\n2\n1\n0.199081\n0.0\n\n\n3\n3\n1\n0.453383\n0.0\n\n\n4\n4\n1\n0.919818\n0.0\n\n\n5\n5\n1\n0.988394\n0.0\n\n\n6\n6\n1\n0.273046\n0.0\n\n\n7\n7\n1\n0.148202\n0.0\n\n\n8\n8\n1\n0.391817\n0.0\n\n\n9\n9\n1\n0.970247\n0.0\n\n\n10\n10\n1\n0.706999\n0.0\n\n\n11\n11\n1\n0.824522\n0.0\n\n\n12\n12\n1\n0.813191\n0.0\n\n\n13\n13\n1\n0.946668\n0.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n989\n989\n1\n0.37487\n0.0\n\n\n990\n990\n1\n0.469177\n0.0\n\n\n991\n991\n1\n0.71334\n0.0\n\n\n992\n992\n1\n0.0471244\n0.0\n\n\n993\n993\n1\n0.563055\n0.0\n\n\n994\n994\n1\n0.781447\n0.0\n\n\n995\n995\n1\n0.39244\n0.0\n\n\n996\n996\n1\n0.260536\n0.0\n\n\n997\n997\n1\n0.38682\n0.0\n\n\n998\n998\n1\n0.111589\n0.0\n\n\n999\n999\n1\n0.23647\n0.0\n\n\n1000\n1000\n1\n0.146541\n0.0\n\n\n\n\n\n\n\n\n\n1.2 Sample from posterior\n\nposterior = sample(coin(coin_flips), NUTS(), 1000)\n\n┌ Info: Found initial step size\n└   ϵ = 0.8\n\n\n\nChains MCMC chain (1000×13×1 Array{Float64, 3}):\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 2.47 seconds\nCompute duration  = 2.47 seconds\nparameters        = p\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.7050    0.0456    0.0020   530.9313   637.9505    1.0044     ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           p    0.6143    0.6736    0.7077    0.7378    0.7868\n\n\n\n\n\nsummarystats(posterior)\n\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           p    0.7050    0.0456    0.0020   530.9313   637.9505    1.0044     ⋯\n                                                                1 column omitted\n\n\n\n\n\nDataFrame(posterior)\n\n\n1000×15 DataFrame975 rows omitted\n\n\n\nRow\niteration\nchain\np\nlp\nn_steps\nis_accept\nacceptance_rate\nlog_density\nhamiltonian_energy\nhamiltonian_energy_error\nmax_hamiltonian_energy_error\ntree_depth\nnumerical_error\nstep_size\nnom_step_size\n\n\n\nInt64\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n501\n1\n0.667248\n-62.1415\n3.0\n1.0\n1.0\n-62.1415\n63.0626\n-0.661625\n-0.661625\n2.0\n0.0\n1.50987\n1.50987\n\n\n2\n502\n1\n0.668576\n-62.1183\n1.0\n1.0\n1.0\n-62.1183\n62.2926\n-0.0117913\n-0.0117913\n1.0\n0.0\n1.50987\n1.50987\n\n\n3\n503\n1\n0.668576\n-62.1183\n1.0\n1.0\n0.00995441\n-62.1183\n66.3039\n0.0\n4.60974\n1.0\n0.0\n1.50987\n1.50987\n\n\n4\n504\n1\n0.697756\n-61.8074\n1.0\n1.0\n1.0\n-61.8074\n61.9722\n-0.153328\n-0.153328\n1.0\n0.0\n1.50987\n1.50987\n\n\n5\n505\n1\n0.632376\n-63.0164\n3.0\n1.0\n0.673321\n-63.0164\n63.0961\n0.596828\n0.596828\n2.0\n0.0\n1.50987\n1.50987\n\n\n6\n506\n1\n0.771892\n-62.9796\n3.0\n1.0\n0.887346\n-62.9796\n63.8398\n0.11754\n0.163352\n2.0\n0.0\n1.50987\n1.50987\n\n\n7\n507\n1\n0.747049\n-62.2338\n3.0\n1.0\n0.979613\n-62.2338\n62.9818\n-0.314224\n-0.415572\n2.0\n0.0\n1.50987\n1.50987\n\n\n8\n508\n1\n0.711306\n-61.7986\n1.0\n1.0\n1.0\n-61.7986\n62.0348\n-0.199251\n-0.199251\n1.0\n0.0\n1.50987\n1.50987\n\n\n9\n509\n1\n0.701567\n-61.7959\n3.0\n1.0\n0.995036\n-61.7959\n61.8093\n-0.0014947\n0.00805905\n2.0\n0.0\n1.50987\n1.50987\n\n\n10\n510\n1\n0.618061\n-63.519\n3.0\n1.0\n0.630806\n-63.519\n63.5237\n0.867687\n0.867687\n2.0\n0.0\n1.50987\n1.50987\n\n\n11\n511\n1\n0.682247\n-61.9246\n1.0\n1.0\n1.0\n-61.9246\n62.7756\n-0.823189\n-0.823189\n1.0\n0.0\n1.50987\n1.50987\n\n\n12\n512\n1\n0.690049\n-61.8517\n1.0\n1.0\n1.0\n-61.8517\n61.9176\n-0.0359659\n-0.0359659\n1.0\n0.0\n1.50987\n1.50987\n\n\n13\n513\n1\n0.646631\n-62.5978\n3.0\n1.0\n0.759719\n-62.5978\n62.7036\n0.368599\n0.386359\n2.0\n0.0\n1.50987\n1.50987\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n989\n1489\n1\n0.686867\n-61.8781\n1.0\n1.0\n0.459699\n-61.8781\n62.669\n0.0\n0.777183\n1.0\n0.0\n1.50987\n1.50987\n\n\n990\n1490\n1\n0.727867\n-61.9138\n3.0\n1.0\n0.97269\n-61.9138\n61.991\n0.0215458\n0.0401627\n2.0\n0.0\n1.50987\n1.50987\n\n\n991\n1491\n1\n0.727867\n-61.9138\n1.0\n1.0\n0.871949\n-61.9138\n62.1296\n0.0\n0.137025\n1.0\n0.0\n1.50987\n1.50987\n\n\n992\n1492\n1\n0.710052\n-61.7956\n3.0\n1.0\n0.609523\n-61.7956\n62.9231\n-0.0857302\n0.958995\n2.0\n0.0\n1.50987\n1.50987\n\n\n993\n1493\n1\n0.710052\n-61.7956\n1.0\n1.0\n0.199751\n-61.7956\n63.3779\n0.0\n1.61068\n1.0\n0.0\n1.50987\n1.50987\n\n\n994\n1494\n1\n0.710052\n-61.7956\n1.0\n1.0\n0.00809261\n-61.7956\n66.217\n0.0\n4.8168\n1.0\n0.0\n1.50987\n1.50987\n\n\n995\n1495\n1\n0.706931\n-61.7916\n3.0\n1.0\n0.924371\n-61.7916\n61.9295\n-0.00262086\n0.123828\n2.0\n0.0\n1.50987\n1.50987\n\n\n996\n1496\n1\n0.647596\n-62.5725\n3.0\n1.0\n0.793091\n-62.5725\n62.5726\n0.383766\n0.383766\n1.0\n0.0\n1.50987\n1.50987\n\n\n997\n1497\n1\n0.605831\n-64.0124\n3.0\n1.0\n0.38031\n-64.0124\n65.1498\n0.7014\n1.35955\n2.0\n0.0\n1.50987\n1.50987\n\n\n998\n1498\n1\n0.788496\n-63.7146\n3.0\n1.0\n1.0\n-63.7146\n64.0344\n-0.181217\n-0.988639\n2.0\n0.0\n1.50987\n1.50987\n\n\n999\n1499\n1\n0.741977\n-62.1287\n1.0\n1.0\n1.0\n-62.1287\n63.1338\n-0.659711\n-0.659711\n1.0\n0.0\n1.50987\n1.50987\n\n\n1000\n1500\n1\n0.71439\n-61.8093\n1.0\n1.0\n1.0\n-61.8093\n61.9865\n-0.145926\n-0.145926\n1.0\n0.0\n1.50987\n1.50987\n\n\n\n\n\n\n\n\n\n1.3 Prior predictive check\nHere we pass in a vector containing missing. Sampling then generates observations.\n\nobservations = Vector{Missing}(missing, length(coin_flips))\nprior_check = predict(coin(observations), prior)\n\n\nChains MCMC chain (1000×100×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = data[1], data[2], data[3], data[4], data[5], data[6], data[7], data[8], data[9], data[10], data[11], data[12], data[13], data[14], data[15], data[16], data[17], data[18], data[19], data[20], data[21], data[22], data[23], data[24], data[25], data[26], data[27], data[28], data[29], data[30], data[31], data[32], data[33], data[34], data[35], data[36], data[37], data[38], data[39], data[40], data[41], data[42], data[43], data[44], data[45], data[46], data[47], data[48], data[49], data[50], data[51], data[52], data[53], data[54], data[55], data[56], data[57], data[58], data[59], data[60], data[61], data[62], data[63], data[64], data[65], data[66], data[67], data[68], data[69], data[70], data[71], data[72], data[73], data[74], data[75], data[76], data[77], data[78], data[79], data[80], data[81], data[82], data[83], data[84], data[85], data[86], data[87], data[88], data[89], data[90], data[91], data[92], data[93], data[94], data[95], data[96], data[97], data[98], data[99], data[100]\ninternals         = \nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk   ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64     Float64    Float64   Float64    ⋯\n     data[1]    0.4820    0.4999    0.0157   1014.0717        NaN    0.9994    ⋯\n     data[2]    0.4800    0.4998    0.0158    996.0318        NaN    1.0026    ⋯\n     data[3]    0.5080    0.5002    0.0161    963.6780        NaN    1.0000    ⋯\n     data[4]    0.4790    0.4998    0.0161    965.2584        NaN    1.0008    ⋯\n     data[5]    0.4870    0.5001    0.0148   1141.3400        NaN    0.9991    ⋯\n     data[6]    0.5000    0.5003    0.0158   1004.4501        NaN       NaN    ⋯\n     data[7]    0.4780    0.4998    0.0163    937.4507        NaN    1.0003    ⋯\n     data[8]    0.4910    0.5002    0.0170    866.6538        NaN    1.0015    ⋯\n     data[9]    0.5110    0.5001    0.0163    942.5012        NaN    0.9999    ⋯\n    data[10]    0.4810    0.4999    0.0163    937.8369        NaN    0.9992    ⋯\n    data[11]    0.5010    0.5002    0.0160    982.6567        NaN    1.0057    ⋯\n    data[12]    0.4910    0.5002    0.0165    915.1037        NaN    1.0064    ⋯\n    data[13]    0.4940    0.5002    0.0163    945.1988        NaN    1.0021    ⋯\n    data[14]    0.4860    0.5001    0.0154   1060.5513        NaN    0.9993    ⋯\n    data[15]    0.4910    0.5002    0.0171    853.5002        NaN    1.0008    ⋯\n    data[16]    0.5080    0.5002    0.0167    896.4202        NaN    1.0031    ⋯\n    data[17]    0.4900    0.5002    0.0157   1016.2156        NaN    1.0000    ⋯\n    data[18]    0.5020    0.5002    0.0162    956.4828        NaN    1.0006    ⋯\n    data[19]    0.4680    0.4992    0.0153   1058.3959        NaN    1.0006    ⋯\n    data[20]    0.5130    0.5001    0.0149   1127.9403        NaN    0.9990    ⋯\n    data[21]    0.5090    0.5002    0.0154   1053.1414        NaN    0.9992    ⋯\n    data[22]    0.5240    0.4997    0.0165    915.7678        NaN    0.9990    ⋯\n    data[23]    0.5030    0.5002    0.0153   1069.7798        NaN    1.0004    ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮          ⋮          ⋮       ⋱\n                                                    1 column and 77 rows omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n     data[1]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[2]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[3]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[4]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[5]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[6]    0.0000    0.0000    0.5000    1.0000    1.0000\n     data[7]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[8]    0.0000    0.0000    0.0000    1.0000    1.0000\n     data[9]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[10]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[11]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[12]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[13]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[14]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[15]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[16]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[17]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[18]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[19]    0.0000    0.0000    0.0000    1.0000    1.0000\n    data[20]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[21]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[22]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[23]    0.0000    0.0000    1.0000    1.0000    1.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                 77 rows omitted\n\n\n\n\nprior_check is a Chains instance. To convert to an Array, use\n\nArray(prior_check, [:parameters])\n\n1000×100 Matrix{Float64}:\n 0.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0  …  1.0  1.0  1.0  1.0  0.0  1.0  1.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0     0.0  1.0  1.0  0.0  1.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 0.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  …  1.0  0.0  0.0  0.0  0.0  0.0  1.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  1.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  1.0  0.0  1.0  0.0  1.0  0.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  0.0  0.0  1.0  0.0     1.0  1.0  1.0  1.0  1.0  0.0  1.0\n 1.0  1.0  1.0  1.0  0.0  1.0  1.0  1.0  …  0.0  1.0  0.0  1.0  1.0  0.0  0.0\n 1.0  1.0  1.0  0.0  1.0  0.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  0.0  1.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  0.0  1.0  1.0  1.0  1.0\n ⋮                        ⋮              ⋱            ⋮                   \n 0.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0     0.0  1.0  1.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  0.0  1.0  0.0  1.0  …  0.0  1.0  1.0  1.0  1.0  0.0  1.0\n 1.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  1.0  0.0  1.0  1.0  1.0     0.0  1.0  1.0  0.0  1.0  0.0  0.0\n 1.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0     1.0  1.0  1.0  1.0  0.0  1.0  0.0\n 1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0     1.0  1.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  1.0  1.0  0.0  …  0.0  1.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  1.0  1.0  1.0  1.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n\n\n\n\n1.4 Posterior predictive check\n\nobservations = Vector{Missing}(missing, length(coin_flips))\nposterior_check = predict(coin(observations), posterior)\n\n\nChains MCMC chain (1000×100×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = data[1], data[2], data[3], data[4], data[5], data[6], data[7], data[8], data[9], data[10], data[11], data[12], data[13], data[14], data[15], data[16], data[17], data[18], data[19], data[20], data[21], data[22], data[23], data[24], data[25], data[26], data[27], data[28], data[29], data[30], data[31], data[32], data[33], data[34], data[35], data[36], data[37], data[38], data[39], data[40], data[41], data[42], data[43], data[44], data[45], data[46], data[47], data[48], data[49], data[50], data[51], data[52], data[53], data[54], data[55], data[56], data[57], data[58], data[59], data[60], data[61], data[62], data[63], data[64], data[65], data[66], data[67], data[68], data[69], data[70], data[71], data[72], data[73], data[74], data[75], data[76], data[77], data[78], data[79], data[80], data[81], data[82], data[83], data[84], data[85], data[86], data[87], data[88], data[89], data[90], data[91], data[92], data[93], data[94], data[95], data[96], data[97], data[98], data[99], data[100]\ninternals         = \nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk   ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64     Float64    Float64   Float64    ⋯\n     data[1]    0.7110    0.4535    0.0155    852.2832        NaN    1.0050    ⋯\n     data[2]    0.7250    0.4467    0.0144    967.8324        NaN    0.9991    ⋯\n     data[3]    0.7010    0.4580    0.0144   1006.9710        NaN    1.0001    ⋯\n     data[4]    0.7040    0.4567    0.0143   1018.5811        NaN    0.9991    ⋯\n     data[5]    0.6800    0.4667    0.0137   1168.7328        NaN    0.9991    ⋯\n     data[6]    0.6740    0.4690    0.0149    985.2633        NaN    0.9992    ⋯\n     data[7]    0.7070    0.4554    0.0163    777.3561        NaN    0.9998    ⋯\n     data[8]    0.7010    0.4580    0.0159    828.1807        NaN    0.9991    ⋯\n     data[9]    0.7150    0.4516    0.0147    950.3636        NaN    1.0008    ⋯\n    data[10]    0.7000    0.4585    0.0148    959.2388        NaN    0.9993    ⋯\n    data[11]    0.7050    0.4563    0.0139   1076.4233        NaN    1.0001    ⋯\n    data[12]    0.7130    0.4526    0.0159    808.8252        NaN    0.9990    ⋯\n    data[13]    0.7030    0.4572    0.0159    822.7494        NaN    1.0001    ⋯\n    data[14]    0.6900    0.4627    0.0143   1042.8909        NaN    0.9991    ⋯\n    data[15]    0.7060    0.4558    0.0155    865.6955        NaN    0.9991    ⋯\n    data[16]    0.6970    0.4598    0.0139   1094.5315        NaN    0.9996    ⋯\n    data[17]    0.7140    0.4521    0.0149    920.1157        NaN    0.9992    ⋯\n    data[18]    0.6900    0.4627    0.0143   1053.3605        NaN    1.0013    ⋯\n    data[19]    0.7130    0.4526    0.0148    934.3333        NaN    0.9991    ⋯\n    data[20]    0.6960    0.4602    0.0155    882.8872        NaN    0.9992    ⋯\n    data[21]    0.7030    0.4572    0.0170    719.7857        NaN    0.9990    ⋯\n    data[22]    0.6800    0.4667    0.0146   1019.3042        NaN    0.9997    ⋯\n    data[23]    0.6950    0.4606    0.0145   1012.6167        NaN    1.0019    ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮          ⋮          ⋮       ⋱\n                                                    1 column and 77 rows omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n     data[1]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[2]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[3]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[4]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[5]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[6]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[7]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[8]    0.0000    0.0000    1.0000    1.0000    1.0000\n     data[9]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[10]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[11]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[12]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[13]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[14]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[15]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[16]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[17]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[18]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[19]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[20]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[21]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[22]    0.0000    0.0000    1.0000    1.0000    1.0000\n    data[23]    0.0000    0.0000    1.0000    1.0000    1.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                 77 rows omitted\n\n\n\n\n\n\n1.5 Chains\nThe results of sampling runs are MCMCChains.Chains instances.\nA Chains instance has the following fields:\n\nvalue: AxisArray object of size :iter x :var x :chain\nlogevidence\nname_map (to define sections, NamedTuple section -&gt; names, default section is :parameters)\ninfo\n\n\nvalue = rand(500, 2, 3)\nchn = Chains(value, [:a, :b])\nchn2 = Chains(value, [\"A[1]\", \"A[2]\"])\n\n\nChains MCMC chain (500×2×3 Array{Float64, 3}):\nIterations        = 1:1:500\nNumber of chains  = 3\nSamples per chain = 500\nparameters        = A[1], A[2]\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n        A[1]    0.5013    0.2900    0.0078   1406.5702   1488.8305    1.0003   ⋯\n        A[2]    0.4917    0.2859    0.0074   1517.9844   1532.4318    1.0014   ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n        A[1]    0.0275    0.2505    0.5079    0.7505    0.9847\n        A[2]    0.0271    0.2498    0.4863    0.7388    0.9757\n\n\n\n\nOne can restrict the output (an AxisArray) to some parameters by using chn[:a], for example. To get a Chains instance with only parameter a, do chn[[:a]]. Weird!\nAn AxisArray has fields :axes and :data. It has advanced indexing.\nFor example, our prior_check from above has axes [:iter, :var, :chain].\nCan use `prior_check.value[var]\nI.e. to get only a slice of a dimension :dim, one can use A[dim = idx], so in the case of a MCMC sample, we might use chn.value[var = :a], or chn.value[chain = 2]\nsummarystats(chn; sections = :parameters)\nquantile(chn; sections = :parameters)\nor describe(chn; sections = :parameters) to get both at once\nTurings sample returns a Chains instance with sections :parameters and :internals.\n\nposterior.name_map\n\n(parameters = [:p], internals = [:lp, :n_steps, :is_accept, :acceptance_rate, :log_density, :hamiltonian_energy, :hamiltonian_energy_error, :max_hamiltonian_energy_error, :tree_depth, :numerical_error, :step_size, :nom_step_size])\n\n\nTo convert a chain to a DataFrame containing the :parameters section only, one can do\n\nDataFrame(posterior[posterior.name_map.parameters])\n\n# or DataFrame(MCMCchains.get_sections(posterior, :parameters))\n\n\n1000×3 DataFrame975 rows omitted\n\n\n\nRow\niteration\nchain\np\n\n\n\nInt64\nInt64\nFloat64\n\n\n\n\n1\n501\n1\n0.667248\n\n\n2\n502\n1\n0.668576\n\n\n3\n503\n1\n0.668576\n\n\n4\n504\n1\n0.697756\n\n\n5\n505\n1\n0.632376\n\n\n6\n506\n1\n0.771892\n\n\n7\n507\n1\n0.747049\n\n\n8\n508\n1\n0.711306\n\n\n9\n509\n1\n0.701567\n\n\n10\n510\n1\n0.618061\n\n\n11\n511\n1\n0.682247\n\n\n12\n512\n1\n0.690049\n\n\n13\n513\n1\n0.646631\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n989\n1489\n1\n0.686867\n\n\n990\n1490\n1\n0.727867\n\n\n991\n1491\n1\n0.727867\n\n\n992\n1492\n1\n0.710052\n\n\n993\n1493\n1\n0.710052\n\n\n994\n1494\n1\n0.710052\n\n\n995\n1495\n1\n0.706931\n\n\n996\n1496\n1\n0.647596\n\n\n997\n1497\n1\n0.605831\n\n\n998\n1498\n1\n0.788496\n\n\n999\n1499\n1\n0.741977\n\n\n1000\n1500\n1\n0.71439"
  },
  {
    "objectID": "posts/DoublePendulum/index.html",
    "href": "posts/DoublePendulum/index.html",
    "title": "Double pendulum",
    "section": "",
    "text": "The double pendulum in Julia.\nFirst, lets define the Lagrangian \\(L = T - V\\), where \\(T\\) is the kinetic energy of the system and \\(V\\) the potential energy.\nusing CairoMakie\nusing Symbolics\n\nset_theme!()\nDefine all the variables needed to specify the problem\n@variables g, t, m₁, m₂, l₁, l₂, Θ₁(t), Θ₂(t);\nThe kinetic and potential energy are easier to express in cartesian coordinates.\nx₁ = l₁ * sin(Θ₁)\ny₁ = - l₁ * cos(Θ₁)\nx₂ = x₁ + l₂ * sin(Θ₂)\ny₂ = y₁ - l₂ * cos(Θ₂);\nDefine kinetic and potential energy in cartesian coordinates\nDt = Differential(t)\n\nẋ₁ = Dt(x₁)\nẏ₁ = Dt(y₁)\nẋ₂ = Dt(x₂)\nẏ₂ = Dt(y₂)\n\nT₁ = 1/2 * m₁ * (ẋ₁^2 + ẏ₁^2)\nT₂ = 1/2 * m₂ * (ẋ₂^2 + ẏ₂^2)\nT = T₁ + T₂\n\nV₁ = m₁ * g * y₁\nV₂ = m₂ * g * y₂\nV = V₁ + V₂\n\nL = T - V;\nsimplify(expand_derivatives(L))\n\n\\[ \\begin{equation}\ng l_1 m_1 \\cos\\left( \\Theta_1\\left( t \\right) \\right) - g \\left(  - l_1 \\cos\\left( \\Theta_1\\left( t \\right) \\right) - l_2 \\cos\\left( \\Theta_2\\left( t \\right) \\right) \\right) m_2 + 0.5 \\left( \\left( \\frac{\\mathrm{d} \\Theta_1\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} \\cos^{2}\\left( \\Theta_1\\left( t \\right) \\right) l_1^{2} + \\sin^{2}\\left( \\Theta_1\\left( t \\right) \\right) \\left( \\frac{\\mathrm{d} \\Theta_1\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} l_1^{2} \\right) m_1 + 0.5 \\left( \\left( l_1 \\cos\\left( \\Theta_1\\left( t \\right) \\right) \\frac{\\mathrm{d} \\Theta_1\\left( t \\right)}{\\mathrm{d}t} + l_2 \\cos\\left( \\Theta_2\\left( t \\right) \\right) \\frac{\\mathrm{d} \\Theta_2\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} + \\left( l_1 \\frac{\\mathrm{d} \\Theta_1\\left( t \\right)}{\\mathrm{d}t} \\sin\\left( \\Theta_1\\left( t \\right) \\right) + l_2 \\frac{\\mathrm{d} \\Theta_2\\left( t \\right)}{\\mathrm{d}t} \\sin\\left( \\Theta_2\\left( t \\right) \\right) \\right)^{2} \\right) m_2\n\\end{equation}\n\\]\nThe equations of motion are obtained from the Lagrange eqautions for \\(\\Theta_1\\) and \\(\\Theta_2\\)\n\\[\n\\frac{\\partial L}{\\partial \\Theta_1} - \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\partial L}{\\partial \\dot{\\Theta}_1} = 0\n\\]\n\\[\n\\frac{\\partial L}{\\partial \\Theta_2} - \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\partial L}{\\partial \\dot{\\Theta}_2} = 0\n\\]\nΘ̇₁ = Dt(Θ₁)\nΘ̇₂ = Dt(Θ₂)\n\nΘ̈₁ = Dt(Θ̇₁)\nΘ̈₂ = Dt(Θ̇₂)\n\nDΘ₁ = Differential(Θ₁)\nDΘ₂ = Differential(Θ₂)\n\nDΘ̇₁ = Differential(Θ̇₁)\nDΘ̇₂ = Differential(Θ̇₂)\n\nL = simplify(expand_derivatives(L))\n\nLE₁ = simplify(expand_derivatives(DΘ₁(L) - Dt(DΘ̇₁(L))); expand = true)\nLE₂ = simplify(expand_derivatives(DΘ₂(L) - Dt(DΘ̇₂(L))); expand = true);\nWe need to run expand_derivatives for LE1 and LE2 to be able to solve the equation! Otherwise we get a singular exception!\nNow solve for \\(\\ddot{\\Theta}_1\\) and \\(\\ddot{\\Theta}_2\\)\ndu1, du2 = simplify.(symbolic_linear_solve([LE₁ ~ 0, LE₂ ~ 0], [Θ̈₁, Θ̈₂]))\n\n2-element Vector{SymbolicUtils.BasicSymbolic{Real}}:\n (-g*m₁*(cos(Θ₂(t))^2)*sin(Θ₁(t)) - g*m₁*sin(Θ₁(t))*(sin(Θ₂(t))^2) + g*m₂*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₂(t)) - g*m₂*(cos(Θ₂(t))^2)*sin(Θ₁(t)) + l₁*m₂*(cos(Θ₁(t))^2)*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*sin(Θ₂(t)) - l₁*m₂*cos(Θ₁(t))*(cos(Θ₂(t))^2)*(Differential(t)(Θ₁(t))^2)*sin(Θ₁(t)) + l₁*m₂*cos(Θ₁(t))*(Differential(t)(Θ₁(t))^2)*sin(Θ₁(t))*(sin(Θ₂(t))^2) - l₁*m₂*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^2)*sin(Θ₂(t)) + l₂*m₂*cos(Θ₁(t))*(cos(Θ₂(t))^2)*(Differential(t)(Θ₂(t))^2)*sin(Θ₂(t)) + l₂*m₂*cos(Θ₁(t))*(Differential(t)(Θ₂(t))^2)*(sin(Θ₂(t))^3) - l₂*m₂*(cos(Θ₂(t))^3)*(Differential(t)(Θ₂(t))^2)*sin(Θ₁(t)) - l₂*m₂*cos(Θ₂(t))*(Differential(t)(Θ₂(t))^2)*sin(Θ₁(t))*(sin(Θ₂(t))^2)) / (l₁*(m₁*(cos(Θ₁(t))^2)*(cos(Θ₂(t))^2) + m₁*(cos(Θ₁(t))^2)*(sin(Θ₂(t))^2) + m₁*(cos(Θ₂(t))^2)*(sin(Θ₁(t))^2) + m₁*(sin(Θ₁(t))^2)*(sin(Θ₂(t))^2) + m₂*(cos(Θ₁(t))^2)*(sin(Θ₂(t))^2) - (2//1)*m₂*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₁(t))*sin(Θ₂(t)) + m₂*(cos(Θ₂(t))^2)*(sin(Θ₁(t))^2)))\n (-g*m₁*(cos(Θ₁(t))^2)*sin(Θ₂(t)) + g*m₁*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₁(t)) - g*m₂*(cos(Θ₁(t))^2)*sin(Θ₂(t)) + g*m₂*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₁(t)) - l₁*m₁*(cos(Θ₁(t))^3)*(Differential(t)(Θ₁(t))^2)*sin(Θ₂(t)) + l₁*m₁*(cos(Θ₁(t))^2)*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*sin(Θ₁(t)) - l₁*m₁*cos(Θ₁(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^2)*sin(Θ₂(t)) + l₁*m₁*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^3) - l₁*m₂*(cos(Θ₁(t))^3)*(Differential(t)(Θ₁(t))^2)*sin(Θ₂(t)) + l₁*m₂*(cos(Θ₁(t))^2)*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*sin(Θ₁(t)) - l₁*m₂*cos(Θ₁(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^2)*sin(Θ₂(t)) + l₁*m₂*cos(Θ₂(t))*(Differential(t)(Θ₁(t))^2)*(sin(Θ₁(t))^3) - l₂*m₂*(cos(Θ₁(t))^2)*cos(Θ₂(t))*(Differential(t)(Θ₂(t))^2)*sin(Θ₂(t)) + l₂*m₂*cos(Θ₁(t))*(cos(Θ₂(t))^2)*(Differential(t)(Θ₂(t))^2)*sin(Θ₁(t)) - l₂*m₂*cos(Θ₁(t))*(Differential(t)(Θ₂(t))^2)*sin(Θ₁(t))*(sin(Θ₂(t))^2) + l₂*m₂*cos(Θ₂(t))*(Differential(t)(Θ₂(t))^2)*(sin(Θ₁(t))^2)*sin(Θ₂(t))) / (l₂*m₁*(cos(Θ₁(t))^2)*(cos(Θ₂(t))^2) + l₂*m₁*(cos(Θ₁(t))^2)*(sin(Θ₂(t))^2) + l₂*m₁*(cos(Θ₂(t))^2)*(sin(Θ₁(t))^2) + l₂*m₁*(sin(Θ₁(t))^2)*(sin(Θ₂(t))^2) + l₂*m₂*(cos(Θ₁(t))^2)*(sin(Θ₂(t))^2) - (2//1)*l₂*m₂*cos(Θ₁(t))*cos(Θ₂(t))*sin(Θ₁(t))*sin(Θ₂(t)) + l₂*m₂*(cos(Θ₂(t))^2)*(sin(Θ₁(t))^2))\nand create a Julia function that can be used in ODEProblem.\nu̇ = [Θ̇₁; Θ̇₂; du1; du2]\n\ndu, du! = build_function(u̇, [Θ₁, Θ₂, Θ̇₁, Θ̇₂],  [m₁, m₂, l₁, l₂, g], t; expression = Val{false});\nNow we can solve the ODE system numerically\nusing DifferentialEquations\n\ntspan = (0, 40)\nparams = [2.0, 1.0, 2.0, 1.0, 9.81]\ninit = [1, -3, -1, 5]\n\nprob = ODEProblem(du, init, tspan, params);\nsol = solve(prob);\nti = range(tspan...; length = 1001)\ntheta1 = [sol(t)[1] for t in ti]\ntheta2 = [sol(t)[2] for t in ti]\n\nf = Figure()\nlines(f[1,1], ti, theta1)\nlines!(f[1,1], ti, theta2)\n\nf"
  },
  {
    "objectID": "posts/DoublePendulum/index.html#animation",
    "href": "posts/DoublePendulum/index.html#animation",
    "title": "Double pendulum",
    "section": "Animation",
    "text": "Animation\n\nl1 = 2.0\nl2 = 1.0\n\nx1 = l1 * sin.(theta1)\ny1 = -l1 * cos.(theta1)\nx2 = x1 .+ l2 * sin.(theta2)\ny2 = y1 .- l2 * cos.(theta2)\n\nf = Figure()\n\nax1 = Axis(f[1,1], xlabel = L\"t\", ylabel = L\"\\Theta\")\nxlims!(ax1, tspan...)\nylims!(ax1, \n    1.1 * minimum(vcat(theta1, theta2)),\n    1.1 * maximum(vcat(theta1, theta2)))\n\nax2 = Axis(f[1,2]; aspect = 1)\nhidedecorations!(ax2)\nhidespines!(ax2)\nl = 1.1*(l1 + l2)\nxlims!(ax2, -l,l)\nylims!(ax2, -l,l)\n\nrecord(f, \"double_pendulum_animation.gif\", 1:length(ti), framerate = 30) do i\n\n    empty!(ax1)\n    lines!(ax1, ti[1:i], theta1[1:i]; color = :blue)\n    lines!(ax1, ti[1:i], theta2[1:i]; color = :red)\n\n    empty!(ax2)\n \n    lines!(ax2, [0, x1[i], x2[i]], [0, y1[i], y2[i]]; color = :blue)\n    lines!(ax2, x2[1:i], y2[1:i]; color = (:red, 0.1))\n    scatter!(ax2, [x1[i], x2[i]], [y1[i], y2[i]]; markersize = 20, color = [:blue, :red])\n\nend;\n\n\n\n\nDouble pendulum animation"
  },
  {
    "objectID": "posts/DoublePendulum/index.html#references",
    "href": "posts/DoublePendulum/index.html#references",
    "title": "Double pendulum",
    "section": "References",
    "text": "References\nsee double-pendulum.jl here: https://www.phys.uconn.edu/~rozman/Courses/P3101_22F/downloads/\nsee https://cooperrc.github.io/Julia-learning/day_06.html"
  },
  {
    "objectID": "posts/TensorNotation/index.html",
    "href": "posts/TensorNotation/index.html",
    "title": "Tensor notation",
    "section": "",
    "text": "Vectors\nIn tensor notation, a vector is written as\n\\[\n\\mathbf{u} = u_i\n\\]\nwhere \\(i = 1,2,3\\), corresponding to the \\(x\\), \\(y\\), and \\(z\\) component, respectively.\n\\[\n\\mathbf{u} + \\mathbf{v} = u_i + v_i\n\\]\n\n\nEinstein summation\nAny index appearing twice is automatically summed from 1 to 3. This is called Einstein summation. Any index can at most appear twice.\n\n\nDot (inner) product\nUsing Einstein summation, the dot (inner) product of two vectors is\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_i v_i = u_1 v_1 + u_2 v_2 + u_3 v_3\n\\]\n\n\nDyadic (outer) product\nThe dyadic (outer) product of two vectors is\n\\[\n\\mathbf{u} \\otimes \\mathbf{v} = u_i v_j =\n\\left[\n\\begin{matrix}\nu_1 v_1 & u_1 v_2 & u_1 v_3 \\cr\nu_2 v_1 & u_2 v_2 & u_2 v_3 \\cr\nu_3 v_1 & u_3 v_2 & u_3 v_3\n\\end{matrix}\n\\right]\n\\]\n\n\nDifferentiation with respect to time\nDifferentiation with respect to time can be written as\n\\[\n\\frac{\\text{d}\\mathbf{x}}{\\text{d}t} = \\mathbf{\\dot{x}} = \\dot{x_i} = x_{i|t}\n\\]\n\n\nDifferentiation with respect to space\nThe gradient of a scalar function is given by\n\\[\n\\nabla \\phi = \\partial_i \\phi = \\phi_{|i}\n\\]\nThe component-wise spatial derivative of a vector\n\\[\n\\frac{\\partial \\mathbf{u}}{\\partial x_j} = u_{i|j}\n\\]\nThe divergence of a vector is\n\\[\n\\text{div} \\ \\mathbf{u} = \\nabla \\cdot \\mathbf{u} = u_{i|i}\n\\]\n\n\nKronecker delta\nThe Kronecker delta is defined as\n\\[\n\\delta_{ij} = \\begin{cases}\n\\begin{matrix}\n1 & i = j \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nKronecker delta is also often called the substitution operator, as\n\\[\nu_i \\delta_{ij} = u_j\n\\]\nThe Kronecker delta often occurs when one deals with spatial derivatives of position\n\\[\n\\frac{\\partial x_i}{\\partial x_j} = \\partial_j x_i = x_{i|j} = \\delta_{ij}\n\\]\n\n\nLevi-Civita alternating tensor\n\\[\n\\epsilon_{ijk} = \\begin{cases}\n\\begin{matrix}\n1 & ijk \\in \\{123, 231, 312\\} \\cr\n-1 & ijk \\in \\{132, 213, 321\\} \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nSometimes the following equality can be useful to simplify expressions:\n\\[\n\\epsilon_{ijk} \\epsilon_{imn} = \\delta_{jm}\\delta_{kn}-\\delta_{jn}\\delta_{km}\n\\]\n\n\nRotation or curl of a vector\n\\[\n\\text{rot} \\ \\mathbf{u} = \\nabla \\times \\mathbf{u} = \\epsilon_{ijk} u_{k|j}\n\\]\nReferences: [https://www.continuummechanics.org]"
  },
  {
    "objectID": "posts/BayesianLCA/index.html",
    "href": "posts/BayesianLCA/index.html",
    "title": "Bayesian Latent Class Analysis",
    "section": "",
    "text": "Bayesian Latent Class Analysis (BLCA) is a powerful statistical method used to classify subjects into unobserved (latent) groups based on their responses to observed variables. The method relies on the Bayesian framework to incorporate prior knowledge and manage uncertainty, making it robust and flexible for various applications, such as in social sciences, medicine, and marketing.\n\nBasic Concepts of Latent Class Analysis (LCA)\nLCA models assume that there is an underlying categorical variable (latent class) that explains the patterns of responses observed in the data. Each subject belongs to one of these latent classes, and the probability of each observed response depends on the latent class membership.\n\n\nBayesian Framework\nIn the Bayesian approach, we introduce prior distributions for the model parameters and update these priors with the observed data to obtain posterior distributions. This incorporation of prior knowledge can help guide the analysis, especially when data is sparse or noisy.\n\n\nGenerative model\n\n\n\n\n\n\n\nG\n\n\ncluster_N\n\n\n\ncluster_J\n\n\n\n\nalpha\n\nu\nc\n\n\n\npi\n\nπ\n\n\n\nalpha-&gt;pi\n\n\n\n\n\nbeta\n\nα\njk\n, \nβ\njk\n\n\n\ntheta\n\nθ\njk\n\n\n\nbeta-&gt;theta\n\n\n\n\n\nc\n\nc\ni\n\n\n\npi-&gt;c\n\n\n\n\n\ny\n\ny\nik\n\n\n\ntheta-&gt;y\n\n\n\n\n\nc-&gt;y\n\n\n\n\n\nN_label\ni = 1 .. N subjects\n\n\n\nJ_label\nk = 1 .. K questions\n\n\n\n\n\n\n\n\nFrom the diagram we can see that the joint distribution can be decomposed into the following factors\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) =\n    P(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) P(\\mathbf{c}|\\pi)\n    P(\\pi|u) P(\\mathbf{\\Theta}| \\alpha, \\beta)\n\\]\nAssume \\(N\\) subjects are distributed over \\(C\\) classes.\nThe Dirichlet distribution is the conjugate prior of the Categorical and Multinomial distributions. The mean is \\(\\pi_j = u_j / \\sum_j u_j\\) while the mode is \\((u_j - 1)/\\sum_j(u_j - 1)\\) for \\(u_j &gt; 1\\). It is the appropriate prior when we need to make a \\(C\\)-way choice. The prior probabilities for the class membership is assumed to be Dirichlet\n\\[\n\\pi \\sim \\mathrm{Dirichlet}(u_1, \\ldots, u_C) \\propto \\prod_{j=1}^C \\pi_j^{u_j - 1}\n\\]\nThe probability that subject \\(i\\) is in class \\(j\\) is given by\n\\[\nP(c_{i} = j | \\pi) = \\pi_j\n\\]\nGiven \\(\\pi\\), the class assignments for the \\(N\\) subjects are independent. Therefore,\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{i=1}^N P(c_i|\\pi)\n\\]\nIf we let \\(N_j\\) denote the number of subjects in class \\(j\\), this expression can be simplified to\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{j=1}^C \\pi_j^{N_j}\n\\]\nLet \\(\\Theta_{jk}\\) be the probability that a subject of class \\(j\\) answers question \\(k\\) positive. Then\n\\[\nP(y_{ik} = 1 | c_{i} = j, \\mathbf{\\Theta}) = \\Theta_{jk}\n\\]\nand \\[\nP(y_{ik} = 0 | c_{i} = j, \\mathbf{\\Theta}) = 1 - \\Theta_{jk}\n\\]\nAnswering a yes/no question is a two-way choice (or Bernoulli experiment). The Beta distribution is an appropriate prior for a two-way choice, and we assume \\(\\Theta_{jk}\\) to be Beta distributed\n\\[\nP(\\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K \\mathrm{Beta}(\\alpha_{jk}, \\beta_{jk}) \\propto \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n\\]\nLooking at the plate diagram, we can see that the \\(K\\) questions the \\(N\\) subjects answer are independent given it classes \\(\\mathbf{c}\\) and parameters \\(\\mathbf{\\Theta}\\). Therefore,\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n  \\prod_{i=1}^N \\prod_{k=1}^K  \\Theta_{c_i k}^{y_{ik}}(1-\\Theta_{c_i k})^{1-y_{ik}}\n\\]\nThis expression can be simplified by counting how often the factors \\(\\Theta_{jk}\\) and \\((1-\\Theta_{jk})\\) occur.\nLet \\(N_{jk}\\) denote number of times the question \\(k\\) was answered positive for members of class \\(j\\), then \\(N_j - N_{jk}\\) the number of times it was answered negative.\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j - N_{jk}}\n\\]\nPutting everything together, we end up with the following expression for the joint probability\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j -N_{jk}} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{N_j} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{u_j - 1} \\right)\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K\n        \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n    \\right)\n\\]\nSome rearrangement yields\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n\\left( \\prod_{j=1}^C  \\pi_j^{N_j + u_j - 1} \\right)\n\\left(\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk} + \\alpha_{jk} - 1} (1 - \\Theta_{jk})^{N_j - N_{jk} + \\beta_{jk} - 1}\n\\right)\n\\]\nThis shows that the posterior joint distribution factors into a Dirichlet posterior for \\(\\pi\\) and a product of Beta posteriors for \\(\\Theta\\).\nWe can generate data according to the above model using the following Julia code\n\nusing Distributions\n\nC = 2       # number of classes, j = 1 .. C\nK = 3       # number of questions, k = 1 .. K\nN = 1000     # number of subjects, i = 1 .. N\n\nπ = [0.2, 0.8]\nΘ = [0.1 0.3 0.7;\n     0.5 0.8 0.1]   \n\nc = rand(Categorical(π), N)\n\ngen = (;π, Θ, c)\n\ny = rand.(Bernoulli.(Θ[c, :]));\n\n\nusing CairoMakie\n\nf = Figure(;size=(600, 400))\n\nfor j in 1:K\n    hist(f[1,j], y[:, j]; axis = (;title = \"Question $j\"))\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nGibbs sampling\nGibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to approximate the posterior distributions of the model parameters. It iteratively samples from the conditional distributions of each parameter given the current values of the others.\nAbove we had derived an expression for the joint probability \\(P(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}, \\mathbf{\\Theta})\\). As \\(\\mathbf{y}\\) is observed, we only need to create conditional samples for \\(\\mathbf{c}\\), \\(\\mathbf{\\pi}\\), and \\(\\mathbf{\\Theta}\\).\n\\[\nP(\\mathbf{\\pi} |  \\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\mathrm{Dirichlet}(\\ldots, u_j + N_j, \\ldots)\n\\]\n\\[\nP(\\Theta_{jk} | \\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}) =\n    \\mathrm{Beta}(\\alpha_{jk} + N_{jk}, \\beta_{jk} + N_j -  N_{jk})\n\\]\nTo draw a sample for \\(c_i\\), we have to compute \\[\nP(c_i | \\mathbf{y}, \\mathbf{c}_{-i}, \\mathbf{\\Theta}, \\pi)\n\\]\nwhere \\(\\mathbf{c}_{-i}\\) denotes the vector of class membership for all subjects except \\(i\\). From the plate diagram, we can see that conditioned on \\(\\pi\\) and \\(\\mathrm{\\Theta}\\), \\(c_i\\) only depends on \\(\\pi\\), \\(\\mathrm{\\Theta}\\), and \\(y_{ik}\\). We have\n\\[\nP(c_i, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    P(c_i|\\pi) \\prod_{k=1}^K P(y_{ik}|c_i, \\mathrm{\\Theta})\n\\]\n\\[\nP(c_i = j, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    \\pi_j \\prod_{k=1}^K \\Theta_{jk}^{y_{ik}}(1 - \\Theta_{jk})^{(1-y_{ik})}\n\\]\nThe Gibbs sampling algorithm can be summarized as follows:\n\nInitialize \\(\\mathbf{\\Theta}\\), \\(\\pi\\), \\(\\mathbf{c}\\)\ncompute \\(N_j\\), \\(N_{jk}\\)\nfor iter in 1:max_iter\n\nfor i in 1:N\n\nsample \\(c_i\\), and update \\(N_j\\) and \\(N_{jk}\\)\n\nsample \\(\\mathbf{\\pi}\\) using \\(N_j\\)\nsample \\(\\mathbf{\\Theta}\\) using \\(N_{jk}\\)\n\n\n\nusing DataFrames\nusing StatsBase: sample, Weights\nusing StatsFuns: logsumexp\n\n# Initialise\nu = ones(C)\nα = ones(C,K)\nβ = ones(C,K)\n\nπ = rand(Dirichlet(u))\nΘ = rand.(Beta.(α, β))\nc = rand(Categorical(π), N)\n\nN_j = zeros(C)      # number of subjects in class\nN_jk = zeros(C, K)  # number of positive answers for class j and question k\n\nfor i in 1:N\n    j = c[i]\n    N_j[j] += 1\n    for k in 1:K\n        N_jk[j, k] += y[i,k]\n    end\nend\n\nmax_iter = 11000\nburnin = 1000\nthinning = 10\n\nsamples = DataFrame()\n\nfor iter in 1:max_iter\n    # sample c[i]\n    for i in 1:N\n        # remove c[i]from N_j and N_jk\n        N_j[c[i]] -= 1\n        for k in 1:K\n            N_jk[c[i], k] -= y[i,k]\n        end\n        # compute p(c_i = j)\n        log_p_c = zeros(Float64, C)\n        for j in 1:C\n            log_p_c[j] += log(π[j])\n            for k in 1:K\n                log_p_c[j] += y[i,k]*log(Θ[j,k]) + (1.0 - y[i,k])*log(1.0 - Θ[j,k])\n            end\n        end\n        p_c = exp.(log_p_c .- logsumexp(log_p_c))\n        # and sample c[i]\n        c[i] = sample(1:K, Weights(p_c))\n        # add c[i] to N_j and N_jk\n        N_j[c[i]] += 1\n        for k in 1:K\n            N_jk[c[i], k] += y[i,k]\n        end\n    end\n\n    # sample \\pi\n    π = rand(Dirichlet(u .+ N_j))\n    \n    # sample \\Theta\n    Θ = rand.(Beta.(α .+ N_jk, β .+ N_j .- N_jk))\n\n    if iter &gt; burnin && (iter - burnin) % thinning == 0\n        push!(samples, (;π, Θ, c = copy(c)))\n    end\n\nend\n\nLet’s look at trace plots of the sampled parameters:\n\nf = Figure()\n\nlines(f[1:2,1], [x[1] for x in samples.π]; axis = (;title = \"π\"))\nlines!(f[1:2,1], [x[2] for x in samples.π])\nylims!(0,1)\n\nfor j in 1:C\n    for k in 1:K\n        lines(f[j,k+1], [x[j, k] for x in samples.Θ]; axis = (;title = \"Θ[$j,$k]\"))\n        ylims!(0,1)\n    end\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nExpectation-Maximization (EM)\nThe Expectation-Maximization method results in point estimates of the parameters \\(\\mathbf{\\Theta}\\) and \\(\\pi\\), and probability distributions of latent variables \\(\\mathbf{c}\\).\nSimilar to the Gibbs sampling, we compute for each subject the class membership probabilities \\(P(\\mathbf{c}|\\mathbf{\\Theta}, \\pi)\\). EM uses the joint class distributions to compute the MLE of parameters \\(\\pi\\) and \\(\\mathbf{\\Theta}\\).\n\\[\n\\pi_j = \\frac{\\sum_{i=1}^N P(c_i = j|\\mathbf{\\Theta}, \\pi)}{N} = \\frac{N_j}{N}\n\\]\n\\[\n\\Theta_{jk} = \\frac{\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi) y_{ik}}\n                    {\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi)} =\n                    \\frac{N_{jk}}{N_j}\n\\]\nIf one also incorporates prior counts (\\(u\\) and \\(\\alpha_{jk}\\), \\(\\beta_{jk]}\\) above), the MLE is replaced by a MAP estimate.\n\\[\n\\pi_j = \\frac{N_j + u_j - 1}{N + \\sum_{j=1}^C (u_j - 1)}\n\\]\n\\[\n\\Theta_{jk} = \\frac{N_{jk} + \\alpha_{jk} - 1}{N_j + \\alpha_{jk} + \\beta_{jk} - 2}\n\\]\n\n\nReferences\n\nResnik et al.(2010), Gibbs sampling for the uninitiated, http://users.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf\nLi et al.(2019), Bayesian Latent Class Analysis Tutorial https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6364555/, but here the notation seems seriously flawed"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]