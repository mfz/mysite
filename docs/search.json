[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Notes",
    "section": "",
    "text": "Tensor notation\n\n\n\nphysics\n\n\n\nIntroduction to tensor, index, or Einstein notation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPendulum\n\n\n\njulia\n\n\nphysics\n\n\n\nSimulating a pendulum in Julia, analytically, numerically, and symbolically\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Latent Class Analysis\n\n\n\njulia\n\n\nBayes\n\n\nMCMC\n\n\nEM\n\n\n\nLatent class analysis using Gibbs sampling and Expectation-Maximization\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nFor a nice tutorial on how to create them visit https://ucsb-meds.github.io/creating-quarto-websites/"
  },
  {
    "objectID": "posts/Pendulum/index.html",
    "href": "posts/Pendulum/index.html",
    "title": "Pendulum",
    "section": "",
    "text": "A pendulum can be modelled as a mass \\(m\\) on a weightless rod of length \\(l\\). The forces acting on the mass are gravity and the tension in the rod.\nWe can use Newton’s second law to derive equations of motion.\nThe component of gravity along the direction of the string is compensated by the tension in the string. Only the force perpendicular to the string is used to accelerate the mass. Here we consider arclength \\(s = l \\Theta\\)\n\\[\nm \\frac{\\mathrm{d}^2s}{\\mathrm{d}t^2} = - m g \\sin(\\Theta)\n\\]\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\sin(\\Theta)\n\\]\nThis is a non-linear ordinary differential equation (ODE) of second order."
  },
  {
    "objectID": "posts/Pendulum/index.html#equations-of-motion",
    "href": "posts/Pendulum/index.html#equations-of-motion",
    "title": "Pendulum",
    "section": "",
    "text": "A pendulum can be modelled as a mass \\(m\\) on a weightless rod of length \\(l\\). The forces acting on the mass are gravity and the tension in the rod.\nWe can use Newton’s second law to derive equations of motion.\nThe component of gravity along the direction of the string is compensated by the tension in the string. Only the force perpendicular to the string is used to accelerate the mass. Here we consider arclength \\(s = l \\Theta\\)\n\\[\nm \\frac{\\mathrm{d}^2s}{\\mathrm{d}t^2} = - m g \\sin(\\Theta)\n\\]\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\sin(\\Theta)\n\\]\nThis is a non-linear ordinary differential equation (ODE) of second order."
  },
  {
    "objectID": "posts/Pendulum/index.html#analytic-solution-of-linearized-ode",
    "href": "posts/Pendulum/index.html#analytic-solution-of-linearized-ode",
    "title": "Pendulum",
    "section": "Analytic solution of linearized ODE",
    "text": "Analytic solution of linearized ODE\nTo solve that equation analytically, it can be linearized using the Taylor expansion\n\\[\n\\sin(\\Theta) = \\Theta - \\frac{\\Theta^3}{3!} + \\frac{\\Theta^5}{5!} + \\ldots\n\\]\n\nusing CairoMakie\nusing LaTeXStrings\n\nset_theme!()\n\nΘ = range(0, π/2; length = 100)\napprox(Θ) = Θ - Θ^3/6 + Θ^5/120\n\nf = Figure()\na, p = lines(f[1,1], Θ, sin.(Θ); \n            linewidth = 7,\n            color = (:blue, 0.2), \n            label = L\"\\sin(Θ)\", \n            axis = (;title = \"Taylor approximation of sin(Θ)\", xlabel = L\"Θ\"))\nlines!(f[1,1], Θ, Θ; label = L\"Θ\")\nlines!(f[1,1], Θ, Θ - Θ .^ 3/6, label = L\"Θ - \\frac{Θ^3}{3!}\")\nlines!(f[1,1], Θ, Θ - Θ .^ 3 / 6 + Θ .^ 5 / 120, label = L\"Θ - \\frac{Θ^3}{3!} + \\frac{Θ^5}{5!}\")\n\naxislegend(a; position = :lt)\n\nf\n\n\n\n\n\n\n\n\nFor small angles (\\(\\Theta \\le 30°\\)) the first order Taylor approximation is reasonable. For larger angles, we overestimate the restoring force and, therefore, the frequency.\nThe resulting linear second-order ODE\n\\[\n\\frac{\\mathrm{d}^2\\Theta}{\\mathrm{d}t^2} = - \\frac{g}{l} \\Theta\n\\]\ncan be solved analytically with solution\n\\[\n\\Theta(t) = A \\cos(\\omega t) + B \\cos(\\omega t)\n\\]\nwith\n\\[\n\\omega = \\sqrt{\\frac{g}{l}}\n\\]\nThe solution to the linear ODE with \\(\\Theta_0 = \\pi/6 = 30°\\) is plotted below\n\ng = 9.81\nl = 1\nω = sqrt(g/l)\nt = range(0, 4*pi/ω, length = 200)\nΘ₀ = π/6\nanalytical = Θ₀ * cos.(ω*t)\n\nf2 = Figure()\n\nlines(f2[1,1], t, analytical * 180/pi;\n      axis = (;xlabel = \"time (s)\", ylabel = L\"$\\theta$ (deg)\"))\n\nf2"
  },
  {
    "objectID": "posts/Pendulum/index.html#numerical-solution-of-non-linear-ode",
    "href": "posts/Pendulum/index.html#numerical-solution-of-non-linear-ode",
    "title": "Pendulum",
    "section": "Numerical solution of non-linear ODE",
    "text": "Numerical solution of non-linear ODE\nThe nonlinear ODE can be solved using DifferentialEquations.jl. First we need to tranform the 2-order ODE into a 1-order ODE.\nThe state \\(u(t)\\) is given as \\[\nu(t) = \\begin{pmatrix}\n    \\Theta(t) \\\\\n    \\dot{\\Theta}(t)\n    \\end{pmatrix}\n\\]\n\\[\n\\dot{u}(t) = \\begin{pmatrix}\n    \\dot{\\Theta}(t) \\\\\n    - \\frac{g}{l} \\sin(\\Theta(t))\n\\end{pmatrix}\n\\]\nNow we need to define a function that provides \\(\\dot{u}(t)\\) given \\(u(t)\\), \\(t\\), and possible parameters.\n\nfunction pendulum(u, params, t)\n    du = zeros(length(u))\n    l = params[1]\n    du[1] = u[2]\n    du[2] = -g/l*sin(u[1])\n    return du\nend;\n\n\nusing DifferentialEquations\n\ntspan = (0, 4π/ω)\nparams = [l]\ninit = [π/6, 0]\nprob = ODEProblem(pendulum, init, tspan, params);\n\nu = solve(prob) returns the solution to the ODEProblem. Note that the time stepping is done in an adaptive manner, but u(t) interpolates the system state for any time \\(t \\in\\) tspan.\n\nu = solve(prob)\n\nlines!(f2[1,1], t, 180/π*[u(ti)[1] for ti in t])\n\nf2\n\n\n\n\n\n\n\n\nAs already stated above, the small-angle approximation overestimates the frequency and underestimates the period."
  },
  {
    "objectID": "posts/Pendulum/index.html#symbolic-solution-using-lagrangian",
    "href": "posts/Pendulum/index.html#symbolic-solution-using-lagrangian",
    "title": "Pendulum",
    "section": "Symbolic solution using Lagrangian",
    "text": "Symbolic solution using Lagrangian\nAbove, we had derived the equations of motion by hand. This is not always feasible. Julia has symbolic computation capabilities through the package Symbolics.jl. This allows us to use Julia as a computer algebra system (CAS) to derive the equations of motion from the Lagrangian, which is easy to specify.\nFirst we define the Lagrangian symbolically\n\nusing Symbolics\n\n@variables t m g l Θ(t)\n\n# define Theta, x, and y as above\nx = l*sin(Θ)\ny = -l*cos(Θ)\n\nDt = Differential(t)\n\n# define kinetic and potential energies\nT = 1/2*m*(Dt(x)^2 + Dt(y)^2)\nV = m*g*y\n\n# and the Lagrangian\nL = T - V;\n\nNote that Julia has not performed any differentiation yet. We need to call expand_derivatives to do so.\n\nexpand_derivatives(L)\n\n\\[ \\begin{equation}\ng l m \\cos\\left( \\Theta\\left( t \\right) \\right) + 0.5 \\left( \\sin^{2}\\left( \\Theta\\left( t \\right) \\right) \\left( \\frac{\\mathrm{d} \\Theta\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} l^{2} + \\cos^{2}\\left( \\Theta\\left( t \\right) \\right) \\left( \\frac{\\mathrm{d} \\Theta\\left( t \\right)}{\\mathrm{d}t} \\right)^{2} l^{2} \\right) m\n\\end{equation}\n\\]\n\n\nTo get the equations of motion from the Lagrangian, we need to compute\n\\[\n\\frac{\\partial L}{\\partial \\Theta} - \\frac{\\mathrm{d}}{\\mathrm{d}t} \\frac{\\partial L}{\\partial \\dot{\\Theta}}\n\\]\nand solve for \\(\\ddot{\\Theta}\\). This can be done using symbolic_linear_solve.\n\nΘ̇ = Dt(Θ)\nΘ̈ = Dt(Θ̇)\nDΘ = Differential(Θ)\nDΘ̇ = Differential(Θ̇)\n\nLE = simplify(expand_derivatives(DΘ(L) -  Dt(DΘ̇(L))))\n\nu̇ = [Θ̇; simplify(symbolic_linear_solve(LE, Θ̈))]\n\n# define a function du(u, params, t) to be used in ODEProblem\ndu, du! = build_function(u̇, [Θ, Θ̇],  [l, g], t; expression = Val{false});\n\nThese functions can now be used to solve the DE numerically using DifferentialEquations.jl as above.\n\ntspan = (0, 4π/ω)\nparams = [1.0, 9.81]\ninit = [π/6, 0]\nprob = ODEProblem(du, init, tspan, params);\n\n\nsol = solve(prob)\n\nt = range(0, 4*pi/ω, length = 200)\nlines!(f2[1,1], t, 180/π*[sol(ti)[1] for ti in t])\n\nf2"
  },
  {
    "objectID": "posts/Pendulum/index.html#animation",
    "href": "posts/Pendulum/index.html#animation",
    "title": "Pendulum",
    "section": "Animation",
    "text": "Animation\nWe can use the record function in CairoMakie.jl to create an animation.\n\n\nu1 = [u(ti)[1] for ti in t]\n\nf = Figure()\n\nax1 = Axis(f[1,1], xlabel = L\"t\", ylabel = L\"Θ\")\nxlims!(ax1, tspan...)\nylims!(ax1, -1,1)\n\nax2 = Axis(f[1,2]; aspect = 1)\nhidedecorations!(ax2)\nhidespines!(ax2)\nxlims!(ax2, -12,12)\nylims!(ax2, -12,12)\n\nr = 10.0\n\nrecord(f, \"incremental_plot_animation.gif\", 1:length(t), framerate = 30) do i\n\n    empty!(ax1)\n    lines!(ax1, t[1:i], u1[1:i]; color = :blue)\n    lines!(ax1, t[1:i], analytical[1:i]; color = :red)\n\n    empty!(ax2)\n    y = -r*cos(u1[i])\n    x = r*sin(u1[i])\n    lines!(ax2, [0,x], [0,y]; color = :blue)\n    scatter!(ax2, x, y; markersize = 20, color = :blue)\n\n    y = -r*cos(analytical[i])\n    x = r*sin(analytical[i])\n    lines!(ax2, [0,x], [0,y]; color = :red)\n    scatter!(ax2, x, y; markersize = 20, color = :red)\nend;\n\n\n\n\nAnimation"
  },
  {
    "objectID": "posts/Pendulum/index.html#references",
    "href": "posts/Pendulum/index.html#references",
    "title": "Pendulum",
    "section": "References",
    "text": "References\n\nhttps://cooperrc.github.io/Julia-learning/day_05.html"
  },
  {
    "objectID": "posts/TensorNotation/index.html",
    "href": "posts/TensorNotation/index.html",
    "title": "Tensor notation",
    "section": "",
    "text": "Vectors\nIn tensor notation, a vector is written as\n\\[\n\\mathbf{u} = u_i\n\\]\nwhere \\(i = 1,2,3\\), corresponding to the \\(x\\), \\(y\\), and \\(z\\) component, respectively.\n\\[\n\\mathbf{u} + \\mathbf{v} = u_i + v_i\n\\]\n\n\nEinstein summation\nAny index appearing twice is automatically summed from 1 to 3. This is called Einstein summation. Any index can at most appear twice.\n\n\nDot (inner) product\nUsing Einstein summation, the dot (inner) product of two vectors is\n\\[\n\\mathbf{u} \\cdot \\mathbf{v} = u_i v_i = u_1 v_1 + u_2 v_2 + u_3 v_3\n\\]\n\n\nDyadic (outer) product\nThe dyadic (outer) product of two vectors is\n\\[\n\\mathbf{u} \\otimes \\mathbf{v} = u_i v_j =\n\\left[\n\\begin{matrix}\nu_1 v_1 & u_1 v_2 & u_1 v_3 \\cr\nu_2 v_1 & u_2 v_2 & u_2 v_3 \\cr\nu_3 v_1 & u_3 v_2 & u_3 v_3\n\\end{matrix}\n\\right]\n\\]\n\n\nDifferentiation with respect to time\nDifferentiation with respect to time can be written as\n\\[\n\\frac{\\text{d}\\mathbf{x}}{\\text{d}t} = \\mathbf{\\dot{x}} = \\dot{x_i} = x_{i|t}\n\\]\n\n\nDifferentiation with respect to space\nThe gradient of a scalar function is given by\n\\[\n\\nabla \\phi = \\partial_i \\phi = \\phi_{|i}\n\\]\nThe component-wise spatial derivative of a vector\n\\[\n\\frac{\\partial \\mathbf{u}}{\\partial x_j} = u_{i|j}\n\\]\nThe divergence of a vector is\n\\[\n\\text{div} \\ \\mathbf{u} = \\nabla \\cdot \\mathbf{u} = u_{i|i}\n\\]\n\n\nKronecker delta\nThe Kronecker delta is defined as\n\\[\n\\delta_{ij} = \\begin{cases}\n\\begin{matrix}\n1 & i = j \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nKronecker delta is also often called the substitution operator, as\n\\[\nu_i \\delta_{ij} = u_j\n\\]\nThe Kronecker delta often occurs when one deals with spatial derivatives of position\n\\[\n\\frac{\\partial x_i}{\\partial x_j} = \\partial_j x_i = x_{i|j} = \\delta_{ij}\n\\]\n\n\nLevi-Civita alternating tensor\n\\[\n\\epsilon_{ijk} = \\begin{cases}\n\\begin{matrix}\n1 & ijk \\in \\{123, 231, 312\\} \\cr\n-1 & ijk \\in \\{132, 213, 321\\} \\cr\n0 & \\text{otherwise}\n\\end{matrix}\n\\end{cases}\n\\]\nSometimes the following equality can be useful to simplify expressions:\n\\[\n\\epsilon_{ijk} \\epsilon_{imn} = \\delta_{jm}\\delta_{kn}-\\delta_{jn}\\delta_{km}\n\\]\n\n\nRotation or curl of a vector\n\\[\n\\text{rot} \\ \\mathbf{u} = \\nabla \\times \\mathbf{u} = \\epsilon_{ijk} u_{k|j}\n\\]\nReferences: [https://www.continuummechanics.org]"
  },
  {
    "objectID": "posts/BayesianLCA/index.html",
    "href": "posts/BayesianLCA/index.html",
    "title": "Bayesian Latent Class Analysis",
    "section": "",
    "text": "Bayesian Latent Class Analysis (BLCA) is a powerful statistical method used to classify subjects into unobserved (latent) groups based on their responses to observed variables. The method relies on the Bayesian framework to incorporate prior knowledge and manage uncertainty, making it robust and flexible for various applications, such as in social sciences, medicine, and marketing.\n\nBasic Concepts of Latent Class Analysis (LCA)\nLCA models assume that there is an underlying categorical variable (latent class) that explains the patterns of responses observed in the data. Each subject belongs to one of these latent classes, and the probability of each observed response depends on the latent class membership.\n\n\nBayesian Framework\nIn the Bayesian approach, we introduce prior distributions for the model parameters and update these priors with the observed data to obtain posterior distributions. This incorporation of prior knowledge can help guide the analysis, especially when data is sparse or noisy.\n\n\nGenerative model\n\n\n\n\n\n\n\nG\n\n\ncluster_N\n\n\n\ncluster_J\n\n\n\n\nalpha\n\nu\nc\n\n\n\npi\n\nπ\n\n\n\nalpha-&gt;pi\n\n\n\n\n\nbeta\n\nα\njk\n, \nβ\njk\n\n\n\ntheta\n\nθ\njk\n\n\n\nbeta-&gt;theta\n\n\n\n\n\nc\n\nc\ni\n\n\n\npi-&gt;c\n\n\n\n\n\ny\n\ny\nik\n\n\n\ntheta-&gt;y\n\n\n\n\n\nc-&gt;y\n\n\n\n\n\nN_label\ni = 1 .. N subjects\n\n\n\nJ_label\nk = 1 .. K questions\n\n\n\n\n\n\n\n\nFrom the diagram we can see that the joint distribution can be decomposed into the following factors\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) =\n    P(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) P(\\mathbf{c}|\\pi)\n    P(\\pi|u) P(\\mathbf{\\Theta}| \\alpha, \\beta)\n\\]\nAssume \\(N\\) subjects are distributed over \\(C\\) classes.\nThe Dirichlet distribution is the conjugate prior of the Categorical and Multinomial distributions. The mean is \\(\\pi_j = u_j / \\sum_j u_j\\) while the mode is \\((u_j - 1)/\\sum_j(u_j - 1)\\) for \\(u_j &gt; 1\\). It is the appropriate prior when we need to make a \\(C\\)-way choice. The prior probabilities for the class membership is assumed to be Dirichlet\n\\[\n\\pi \\sim \\mathrm{Dirichlet}(u_1, \\ldots, u_C) \\propto \\prod_{j=1}^C \\pi_j^{u_j - 1}\n\\]\nThe probability that subject \\(i\\) is in class \\(j\\) is given by\n\\[\nP(c_{i} = j | \\pi) = \\pi_j\n\\]\nGiven \\(\\pi\\), the class assignments for the \\(N\\) subjects are independent. Therefore,\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{i=1}^N P(c_i|\\pi)\n\\]\nIf we let \\(N_j\\) denote the number of subjects in class \\(j\\), this expression can be simplified to\n\\[\nP(\\mathbf{c}|\\pi) = \\prod_{j=1}^C \\pi_j^{N_j}\n\\]\nLet \\(\\Theta_{jk}\\) be the probability that a subject of class \\(j\\) answers question \\(k\\) positive. Then\n\\[\nP(y_{ik} = 1 | c_{i} = j, \\mathbf{\\Theta}) = \\Theta_{jk}\n\\]\nand \\[\nP(y_{ik} = 0 | c_{i} = j, \\mathbf{\\Theta}) = 1 - \\Theta_{jk}\n\\]\nAnswering a yes/no question is a two-way choice (or Bernoulli experiment). The Beta distribution is an appropriate prior for a two-way choice, and we assume \\(\\Theta_{jk}\\) to be Beta distributed\n\\[\nP(\\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K \\mathrm{Beta}(\\alpha_{jk}, \\beta_{jk}) \\propto \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n\\]\nLooking at the plate diagram, we can see that the \\(K\\) questions the \\(N\\) subjects answer are independent given it classes \\(\\mathbf{c}\\) and parameters \\(\\mathbf{\\Theta}\\). Therefore,\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n  \\prod_{i=1}^N \\prod_{k=1}^K  \\Theta_{c_i k}^{y_{ik}}(1-\\Theta_{c_i k})^{1-y_{ik}}\n\\]\nThis expression can be simplified by counting how often the factors \\(\\Theta_{jk}\\) and \\((1-\\Theta_{jk})\\) occur.\nLet \\(N_{jk}\\) denote number of times the question \\(k\\) was answered positive for members of class \\(j\\), then \\(N_j - N_{jk}\\) the number of times it was answered negative.\n\\[\nP(\\mathbf{y} | \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j - N_{jk}}\n\\]\nPutting everything together, we end up with the following expression for the joint probability\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K \\Theta_{jk}^{N_{jk}} (1-\\Theta_{jk})^{N_j -N_{jk}} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{N_j} \\right)\n    \\left( \\prod_{j=1}^C \\pi_j^{u_j - 1} \\right)\n    \\left( \\prod_{j=1}^C \\prod_{k=1}^K\n        \\Theta_{jk}^{\\alpha_{jk} - 1} (1 - \\Theta_{jk})^{\\beta_{jk} - 1}\n    \\right)\n\\]\nSome rearrangement yields\n\\[\nP(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}, \\pi | u, \\alpha, \\beta) \\propto\n\\left( \\prod_{j=1}^C  \\pi_j^{N_j + u_j - 1} \\right)\n\\left(\n    \\prod_{j=1}^C \\prod_{k=1}^K\n    \\Theta_{jk}^{N_{jk} + \\alpha_{jk} - 1} (1 - \\Theta_{jk})^{N_j - N_{jk} + \\beta_{jk} - 1}\n\\right)\n\\]\nThis shows that the posterior joint distribution factors into a Dirichlet posterior for \\(\\pi\\) and a product of Beta posteriors for \\(\\Theta\\).\nWe can generate data according to the above model using the following Julia code\n\nusing Distributions\n\nC = 2       # number of classes, j = 1 .. C\nK = 3       # number of questions, k = 1 .. K\nN = 1000     # number of subjects, i = 1 .. N\n\nπ = [0.2, 0.8]\nΘ = [0.1 0.3 0.7;\n     0.5 0.8 0.1]   \n\nc = rand(Categorical(π), N)\n\ngen = (;π, Θ, c)\n\ny = rand.(Bernoulli.(Θ[c, :]));\n\n\nusing CairoMakie\n\nf = Figure(;size=(600, 400))\n\nfor j in 1:K\n    hist(f[1,j], y[:, j]; axis = (;title = \"Question $j\"))\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nGibbs sampling\nGibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to approximate the posterior distributions of the model parameters. It iteratively samples from the conditional distributions of each parameter given the current values of the others.\nAbove we had derived an expression for the joint probability \\(P(\\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}, \\mathbf{\\Theta})\\). As \\(\\mathbf{y}\\) is observed, we only need to create conditional samples for \\(\\mathbf{c}\\), \\(\\mathbf{\\pi}\\), and \\(\\mathbf{\\Theta}\\).\n\\[\nP(\\mathbf{\\pi} |  \\mathbf{y}, \\mathbf{c}, \\mathbf{\\Theta}) =\n    \\mathrm{Dirichlet}(\\ldots, u_j + N_j, \\ldots)\n\\]\n\\[\nP(\\Theta_{jk} | \\mathbf{y}, \\mathbf{c}, \\mathbf{\\pi}) =\n    \\mathrm{Beta}(\\alpha_{jk} + N_{jk}, \\beta_{jk} + N_j -  N_{jk})\n\\]\nTo draw a sample for \\(c_i\\), we have to compute \\[\nP(c_i | \\mathbf{y}, \\mathbf{c}_{-i}, \\mathbf{\\Theta}, \\pi)\n\\]\nwhere \\(\\mathbf{c}_{-i}\\) denotes the vector of class membership for all subjects except \\(i\\). From the plate diagram, we can see that conditioned on \\(\\pi\\) and \\(\\mathrm{\\Theta}\\), \\(c_i\\) only depends on \\(\\pi\\), \\(\\mathrm{\\Theta}\\), and \\(y_{ik}\\). We have\n\\[\nP(c_i, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    P(c_i|\\pi) \\prod_{k=1}^K P(y_{ik}|c_i, \\mathrm{\\Theta})\n\\]\n\\[\nP(c_i = j, \\mathbf{y_i} | \\mathbf{\\Theta}, \\pi) =\n    \\pi_j \\prod_{k=1}^K \\Theta_{jk}^{y_{ik}}(1 - \\Theta_{jk})^{(1-y_{ik})}\n\\]\nThe Gibbs sampling algorithm can be summarized as follows:\n\nInitialize \\(\\mathbf{\\Theta}\\), \\(\\pi\\), \\(\\mathbf{c}\\)\ncompute \\(N_j\\), \\(N_{jk}\\)\nfor iter in 1:max_iter\n\nfor i in 1:N\n\nsample \\(c_i\\), and update \\(N_j\\) and \\(N_{jk}\\)\n\nsample \\(\\mathbf{\\pi}\\) using \\(N_j\\)\nsample \\(\\mathbf{\\Theta}\\) using \\(N_{jk}\\)\n\n\n\nusing DataFrames\nusing StatsBase: sample, Weights\nusing StatsFuns: logsumexp\n\n# Initialise\nu = ones(C)\nα = ones(C,K)\nβ = ones(C,K)\n\nπ = rand(Dirichlet(u))\nΘ = rand.(Beta.(α, β))\nc = rand(Categorical(π), N)\n\nN_j = zeros(C)      # number of subjects in class\nN_jk = zeros(C, K)  # number of positive answers for class j and question k\n\nfor i in 1:N\n    j = c[i]\n    N_j[j] += 1\n    for k in 1:K\n        N_jk[j, k] += y[i,k]\n    end\nend\n\nmax_iter = 11000\nburnin = 1000\nthinning = 10\n\nsamples = DataFrame()\n\nfor iter in 1:max_iter\n    # sample c[i]\n    for i in 1:N\n        # remove c[i]from N_j and N_jk\n        N_j[c[i]] -= 1\n        for k in 1:K\n            N_jk[c[i], k] -= y[i,k]\n        end\n        # compute p(c_i = j)\n        log_p_c = zeros(Float64, C)\n        for j in 1:C\n            log_p_c[j] += log(π[j])\n            for k in 1:K\n                log_p_c[j] += y[i,k]*log(Θ[j,k]) + (1.0 - y[i,k])*log(1.0 - Θ[j,k])\n            end\n        end\n        p_c = exp.(log_p_c .- logsumexp(log_p_c))\n        # and sample c[i]\n        c[i] = sample(1:K, Weights(p_c))\n        # add c[i] to N_j and N_jk\n        N_j[c[i]] += 1\n        for k in 1:K\n            N_jk[c[i], k] += y[i,k]\n        end\n    end\n\n    # sample \\pi\n    π = rand(Dirichlet(u .+ N_j))\n    \n    # sample \\Theta\n    Θ = rand.(Beta.(α .+ N_jk, β .+ N_j .- N_jk))\n\n    if iter &gt; burnin && (iter - burnin) % thinning == 0\n        push!(samples, (;π, Θ, c = copy(c)))\n    end\n\nend\n\nLet’s look at trace plots of the sampled parameters:\n\nf = Figure()\n\nlines(f[1:2,1], [x[1] for x in samples.π]; axis = (;title = \"π\"))\nlines!(f[1:2,1], [x[2] for x in samples.π])\nylims!(0,1)\n\nfor j in 1:C\n    for k in 1:K\n        lines(f[j,k+1], [x[j, k] for x in samples.Θ]; axis = (;title = \"Θ[$j,$k]\"))\n        ylims!(0,1)\n    end\nend\n\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/qMluh/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\nExpectation-Maximization (EM)\nThe Expectation-Maximization method results in point estimates of the parameters \\(\\mathbf{\\Theta}\\) and \\(\\pi\\), and probability distributions of latent variables \\(\\mathbf{c}\\).\nSimilar to the Gibbs sampling, we compute for each subject the class membership probabilities \\(P(\\mathbf{c}|\\mathbf{\\Theta}, \\pi)\\). EM uses the joint class distributions to compute the MLE of parameters \\(\\pi\\) and \\(\\mathbf{\\Theta}\\).\n\\[\n\\pi_j = \\frac{\\sum_{i=1}^N P(c_i = j|\\mathbf{\\Theta}, \\pi)}{N} = \\frac{N_j}{N}\n\\]\n\\[\n\\Theta_{jk} = \\frac{\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi) y_{ik}}\n                    {\\sum_{i=1}^N P(c_i=j|\\mathbf{\\Theta}, \\pi)} =\n                    \\frac{N_{jk}}{N_j}\n\\]\nIf one also incorporates prior counts (\\(u\\) and \\(\\alpha_{jk}\\), \\(\\beta_{jk]}\\) above), the MLE is replaced by a MAP estimate.\n\\[\n\\pi_j = \\frac{N_j + u_j - 1}{N + \\sum_{j=1}^C (u_j - 1)}\n\\]\n\\[\n\\Theta_{jk} = \\frac{N_{jk} + \\alpha_{jk} - 1}{N_j + \\alpha_{jk} + \\beta_{jk} - 2}\n\\]\n\n\nReferences\n\nResnik et al.(2010), Gibbs sampling for the uninitiated, http://users.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf\nLi et al.(2019), Bayesian Latent Class Analysis Tutorial https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6364555/, but here the notation seems seriously flawed"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]